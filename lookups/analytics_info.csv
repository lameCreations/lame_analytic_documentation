description,eventtypes,hosts,indexes,inputlookups,lookups,myapp,outputlookups,queries,sources,sourcetypes,urlField
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| stats dc(dest_ip) by src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| eventstats dc(dest_ip) by src_ip
| sort src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| sort src_ip
| streamstats dc(dest_ip) by src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| streamstats count as EventCount
| table _time, EventCount",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| streamstats avg(bytes_out)  window=10
| table avg(bytes_out), _time",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_dhcp 
| streamstats current=false last(src_ip) as new_dhcp_ip last(_time) as time_of_change by mac
| where src_ip!=new_dhcp_ip 
| convert ctime(time_of_change) as time_of_change 
| rename src_ip as old_dhcp_ip 
| table time_of_change, mac, old_dhcp_ip, new_dhcp_ip",,"sourcetype=corelight_dhcp","12_help_stats_eventstats_streamstats"
tbd,,,"index=os",,,"lame_training",,"index=os process=sshd (action=failure OR action=success) AND (Accepted OR Failed) user=troy
| sort 0 src, dest, user, _time asc
| streamstats count as contiguous_action by src, dest, user action current=true window=50 reset_on_change=true
| where (action=""success"" AND contiguous_action&gt;3) OR (action=""failure"" AND contiguous_action=1)
| sort - _time
| transaction src, dest, user  maxevents=2 startswith=action=""success"" endswith=action=failure
| rename contiguous_action as success_attempts

| stats count values(dest) as distinct_dests sum(success_attempts) as ""Total Successes"" by user",,,"12_help_stats_eventstats_streamstats"
tbd,,,,"inputlookup Demo_SSH_Logs.csv",,"lame_training",,"| inputlookup Demo_SSH_Logs.csv
| sort 0 src, dest, user, _time asc
 | streamstats count as contiguous_action by src, dest, user action current=true window=50 reset_on_change=true
| where (action=""success"" AND contiguous_action&gt;3) OR (action=""failure"" AND contiguous_action=1)
| sort - _time
| transaction src, dest, user  maxevents=2 startswith=action=""success"" endswith=action=failure
| rename contiguous_action as success_attempts

| stats count values(dest) as distinct_dests sum(success_attempts) as ""Total Successes"" by user",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) AND action!=dropped
|  stats sum(bytes_out) as ""sumOfBytesOut"" by src_ip, dest_ip
| eventstats sum(sumOfBytesOut) AS total_bytes_out by src_ip
| table src_ip dest_ip sumOfBytesOut, total_bytes_out
| sort src_ip - sumOfBytesOut",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) AND action!=dropped
|  stats sum(bytes_out) as ""sumOfBytesOut"" by src_ip, dest_ip
| eventstats sum(sumOfBytesOut) AS total_bytes_out by src_ip
| eval percent_bytes_out = sumOfBytesOut/total_bytes_out * 100
| table src_ip dest_ip total_bytes_out percent_bytes_out
| where percent_bytes_out &gt; 51
| sort - percent_bytes_out dest",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) action!=dropped
| sort _time
| streamstats sum(bytes_out) as total_bytes_out by src_ip
| table _time bytes_out total_bytes_out",,,"12_help_stats_eventstats_streamstats"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=25
| streamstats count as from 
| eval type=case(from==1, ""usergroups"", from==2, ""users"", from==3, ""server"", from==4, ""database"",from==5, ""desktop"",from==6, ""laptop"",from==7, ""printer"",from==8, ""harddrive"",from==9,""wifi"",from==10, ""code"",from==11, ""money"",from==12, ""bell""
,from==13, ""shopping-cart"",from==14, ""comment"",from==15, ""exclamation-circle"",from==16, ""file"",from==17, ""globe"",from==18, ""sitemap"",from==19, ""image"",from==20, ""info"", from==21, ""cloud"",from==22,""envelope"",from==23,""star"",from==24,""code-branch"",from==25,""folder"")

| eval to=case(from==2, ""usergroups"", from==3, ""users"", from==4, ""server"", from==5, ""database"",from==6, ""desktop"",from==7, ""laptop"",from==8, ""printer"",from==9, ""harddrive"",from==10,""wifi"",from==11, ""code"",from==12, ""money"",from==13, ""bell""
,from==14, ""shopping-cart"",from==15, ""comment"",from==16, ""exclamation-circle"",from==17, ""file"",from==18, ""globe"",from==19, ""sitemap"",from==20, ""image"",from==21, ""info"", from==22, ""cloud"",from==23,""envelope"",from==24,""star"",from==25,""code-branch"")

| eval from=type

|  eval color = random()%4 | eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"")

| eval value = type
| table from, to, value, type, color
| append[|makeresults | eval from=""Network Diagram Viz"", nodeText="" "", type=""customimage"", customurl=""/static/app/network-diagram-viz/customimages/network-diagram-viz.png"" | table from, nodeText, type, customurl]
      
 |  append [ 
 |  makeresults 
 |  eval raw=""from=\""usergroups\"", x=\""-6842\"", y=\""-1183\"" ### from=\""users\"", x=\""-6751\"", y=\""-1097\"" ### from=\""server\"", x=\""-6653\"", y=\""-1183\"" ### from=\""database\"", x=\""-6550\"", y=\""-1097\"" ### from=\""desktop\"", x=\""-6447\"", y=\""-1183\"" ### from=\""laptop\"", x=\""-6342\"", y=\""-1097\"" ### from=\""printer\"", x=\""-6245\"", y=\""-1183\"" ### from=\""harddrive\"", x=\""-6146\"", y=\""-1097\"" ### from=\""wifi\"", x=\""-6037\"", y=\""-1183\"" ### from=\""code\"", x=\""-5933\"", y=\""-1097\"" ### from=\""money\"", x=\""-5838\"", y=\""-1183\"" ### from=\""bell\"", x=\""-5714\"", y=\""-1097\"" ### from=\""shopping-cart\"", x=\""-5608\"", y=\""-1183\"" ### from=\""comment\"", x=\""-5497\"", y=\""-1097\"" ### from=\""exclamation-circle\"", x=\""-5388\"", y=\""-1183\"" ### from=\""file\"", x=\""-5273\"", y=\""-1097\"" ### from=\""globe\"", x=\""-5167\"", y=\""-1183\"" ### from=\""sitemap\"", x=\""-5049\"", y=\""-1097\"" ### from=\""image\"", x=\""-4943\"", y=\""-1183\"" ### from=\""info\"", x=\""-4838\"", y=\""-1097\"" ### from=\""cloud\"", x=\""-4735\"", y=\""-1183\"" ### from=\""envelope\"", x=\""-4622\"", y=\""-1097\"" ### from=\""star\"", x=\""-4510\"", y=\""-1183\"" ### from=\""code-branch\"", x=\""-4392\"", y=\""-1097\"" ### from=\""folder\"", x=\""-4284\"", y=\""-1183\"" ### from=\""Network Diagram Viz\"", x=\""-5512\"", y=\""-1238\""""
 |  makemv delim=""###"" raw 
 |  mvexpand raw 
 |  rename raw  as  _raw 
 |  extract 
 |  table from, x, y]",,,about
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv 
| search type=""app""
| eval tag=""csv"" 
| rename name as label
| append 
    [| rest /services/apps/local 
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""app"" | table name | rename name as label ] 
    | eval tag=""rest"" ] 
| stats count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by label
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename label as app
| table app status",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""lookup""
| eval tag=""csv"" 
| rename name as title
| append 
    [| rest /servicesNS/-/-/data/lookup-table-files
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""lookup"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as lookup_name
| table lookup_name, status",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""eventtype""
| eval tag=""csv"" 
| rename name as title
| append 
    [|rest servicesNS/-/-/saved/eventtypes
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""eventtype"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats values(search) as search count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as eventtype
| table eventtype, status, search",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""datamodel""
| eval tag=""csv"" 
| rename name as title
| append 
    [ |rest /services/data/models
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""datamodel"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats values(search) as search count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as datamodel
| table datamodel, status",,,"about_this_app"
tbd,,,,,,"splunk_app_soar",,"
      | rest /services/authentication/current-context splunk_server=local | table title roles | mvexpand roles | search roles=""*admin"" OR roles=""*splunk_app_soar"" OR roles=""*splunk_app_soar_dashboards""
    ",,,"action_run_search"
tbd,,,,,,"splunk_app_soar",,"| eval index_prefix_label=index_prefix.""*"" | stats count by index_prefix_label, index_prefix | sort -count",,,"action_run_search"
tbd,,,,,,"splunk_app_soar",,"| stats count by user_id_name ",,,"action_run_search"
tbd,,,"index=*phantom_action_run",,,"splunk_app_soar",,"index=*phantom_action_run sourcetype=phantom_search | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)(action_run)$"" | fields * ",,"sourcetype=phantom_search","action_run_search"
tbd,,,,,,"splunk_app_soar",,"
      | search index=$index_prefix$action_run | dedup index id  | rename id as action_run_id owner as user_id | table * | join type=left user_id index_prefix
          [ search index=$index_prefix$container
          | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)container$""
          | dedup index owner | table index index_prefix owner owner_name
          | rename owner as user_id owner_name as username]
      | eval user_id_name = case(user_id==""null"", ""None (unassigned)"", isnull(username), user_id, isnotnull(username), user_id."" ("".username."")"")
      | search user_id_name=""$user_id_name$"" playbook_run=$playbook_run$ $status$
      | eval start = strptime(create_time,""%Y-%m-%dT%H:%M:%S.%6Q"")
      | eval end = strptime(close_time,""%Y-%m-%dT%H:%M:%S.%6Q"")
      | eval duration=round(end -start,2) 
      | eval index_prefix = index_prefix.""*""
    ",,,"action_run_search"
tbd,,,,,,"splunk_app_soar",,"| timechart count eval(round(avg(duration),2)) as execution_time_sec",,,"action_run_search"
tbd,,,,,,"splunk_app_soar",,"
          | table _time index index_prefix action_run_id playbook_run container container_label action duration status user_id_name playbook_run
          | rename index as ""Index"" action as ""Action"" status as ""Status"" action_run_id as ""Action Run ID"" duration as ""Execution Time (sec)"" playbook_run as ""Playbook Run ID"" container as ""Container ID"" container_label as ""Container Label"" user_id_name as ""User ID (Username)"" index_prefix as ""Index Prefix"" ",,,"action_run_search"
tbd,,,,"inputlookup dashboard_details.csv",,"lame_documentation","outputlookup dashboard_details.csv","| makeresults
| eval id = ""$dURL$""
| eval mitre = ""$mitre$""
| eval details = ""$details$""
| eval usecase = ""$usecase$""
| eval status = ""$status$""
| eval urlField = ""$urlField$""
| eval app = ""$app$""
| table id, mitre, details, usecase, status, urlField, app

| where details != ""Change Me"" 

| append [ | inputlookup dashboard_details.csv ]
| table id, mitre, details, usecase, status, app, urlField

| dedup id, app, urlField
| outputlookup dashboard_details.csv append=true",,,"add_dashboard_details"
tbd,,,,"inputlookup saved_search_details.csv",,"lame_documentation","outputlookup saved_search_details.csv","| makeresults
| eval title = ""$title$""
| eval mitre = ""$mitre$""
| eval details = ""$details$""
| eval usecase = ""$usecase$""
| eval status = ""$status$""
| table title, mitre, details, usecase, status
| where details != ""Change Me"" 

| append [ | inputlookup saved_search_details.csv ]
| table title, mitre, details, usecase, status
| dedup title
| outputlookup saved_search_details.csv",,,"add_saved_search_details"
tbd,,,"index=*",,,"lame_documentation",,"| eventcount summarize=false index=* | dedup index | fields index",,,"add_sourcetype_analytics"
tbd,,,,,,"lame_documentation",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"add_sourcetype_analytics"
tbd,,,,,,"lame_documentation","outputlookup Sourcetype_Analytics","| makeresults
| eval index = ""$idx$""
| eval sourcetype = ""$src_type$""
| eval metric_description = ""$q_description$""
| eval metric_query = ""$query$""
| table index, sourcetype, metric_description, metric_query
| where metric_description != ""Change Me"" 
| outputlookup Sourcetype_Analytics append=true",,,"add_sourcetype_analytics"
tbd,,,"index=*",,,"lame_documentation",,"| eventcount summarize=false index=* | dedup index | fields index",,,"add_sourcetype_fields"
tbd,,,,,,"lame_documentation",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"add_sourcetype_fields"
tbd,,,,,,"lame_documentation",,"index=$idx$ sourcetype=$src_type$ | head 100
| fieldsummary | fields field",,,"add_sourcetype_fields"
tbd,,,,,,"lame_documentation","outputlookup SourcetypeInfo","| makeresults
| eval index = ""$idx$""
| eval sourcetype = ""$src_type$""
| eval rationale = ""$rationale$""
| eval fieldname = ""$fld$""
| eval fieldvalue = ""$fldvalue$""
| table index,  sourcetype, rationale, fieldname, fieldvalue
| where rationale != ""Change Me""

| outputlookup SourcetypeInfo append=true",,,"add_sourcetype_fields"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* prefilter_action=filter
        | stats count by search_type
      ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",," stats sum(count) as total_prefiltered",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_event_local_search_dispatch`
            | stats dc(search_id) as total_dispatched
            | eval output_with_percentage = ""$total_prefiltered$""."" ("".round($total_prefiltered$/($total_prefiltered$ + total_dispatched) * 100, 2)."" %)""
            | fields output_with_percentage
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""adhoc""
            | fields count
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""scheduled""
            | fields count
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* prefilter_action=filter
          | stats count by $SplitByVariable$
        ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eval percent = round(count / $total_prefiltered_searches$ * 100, 2)."" %""
            | rename prefilter_rule as Rule, user as User, app as App, search_type as ""Search Type"", search_name as ""Search Name"", host as Instance, count as Count, percent as ""Percent of Total""
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* search_type=""scheduled*"" prefilter_action=filter
            | stats count as prefiltered_count by prefilter_rule, user, app, host, search_name
            | fields search_name, app, user, host, prefilter_rule, prefiltered_count
            | sort - prefiltered_count
            | rename search_name as ""Search Name"", app as App, user as User, host as Instance, prefilter_rule as ""Prefiltering Rule Triggered"", prefiltered_count as ""Prefiltered Count""
          ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest [inputlookup dmc_assets | where host = $scheduled_search_host_drilldown|s$ | rename serverName AS splunk_server | return splunk_server] ""/servicesNS/-/-/saved/searches/$scheduled_search_drilldown$"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | fields cron_schedule, next_scheduled_time, search 
            | rename cron_schedule as ""Cron Schedule"", next_scheduled_time as ""Next Scheduled Time"", search as ""Search""
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_internal` host=$host$ sourcetype=wlm_* prefilter_action=filter
        | stats count by search_type
      ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",," stats sum(count) as total_prefiltered",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_event_local_search_dispatch`
            | stats dc(search_id) as total_dispatched
            | eval output_with_percentage = ""$total_prefiltered$""."" ("".round($total_prefiltered$/($total_prefiltered$ + total_dispatched) * 100, 2)."" %)""
            | fields output_with_percentage
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""adhoc""
            | fields count
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""scheduled""
            | fields count
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=wlm_* host=$host$ prefilter_action=filter
          | stats count by $SplitByVariable$
        ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eval percent = round(count / $total_prefiltered_searches$ * 100, 2)."" %""
            | rename prefilter_rule as Rule, user as User, app as App, search_type as ""Search Type"", search_name as ""Search Name"", count as Count, percent as ""Percent of Total""
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=wlm_* search_type=""scheduled*"" prefilter_action=filter
            | stats count as prefiltered_count by prefilter_rule, user, app, search_name
            | fields search_name, app, user, prefilter_rule, prefiltered_count
            | sort - prefiltered_count
            | rename search_name as ""Search Name"", app as App, user as User, prefilter_rule as ""Prefiltering Rule Triggered"", prefiltered_count as ""Prefiltered Count""
          ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/$scheduled_search_drilldown$"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | fields cron_schedule, next_scheduled_time, search 
            | rename cron_schedule as ""Cron Schedule"", next_scheduled_time as ""Next Scheduled Time"", search as ""Search""
          ",,,"admission_control_monitoring_instance"
tbd,,,,"inputlookup append",,"log_analysis_made_easy","outputlookup LAME_Alert_Whitelist.csv","| makeresults | eval alert_name = ""$alert$""
| eval date_reviewed = now()
| eval valid_until = relative_time(now(),""+$r_date$"")
| eval _time = valid_until
| eval src_ip = ""$src_ip$""
| eval dest_ip = ""$dest_ip$""
| eval rationale = ""$rationale$""
| eval analyst = ""$env:user$""
| eval risk_score = ""$r_score$""
| eval protect = case(src_ip="""" OR dest_ip="""" OR valid_until="""" OR alert_name="""" OR rationale="""", ""false"", 1=1, ""true"")

| where protect = ""true""

| table risk_score analyst date_reviewed, _time, valid_until, src_ip, dest_ip, alert_name, rationale

| inputlookup append=t LAME_Alert_Whitelist.csv
| sort - _time
| dedup alert_name, src_ip, dest_ip
| table risk_score analyst date_reviewed, _time, valid_until, src_ip, dest_ip, alert_name, rationale

| sort - _time
| dedup alert_name, src_ip, dest_ip
| table risk_score analyst date_reviewed, _time, valid_until, src_ip, dest_ip, alert_name, rationale
| outputlookup LAME_Alert_Whitelist.csv",,,"alert_whitelisting"
tbd,,,,"inputlookup analytics_info",,"lame_analytic_documentation",,"| inputlookup analytics_info
| rename _key as the_key
| table the_key, queries, description, myapp, urlField",,,"analytic_information"
tbd,,,,,"lookup dashboard_details","lame_training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| table eai:data, app.owner, id, eai:acl.app author, eai:acl.sharing | lookup dashboard_details id as id output details, mitre, usecase


",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=* AND author!=""nobody"" 
| table eai:acl.app, author, cron_schedule is_scheduled, disabled",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"
| eval cron_schedule = case(cron_schedule="""", ""n/a"", 1=1, cron_schedule)
| search eai:acl.app=* AND author!=""nobody"" cron_schedule!=""n/a""  is_scheduled=1 disabled=0
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats count by myapp","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats dc(queries) as queries values(myapp) as myapp  values(author) as author by urlField
| sort - queries","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,,,"lookup dashboard_details","lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| table eai:data, app.owner, id, eai:acl.app author, eai:acl.sharing | lookup dashboard_details id as id output details, mitre, usecase


",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=* AND author!=""nobody"" 
| table eai:acl.app, author, cron_schedule is_scheduled, disabled",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",,"| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",," 
| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_documentation",,"
| eval cron_schedule = case(cron_schedule="""", ""n/a"", 1=1, cron_schedule)
| search eai:acl.app=* AND author!=""nobody"" cron_schedule!=""n/a""  is_scheduled=1 disabled=0
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_documentation",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats count by myapp","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_documentation",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats dc(queries) as queries values(myapp) as myapp  values(author) as author by urlField
| sort - queries","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,,"inputlookup savedsearch_info
inputlookup dashboard_info",,"lame_analytic_documentation",,"| inputlookup savedsearch_info 
| eval foo = replace(mitre, "" "", """")
| eval foo = replace(foo, ""\["", """")
| eval foo = replace(foo, ""]"", """")
| eval bar = split(foo, "","")
| mvexpand bar

| stats values(title) as description count by bar
| rename bar as id
| append [ | inputlookup dashboard_info
| eval foo = replace(mitre, "" "", """")
| eval foo = replace(foo, ""\["", """")
| eval foo = replace(foo, ""]"", """")
| eval bar = split(foo, "","")
| mvexpand bar

| stats values(id) as description count by bar
| rename bar as id

| table id, count, description]
",,,"analytics_mapped_to_mitre"
tbd,,,,,,"splunk_app_soar",,"
      | rest /services/authentication/current-context splunk_server=local | table title roles | mvexpand roles | search roles=""*admin"" OR roles=""*splunk_app_soar"" OR roles=""*splunk_app_soar_dashboards""
    ",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| eval index_prefix_label=index_prefix.""*"" | stats count by index_prefix_label, index_prefix | sort -count",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| stats count by user_id_name ",,,"automation_analytics"
tbd,,,"index=*phantom_action_run
index=*phantom_app_run",,,"splunk_app_soar",,"(index=*phantom_action_run OR index=*phantom_app_run) sourcetype=phantom_search | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)(action_run|app_run)$"" | fields * ",,"sourcetype=phantom_search","automation_analytics"
tbd,,,,,,"splunk_app_soar",,"
      | search index=$index_prefix$action_run (status=failed OR status=success) | dedup index id  | rename id as action_run_id owner as user_id | table * | join type=left user_id index_prefix
          [ search index=$index_prefix$container
          | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)container$""
          | dedup index owner | table index index_prefix owner owner_name
          | rename owner as user_id owner_name as username]
      | eval user_id_name = case(user_id==""null"", ""None (unassigned)"", isnull(username), user_id, isnotnull(username), user_id."" ("".username."")"")
    ",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"
      | search index=$index_prefix$app_run status=failed | dedup index id  | rename effective_user as user_id | table * | join type=left user_id index_prefix
          [ search index=$index_prefix$container
          | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)container$""
          | dedup index owner | table index index_prefix owner owner_name
          | rename owner as user_id owner_name as username]
      | eval user_id_name = case(user_id==""null"", ""None (unassigned)"", isnull(username), user_id, isnotnull(username), user_id."" ("".username."")"")
    ",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| search user_id_name=""$user_id_name$""  |  eval start = strptime(create_time,""%Y-%m-%dT%H:%M:%S"")| eval end = strptime(close_time,""%Y-%m-%dT%H:%M:%S"") | eval duration= end -start | timechart span=1h count eval(round(avg(duration),2)) as execution_time by status",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| search user_id_name=""$user_id_name$"" | stats count by index_prefix user_id_name | sort - count | eval index_prefix=index_prefix.""*"" | chart list(count) by user_id_name, index_prefix | rename count as ""Execution Count"" user_id_name as ""User ID (Username)"" | head 10",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| search user_id_name=""$user_id_name$"" | chart count over action by status | addtotals fieldname=temp_sort |  sort - temp_sort  | fields - temp_sort | head 10",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"| search user_id_name=""$user_id_name$"" | stats count by status",,,"automation_analytics"
tbd,,,"index = asset_name.",,,"splunk_app_soar",,"| search user_id_name=""$user_id_name$"" |  rex field=message ""on asset '(?&lt;asset_name&gt;[^']+)'"" |  stats count by asset_name, action, index_prefix | sort - count | head 10 | eval asset_index = asset_name.""("".index_prefix.""*)"" | chart list(count) by action, asset_index ",,,"automation_analytics"
tbd,,,,,,"splunk_app_soar",,"|  search user_id_name=""$user_id_name$"" | chart count over action by status  | addtotals fieldname=temp_sort |  sort - temp_sort  | fields - temp_sort",,,"automation_analytics"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=5
| streamstats count as from 
| eval type=case(from==1, ""triangle"", from==2, ""dot"", from==3, ""square"", from==4, ""diamond"",from==5, ""hexagon"")
| eval to=case(from==1, ""dot"", from==2, ""square"", from==3, ""diamond"",from==4, ""hexagon"")
| eval from=type
| eval color = random()%4 
| eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"")
| eval value = type
| table from, to, value, type, color",,,"available_icons"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=11
| streamstats count as id 
| eval to = from +1
| eval type=case(id==1, ""search-head"",id==2, ""manager-node"", id==3, ""indexer"", id==4, ""index"",id==5, ""deployment-server"",id==6, ""forwarder"",id==7, ""firewall"",id==8, ""heavy-forwarder"",id==9,""license-master"",id==10, ""bucket"",id==11, ""load-balancer"")
| eval to=case(id==2, ""search-head"",id==3, ""manager-node"", id==4, ""indexer"", id==5, ""index"",id==6, ""deployment-server"",id==7, ""forwarder"",id==8, ""firewall"",id==9, ""heavy-forwarder"",id==10,""license-master"",id==11, ""bucket"",id==1, ""load-balancer"")
| eval value = type
| eval from=type
| table from, to, value, type, color",,,"available_icons"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=42
| streamstats count as from 
| eval type=case(from==1, ""usergroups"", from==2, ""users"", from==3, ""server"", from==4, ""database"",from==5, ""desktop"",from==6, ""laptop"",from==7, ""printer"",from==8, ""harddrive"",from==9,""wifi"",from==10, ""code"",from==11, ""money"",from==12, ""bell""
,from==13, ""shopping-cart"",from==14, ""comment"",from==15, ""exclamation-circle"",from==16, ""file"",from==17, ""globe"",from==18, ""sitemap"",from==19, ""image"",from==20, ""info"", from==21, ""cloud"",from==22,""envelope"",from==23,""star"",from==24,""file"",from==25,""folder"",
from==26, ""windows"", from==27, ""adobe"", from==28, ""aws"", from==29, ""paypal"",from==30, ""git"",from==31, ""google"",from==32, ""googledrive"",from==33, ""java"",from==34,""jenkins"",from==35, ""nodejs"",from==36, ""php"",from==37, ""twitter""
,from==38, ""wordpress"",from==39, ""windows"",from==40, ""linux"",from==41, ""skype"",from==42, ""slack"")
| streamstats current=false last(type) as to
| eval from=type
|  eval color = random()%4 | eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"")
| eval value = type
| table from, to, value, type, color",,,"available_icons"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=*  | stats count by index
| fields index",,,"build_a_query"
tbd,,,,,,"CyberSentry_Training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"build_a_query"
tbd,,,,,,"CyberSentry_Training",,"index=$idx$ sourcetype=$src_type$ | head 100 | fieldsummary | fields field",,,"build_a_query"
tbd,,,,,,"CyberSentry_Training",,"index=$idx$ sourcetype=$src_type$ | head 10 | $fld$",,,"build_a_query"
tbd,,,,,,"splunk_monitoring_console",,"
              | `dmc_get_groups_containing_role(dmc_group_search_head)` 
              | search search_group!=""dmc_group_*""
            ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | rest splunk_server=$splunk_server$ /services/search/distributed/bundle/replication/config
            ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | fields replicationPolicy
                | eval replicationPolicy = upper(substr(replicationPolicy,1,1)).substr(replicationPolicy,2)
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | eval replicationThreads = case(isnull(replicationThreads), ""N/A"", isnotnull(replicationThreads), replicationThreads)
                | fields replicationThreads
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                  bundle_type=full_bundle
                  | eval repl_sec = replication_time_msec/1000
                  | stats avg(repl_sec)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                  bundle_type=delta_bundle
                  | eval repl_sec = replication_time_msec/1000
                  | stats avg(repl_sec)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                  bundle_type=full_bundle
                  | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                  | stats avg(size_mb)
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                  bundle_type=delta_bundle
                  | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                  | stats avg(size_mb)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                | fields bundle_name, checksum, id, splunk_server, upload_time
                | mvcombine splunk_server
                | sort upload_time desc
                | eval latest_common=case(mvcount(splunk_server)==[
                  | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                  | stats dc(splunk_server) AS total_server_count
                  | return $total_server_count], 
                ""found"", mvcount(splunk_server)!=[
                  | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                  | stats dc(splunk_server) AS total_server_count
                  | return $total_server_count],
                ""none"")
                | where latest_common==""found""
                | head 1
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | eval upload_time=""Created at: "".(strftime(upload_time, ""%m/%d/%Y %H:%M:%S %p""))
                  | fields upload_time
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | rex field=id ""(?&lt;id&gt;([^\/]+$))""
                  | eval id = ""Bundle Id: "".tostring(id)
                  | fields id
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | eval checksum = ""Checksum: "".tostring(checksum)
                  | fields checksum
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | appendpipe [
                    stats count
                    | eval checksum=""No common knowledge bundle found across all peers""
                    | where count=0]
                  | fields checksum
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=cycle_dispatch
                | stats count
              ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=cycle_dispatch
                | eval ratio = round((peer_success_count/peer_count)*100, 2)
                | stats count by ratio
                | rename ratio as ""Peer Success Ratio""
              ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
              | eval repl_sec = replication_time_msec/1000
              | `dmc_timechart_for_metrics_log` avg(repl_sec) by bundle_type
            ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                | `dmc_timechart_for_metrics_log` avg(size_mb) by bundle_type
            ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                     `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                     | eval repl_sec = replication_time_msec/1000
                     | `dmc_timechart_for_metrics_log` avg(repl_sec) AS avg_repl_time p90(repl_sec) AS 90th_perc_repl_time p10(repl_sec) AS 10th_perc_repl_time
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                     `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                     | eval repl_sec = replication_time_msec/1000
                     | `dmc_timechart_for_metrics_log` limit=50 avg(repl_sec) AS avg_repl_time by peer_name
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` (search_group=dmc_group_indexers OR search_group=dmc_group_search_head) source=*splunkd.log*
              (component=BundlesAdminHandler OR  component=BundleDataProcessor OR component=BundleDeltaHandler OR
              component=BundleReplicationProvider OR component=BundleStatusManager OR component=BundleTransaction OR
              component=CascadePlan OR component=CascadeReplicationReaper OR component=CascadingBundleReplicationProvider OR
              component=CascadingReplicationManager OR component=CascadingReplicationTransaction OR 
              component=CascadingReplicationStatusActor OR component=CascadingUploadHandler OR component=ClassicBundleReplicationProvider OR
              component=DistBundleRestHandler OR component=DistributedBundleReplicationManager OR
              component=GetCascadingReplicationStatusTransaction OR component=RFSManager OR component=RFSBundleReplicationProvider)
              (log_level=WARN OR log_level=ERROR)
              | timechart bins=200 partial=f count by component
            ","source=*splunkd.log*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` (search_group=dmc_group_indexers OR search_group=dmc_group_search_head) source=*splunkd.log*
              (component=BundlesAdminHandler OR  component=BundleDataProcessor OR component=BundleDeltaHandler  OR
              component=BundleReplicationProvider OR component=BundleStatusManager OR component=BundleTransaction OR
              component=CascadePlan OR component=CascadeReplicationReaper OR component=CascadingBundleReplicationProvider OR component=CascadingReplicationManager OR component=CascadingReplicationTransaction OR component=CascadingReplicationStatusActor OR component=CascadingUploadHandler OR component=ClassicBundleReplicationProvider OR component=DistBundleRestHandler OR
              component=DistributedBundleReplicationManager OR component=GetCascadingReplicationStatusTransaction OR
              component=RFSManager OR component=RFSBundleReplicationProvider)
              (log_level=WARN OR log_level=ERROR)
              | chart count by component, log_level
              | sort - ERROR, WARN
            ","source=*splunkd.log*",,"bundle_replication"
tbd,,,,"inputlookup nd-business-process.csv","lookup nd-business-process-searches.csv","network-diagram-viz",,"|inputlookup nd-business-process.csv
| lookup nd-business-process-searches.csv process_step as from OUTPUT color
| eval color=case(color=""red"",""red"",color=""yellow"",""yellow"",color=""green"",""green"",1=1,""grey"")
| eval status = case(color=""red"",""Critical"",color=""yellow"",""Warning"",color=""green"",""OK"",1=1,""No Data"")
| eval value=""Step: "" + nodeText +"". Status: "" + status
 |  append [ 
 |  makeresults 
 |  eval raw=""from=\""start\"", x=\""-1370\"", y=\""-385\"" ### from=\""receive_issue_list\"", x=\""-1369\"", y=\""-195\"" ### from=\""review_issue_list\"", x=\""-1369\"", y=\""-37\"" ### from=\""any_issues_ready\"", x=\""-1369\"", y=\""180\"" ### from=\""end_1\"", x=\""-1127\"", y=\""-48\"" ### from=\""discussion_cycle\"", x=\""-1110\"", y=\""173\"" ### from=\""announce_issues\"", x=\""-886\"", y=\""93\"" ### from=\""collect_votes\"", x=\""-888\"", y=\""-48\"" ### from=\""prepare_results\"", x=\""-891\"", y=\""-219\"" ### from=\""post_results\"", x=\""-614\"", y=\""-389\"" ### from=\""email_results\"", x=\""-614\"", y=\""-219\"" ### from=\""completed_1\"", x=\""-333\"", y=\""-308\"" ### from=\""enough_members_voted\"", x=\""-40\"", y=\""-306\"" ### from=\""issues_wo_majority\"", x=\""499\"", y=\""-313\"" ### from=\""members_are_warned\"", x=\""-40\"", y=\""-48\"" ### from=\""reduce_voting_members\"", x=\""237\"", y=\""-143\"" ### from=\""reannounce_vote\"", x=\""-594\"", y=\""-48\"" ### from=\""end_3\"", x=\""899\"", y=\""-313\"" ### from=\""2nd_time\"", x=\""497\"", y=\""160\"" ### from=\""Critical\"", nodeText=\""Critical\"", value=\""This color denotes the process step is critical\"",type=\""dot\"", color=\""red\"", x=\""-1600\"", y=\""-95\"" ### from=\""Warning\"", color=\""yellow\"", value=\""This color denotes the process step is experiencing some issues\"",type=\""dot\"", nodeText=\""Warning\"", x=\""-1600\"", y=\""-10\"" ### from=\""OK\"", value=\""This color denotes the process step has no issues\"", color=\""green\"", type=\""dot\"",nodeText=\""OK\"", x=\""-1600\"", y=\""80\"" ### from=\""No Data\"", nodeText=\""No Data\"", value=\""This color denotes the process step has no data\"", color=\""grey\"", type=\""dot\"", x=\""-1600\"", y=\""165\""""
 |  makemv delim=""###"" raw 
 |  mvexpand raw 
 |  rename raw  as  _raw 
 |  extract 
 |  table from, x, y, color, type, nodeText, value]",,,"business-process-diagram"
tbd,,,,"inputlookup nd-business-process-searches.csv",,"network-diagram-viz",,"| inputlookup nd-business-process-searches.csv WHERE process_step=$selected_node|s$
| eval Time = strftime(now() - time, ""%Y-%m-%d %H:%M:%S"")
| rename process_step as ""Process Step"", savedsearch_name as ""Splunk Saved Search"", status as Status, description as Description
| table Time, ""Process Step"", ""Splunk Saved Search"", Description, Status",,,"business-process-diagram"
tbd,,,"index=wekan",,"lookup wekan_lists.csv
lookup wekan_users.csv","lame_wekan_pm",,"index=wekan sourcetype=wekan_cards id=$id$
| dedup id
| eval checklistTitleWithStatus = case(sourcetype=""wekan_checklistitems"" AND isFinished=""true"", title + "" - Complete"", sourcetype=""wekan_checklistitems"" AND isFinished=""false"", title + "" - Pending"" )
| eval dateLastActivityTime = case(sourcetype=""wekan_cards"", endAt)
| eval cardTitle = case(sourcetype=""wekan_cards"", title)
| eval dueAt = strftime(dueAt, ""%Y-%m-%d"")
| eval endAt = strftime(endAt, ""%Y-%m-%d"")
| eval startAt = strftime(startAt, ""%Y-%m-%d"")
| eval modifiedAt = strftime(modifiedAt, ""%Y-%m-%d"")
| lookup wekan_lists.csv id as listId output title as listTitle
| lookup wekan_users.csv id as assignees_0 output username as assignees_0
| eval _time = dateLastActivityTime
| table cardTitle, description,  requestedBy, startAt, endAt  dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""
| sort startAt, Task",,"sourcetype=wekan_cards
sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_cards","card_id_drilldown"
tbd,,,"index=wekan",,"lookup wekan_lists.csv
lookup wekan_users.csv","lame_wekan_pm",,"index=wekan sourcetype=wekan_card_comments cardId=$id$
| dedup cardId
| eval checklistTitleWithStatus = case(sourcetype=""wekan_checklistitems"" AND isFinished=""true"", title + "" - Complete"", sourcetype=""wekan_checklistitems"" AND isFinished=""false"", title + "" - Pending"" )
| eval dateLastActivityTime = case(sourcetype=""wekan_cards"", endAt)
| eval cardTitle = case(sourcetype=""wekan_cards"", title)
| eval dueAt = strftime(dueAt, ""%Y-%m-%d"")
| eval endAt = strftime(endAt, ""%Y-%m-%d"")
| eval startAt = strftime(startAt, ""%Y-%m-%d"")
| eval modifiedAt = strftime(modifiedAt, ""%Y-%m-%d"")
| lookup wekan_lists.csv id as listId output title as listTitle
| lookup wekan_users.csv id as assignees_0 output username as assignees_0
| eval _time = dateLastActivityTime
| table  text 
| rename  text as ""Comments on Task"",",,"sourcetype=wekan_card_comments
sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_cards","card_id_drilldown"
tbd,,,"index=wekan",,"lookup wekan_lists.csv
lookup wekan_users.csv","lame_wekan_pm",,"index=wekan sourcetype=""wekan_checklistitems"" cardId=$id$
| dedup cardId
| eval checklistTitleWithStatus = case(sourcetype=""wekan_checklistitems"" AND isFinished=""true"", title + "" - Complete"", sourcetype=""wekan_checklistitems"" AND isFinished=""false"", title + "" - Pending"" )
| eval dateLastActivityTime = case(sourcetype=""wekan_cards"", endAt)
| eval cardTitle = case(sourcetype=""wekan_cards"", title)
| eval dueAt = strftime(dueAt, ""%Y-%m-%d"")
| eval endAt = strftime(endAt, ""%Y-%m-%d"")
| eval startAt = strftime(startAt, ""%Y-%m-%d"")
| eval modifiedAt = strftime(modifiedAt, ""%Y-%m-%d"")
| lookup wekan_lists.csv id as listId output title as listTitle
| lookup wekan_users.csv id as assignees_0 output username as assignees_0
| eval _time = dateLastActivityTime
| table checklistTitleWithStatus, status 
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""
| sort startAt, Task",,"sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_cards","card_id_drilldown"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | where search_group=""$role$""
        ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/search/distributed/bundle/replication/config
          ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | fields replicationPolicy
              | eval replicationPolicy = upper(substr(replicationPolicy,1,1)).substr(replicationPolicy,2)
            ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | fields replicationThreads
            ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by num_levels
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by num_peers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by endpoint
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_replication
               | dedup planid
               | stats count by num_receivers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | stats count by num_receivers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               | multisearch
                 [search `dmc_set_index_internal` source=*metrics.log group=cascading name=plan_replication]
                 [search `dmc_set_index_internal` source=*metrics.log group=cascading name=payload_replication]
               | chart count over name by status
               | replace payload_replication WITH payload
               | replace plan_replication WITH plan
               | rename name as ""Replication Type"" 
               | sort -""Replication Type""
            ","source=*metrics.log
source=*metrics.log",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_replication
               | dedup planid
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_ms) AS avg_plan_msec
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` search_group=dmc_group_indexer source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | eval dispatch_time_sec=dispatch_time_ms/1000
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_sec) AS avg_payload_sec
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` search_group=dmc_group_indexer source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | eval dispatch_time_sec=dispatch_time_ms/1000
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_sec) by peer_guid
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_app_soar",,"
      | rest /services/authentication/current-context splunk_server=local | table title roles | mvexpand roles | search roles=""*admin"" OR roles=""*splunk_app_soar"" OR roles=""*splunk_app_soar_dashboards""
    ",,,"case_search"
tbd,,,"index=*phantom_container
index=*phantom_note",,,"splunk_app_soar",,"(index=*phantom_container OR index=*phantom_note) AND sourcetype=phantom_search | fields * ",,"sourcetype=phantom_search","case_search"
tbd,,,,,,"splunk_app_soar",,"| rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)(container|note)$"" | eval index_prefix_label=index_prefix.""*"" | stats count by index_prefix_label, index_prefix",,,"case_search"
tbd,,,,,,"splunk_app_soar",,"| search index=$index_prefix$container $severity$ $sensitivity$ | stats count by container_label | sort - count",,,"case_search"
tbd,,,,,,"splunk_app_soar",,"| search index=$index_prefix$container $severity$ $sensitivity$ | eval status_label = upper(substr(status,1,1)).substr(status,2) | stats count by status_label, status | sort - count",,,"case_search"
tbd,,,,,,"splunk_app_soar",,"
            search index=$index_prefix$container | spath container_type | search $container_type$ $text$ $severity$ $sensitivity$ $status$ $label$ | dedup index id | table container_update_time index id name description status owner_name container_label severity sensitivity | eval description_preview=substr(description,1,50)
            | fields - description
            | sort - container_update_time
            | rename container_update_time as ""Last Updated"" index as ""Index"" id as ""ContainerID"" name as ""Container Name"" description_preview as ""Description Preview"" status as ""Status"" owner_name as ""Owner"" container_label as ""Label"" sensitivity as ""Sensitivity"" severity as ""Severity""
          ",,,"case_search"
tbd,,,,,,"splunk_app_soar",,"
          search index=$index_prefix$note | spath container_type | search $container_type$ $label$ | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom)_note"" | join type=outer index_prefix container
            [ search index=$index_prefix$container $container_type$ $severity$ $sensitivity$ $status$ $label$
            | dedup index id 
            | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom)_container""
            | rename id as container
            | fields index_prefix container status severity sensitivity]
            | search $text$ $severity$ $sensitivity$ $status$
            | eval content_preview=substr(content,1,50)
            | table modified_time index container id container_type title status container_label severity sensitivity content_preview note_type
            | sort - modified_time
            | rename modified_time as ""Last Updated"" index as ""Index"" container as ""ContainerID"" id as ""NoteID"" container_type as ""Container Type"" title as ""Title"" content_preview as ""Content Preview"" note_type as ""Note Type"" container_label as ""Label"" status as ""Status"" sensitivity as ""Sensitivity"" severity as ""Severity""
        ",,,"case_search"
tbd,,,,"inputlookup cim_validation_dictionary",,"SA-cim_vladiator",,"| inputlookup cim_validation_dictionary | stats count by datamodel",,,"cim_dictionary"
tbd,,,,,,"SA-cim_vladiator",,"| stats count",,,"cim_dictionary"
tbd,,,,,,"SA-cim_vladiator",,"| stats dc(datamodel)",,,"cim_dictionary"
tbd,,,,"inputlookup cim_validation_dictionary",,"SA-cim_vladiator",,"| inputlookup cim_validation_dictionary | search datamodel=""$dm$"" field=""$field$"" | fields - cim_version",,,"cim_dictionary"
tbd,,,,,"lookup cim_validation_regex
lookup cim_validator_recommended_fields","SA-cim_vladiator",,"| datamodel $dm$ | rex max_match=999 ""fieldName\"":\""(?&lt;field&gt;[^\""]+)"" | stats values(field) as field | mvexpand field  | where NOT match(field, ""_time|host|sourcetype|source|[A-Z]+|_bunit|_category|_priority|_requires_av|_should_update"") OR match(field, ""object_category"")   | join type=outer field [$search_type$ $cim_search$ | head $event_limit$ | fieldsummary maxvals=15 | eventstats max(count) AS total | eval percent_coverage=round(count/total*100, 2) | table field, percent_coverage, distinct_count, total, values]  | spath input=values | rename {}.value AS sample_values {}.count AS sample_count distinct_count AS distinct_value_count total AS total_events  | fillnull value=0 percent_coverage, distinct_value_count, total_events   | mvmath field=sample_count field2=total_events | eval field_values=mvzip(mvmath_result, sample_values, "" "")  | lookup cim_validation_regex field | mvrex showcount=t showunmatched=t field=sample_values validation_regex  | eval is_cim_valid=case(total_events==0, ""severe!!!no extracted values found"", percent_coverage &lt; 90, ""elevated!!!event coverage less than 90%"", mvrex_unmatched_count &gt; 0, ""elevated!!!found "".mvrex_unmatched_count."" unexpected values ("".mvjoin(mvrex_unmatched, "", "")."")"", isnull(validation_regex) OR validation_regex=="""", ""check!!!no validation regex was found to evaluate"", 1==1, ""low!!!looking good!"")  | lookup cim_validator_recommended_fields field OUTPUT is_recommended | eval ir=if(is_recommended==""true"", ""star"", null()) | table ir, field, total_events, distinct_value_count, percent_coverage, field_values, is_cim_valid",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| datamodel            | spath modelName           | table modelName           | sort  modelName",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| stats count",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| stats sum(eval(if(match(is_cim_valid, ""^low""), 0, 1))) AS bad",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| stats count sum(eval(if(match(is_cim_valid, ""^low""), 0, 1))) AS bad | eval percent=round((1 -(bad/count))*100) | table percent",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| where isnotnull(ir) | stats count sum(eval(if(match(is_cim_valid, ""^low""), 0, 1))) AS bad | eval percent=round((1 -(bad/count))*100) | table percent",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"| table ir, field, total_events, distinct_value_count, percent_coverage, field_values, is_cim_valid | where $is_only$",,,"cim_validator"
tbd,,,,,,"SA-cim_vladiator",,"$cim_search$ | head $event_limit$",,,"cim_validator"
tbd,,,,,,"lame_training",,"| makeresults 
| eval a=""$ip1$""
| eval b=""$ip2$""
| eval a_port=$ip1_port$
| eval b_port=$ip2_port$
| eval proto=""$proto$""
| cid proto b, a, b_port, a_port
| table a, b, a_port, b_port proto cid",,,"community_id_generator"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"condition_match"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"condition_match"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"condition_match"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_conn src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$"" 
| stats count as eventCount earliest(_time) as firstSeenTime by service
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_conn","conn_log_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_conn src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$"" 

| bin span=""$tspan$"" _time 

| stats  count as eventCount by _time service
| eventstats perc$perc$(eventCount) as perc95 by service
| eval diff = eventCount-perc95
| chart avg(diff) by _time, service",,"sourcetype=corelight_conn","conn_log_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_conn src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$"" 
| stats count earliest(_time) as firstSeenTime by dest_ip
| iplocation dest_ip 
| eval _time = firstSeenTime
| table dest_ip, count, _time, Country, City,",,"sourcetype=corelight_conn","conn_log_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_conn src_ip=""$src_ip$"" dest_ip=""$dest_ip$"" 

| bin span=""$tspan$"" _time 

| stats avg(orig_bytes) as bytes_in avg(resp_bytes) as bytes_out by _time
| eval abs_bytes = abs(bytes_in - bytes_out) 
| eventstats perc90(bytes_in) as bytes_in_perc95 perc90(bytes_out) as bytes_out_perc95, perc90(abs_bytes) as abs_bytes_perc95",,"sourcetype=corelight_conn","conn_log_anomalies"
tbd,,,,,,"network-diagram-viz",,"$search$",,,"create_designs"
tbd,,,,,,"network-diagram-viz",,"$search$",,,"create_designs_dark_mode"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                        index=_internal | stats count by sourcetype
                    ",,,"custom_app_token"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 550 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 450 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 350 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 250 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 150 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,,,,"simple_xml_examples",,"| stats count as value | eval value = 50 | rangemap field=value none=0-99 low=100-199 guarded=200-299 elevated=300-399 high=400-499 severe=500-599 default=none",,,"custom_decorations"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal http:// | head 50 | table _time user referer",,,"custom_drilldown_url_field"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"custom_event_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by $input_token$",,,"custom_event_tokens"
tbd,,,,,,"network-diagram-viz",,"|makeresults
| eval raw=""from=\""user\"",nodetext=\""User\"",to=\""website\"" ###from=\""user\"",to=\""api\""###from=\""api\"",nodetext=\""API Calls\"",to=\""firewall\""###from=\""firewall\"", to=\""splunk\""###from=\""splunk\"",nodetext=\""Splunk Forwarder\""###from=\""website\"",nodetext=\""Website\"", to=\""cart\""###from=\""cart\"",nodetext=\""Cart\""###from=\""website\"",to=\""firewall\""###from=\""firewall\"",nodetext=\""Firewall\"", to=\""router\""###from=\""router\"",nodetext=\""Router\"", to=\""server1\""###from=\""server1\"",nodetext=\""Host 1\""###from=\""server2\"",nodetext=\""Host 2\""###from=\""server3\"",nodetext=\""Host 3\""###from=\""router\"", to=\""server2\""###from=\""router\"", to=\""server3\""###from=\""router\"", to=\""voip\""###from=\""voip\"",nodetext=\""VOIP\""###from=\""voip\"", to=\""phone\""###from=\""phone\"",nodetext=\""VOIP Phone\""### from=\""network-diagram-viz\"", value=\""Network Diagram Viz\"" nodetext=\""   \""""
|makemv delim=""###"" raw
| mvexpand raw
| rename raw as _raw
| extract
| eval type=""customimage""
| eval color=""white"", linkcolor=""black"",nodeTextColor=""#000000""
| eval customurl=""/static/app/network-diagram-viz/customimages/"" . replace(from,""[0-9]"","""") . "".png""
| eval nodetext=if(from=""network-diagram-viz"","" "",nodetext)



      
 |  append [ 
 |  makeresults 
 |  eval raw=""from=\""user\"", x=\""-597\"", y=\""-199\"" ### from=\""website\"", x=\""-379\"", y=\""-254\"" ### from=\""api\"", x=\""-368\"", y=\""-106\"" ### from=\""firewall\"", x=\""-176\"", y=\""-160\"" ### from=\""splunk\"", x=\""-176\"", y=\""-63\"" ### from=\""cart\"", x=\""-179\"", y=\""-278\"" ### from=\""router\"", x=\""30\"", y=\""-148\"" ### from=\""server1\"", x=\""165\"", y=\""-260\"" ### from=\""server2\"", x=\""168\"", y=\""-176\"" ### from=\""server3\"", x=\""171\"", y=\""-98\"" ### from=\""voip\"", x=\""175\"", y=\""-23\"" ### from=\""phone\"", x=\""370\"", y=\""-28\"" ### from=\""network-diagram-viz\"", x=\""-489\"", y=\""-346\""""
 |  makemv delim=""###"" raw 
 |  mvexpand raw 
 |  rename raw  as  _raw 
 |  extract 
 |  table from, x, y]",,,"custom_image_examples"
tbd,,,,,,"network-diagram-viz",,"|makeresults
| eval raw=""from=\""user\"",nodetext=\""User\"",to=\""website\"" ###from=\""user\"",to=\""api\""###from=\""api\"",nodetext=\""API Calls\"",to=\""firewall\""###from=\""firewall\"", to=\""splunk\""###from=\""splunk\"",nodetext=\""Splunk Forwarder\""###from=\""website\"",nodetext=\""Website\"", to=\""cart\""###from=\""cart\"",nodetext=\""Cart\""###from=\""website\"",to=\""firewall\""###from=\""firewall\"",nodetext=\""Firewall\"", to=\""router\""###from=\""router\"",nodetext=\""Router\"", to=\""server1\""###from=\""server1\"",nodetext=\""Host 1\""###from=\""server2\"",nodetext=\""Host 2\""###from=\""server3\"",nodetext=\""Host 3\""###from=\""router\"", to=\""server2\""###from=\""router\"", to=\""server3\""###from=\""router\"", to=\""voip\""###from=\""voip\"",nodetext=\""VOIP\""###from=\""voip\"", to=\""phone\""###from=\""phone\"",nodetext=\""VOIP Phone\""### from=\""network-diagram-viz\"", value=\""Network Diagram Viz\"" nodetext=\""   \""""
|makemv delim=""###"" raw
| mvexpand raw
| rename raw as _raw
| extract
| eval customurl=""/static/app/network-diagram-viz/customimages/"" . replace(from,""[0-9]"","""") . "".png""
| eval nodetext=if(from=""network-diagram-viz"","" "",nodetext)
| eval color = random()%4 

| eval type=if(from=""network-diagram-viz"" OR from=""splunk"",""customimage"",""customimagecircular"")

 |  append [ 
 |  makeresults 
 |  eval raw=""from=\""user\"", x=\""-671\"", y=\""-201\"" ### from=\""website\"", x=\""-379\"", y=\""-254\"" ### from=\""api\"", x=\""-368\"", y=\""-106\"" ### from=\""firewall\"", x=\""-176\"", y=\""-160\"" ### from=\""splunk\"", x=\""-171\"", y=\""-39\"" ### from=\""cart\"", x=\""-184\"", y=\""-337\"" ### from=\""router\"", x=\""30\"", y=\""-148\"" ### from=\""server1\"", x=\""174\"", y=\""-328\"" ### from=\""server2\"", x=\""176\"", y=\""-221\"" ### from=\""server3\"", x=\""182\"", y=\""-127\"" ### from=\""voip\"", x=\""176\"", y=\""-34\"" ### from=\""phone\"", x=\""391\"", y=\""-68\"" ### from=\""network-diagram-viz\"", x=\""-583\"", y=\""-373\""""
 |  makemv delim=""###"" raw 
 |  mvexpand raw 
 |  rename raw  as  _raw 
 |  extract 
 |  table from, x, y]
 
 | eval color=case(color=0,""red"",color=1,""yellow"",color=2,""blue"",color=3,""green"")",,,"custom_image_examples"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by $type$",,,"custom_init_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by $type$",,,"custom_init_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by $type$",,,"custom_init_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by $type$",,,"custom_init_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1 | table host",,,"custom_layout_dark"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"custom_layout_dark"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd_access | head 1000 | table _time method uri_path clientip
                ",,"sourcetype=splunkd_access","custom_layout_dark"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=queue name=parsingqueue
                  | eval fill_perc=round((current_size_kb/max_size_kb)*100,2)
                  | stats max(fill_perc) as count
                  | eval count = round(count,0) . ""%""","source=*metrics.log",,"custom_layout_overlay_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=queue name=aggqueue
                  | eval fill_perc=round((current_size_kb/max_size_kb)*100,2)
                  | stats max(fill_perc) as count
                  | eval count = round(count,0) . ""%""","source=*metrics.log",,"custom_layout_overlay_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=queue name=typingqueue
                  | eval fill_perc=round((current_size_kb/max_size_kb)*100,2)
                  | stats max(fill_perc) as count
                  | eval count = round(count,0) . ""%""","source=*metrics.log",,"custom_layout_overlay_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=queue name=indexqueue
                  | eval fill_perc=round((current_size_kb/max_size_kb)*100,2)
                  | stats max(fill_perc) as count
                  | eval count = round(count,0) . ""%""","source=*metrics.log",,"custom_layout_overlay_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | stats count",,"sourcetype=splunkd","custom_layout_panel_width"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=*access | stats count",,"sourcetype=*access","custom_layout_panel_width"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"custom_layout_panel_width"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"custom_layout_panel_width"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd component=Metrics group=search_concurrency
                    | eval user=coalesce(user, ""system total"")
                    | bucket _time span=1h
                    | stats avg(active_hist_searches) as active_hist_searches avg(active_realtime_searches) as active_realtime_searches by _time,user
                ",,"sourcetype=splunkd","custom_table_cell_highlighting"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd component=Metrics group=search_concurrency
                    | eval user=coalesce(user, ""system total"")
                    | bucket _time span=1h
                    | stats avg(active_hist_searches) as active_hist_searches avg(active_realtime_searches) as active_realtime_searches by _time,user
                ",,"sourcetype=splunkd","custom_table_column_width"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype source",,,"custom_table_data_bar"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype,source,host",,,"custom_table_icon_set_inline"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                    index=_internal
                    | stats count by sourcetype,source,host
                    | rangemap field=count low=0-100 elevated=101-1000 default=severe
                ",,,"custom_table_icon_set_rangemap"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"custom_table_row_expansion"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd component=Metrics group=search_concurrency
                    | eval user=coalesce(user, ""system total"")
                    | bucket _time span=1h
                    | stats avg(active_hist_searches) as active_hist_searches avg(active_realtime_searches) as active_realtime_searches by _time,user
                ",,"sourcetype=splunkd","custom_table_row_highlighting"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"custom_token_links"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"custom_token_links"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by user",,,"custom_token_viewer"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$| stats count by sourcetype",,,"custom_token_viewer"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                    index=_internal user=$username$ sourcetype=$source$
                    | head 1000
                    | table _time, user, sourcetype, _raw
                ",,,"custom_token_viewer"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=pipeline|stats max(cpu_seconds) as cpu_seconds by processor","source=*metrics.log",,"custom_viz_base_search"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=pipeline | stats max(cpu_seconds) as cpu_seconds by processor","source=*metrics.log",,"custom_viz_base_search"
tbd,,,,"inputlookup sales_data.csv","lookup sales_goals_by_metric.csv","simple_xml_examples",,"
            | inputlookup sales_data.csv | stats sum(value) by metric 
            | lookup sales_goals_by_metric.csv metric | fields metric sum(value) range_low range_med range_high
          ",,,"custom_viz_bullet_graph"
tbd,,,,"inputlookup sales_data.csv","lookup sales_goals_by_metric.csv","simple_xml_examples",,"
            | inputlookup sales_data.csv | stats sum(value) by metric 
            | lookup sales_goals_by_metric.csv metric | fields metric sum(value) range_low range_med range_high target
          ",,,"custom_viz_bullet_graph"
tbd,,,,"inputlookup sales_data.csv","lookup sales_goals_by_metric.csv","simple_xml_examples",,"
            | inputlookup sales_data.csv | stats sum(value) by metric 
            | lookup sales_goals_by_metric.csv metric | fields metric sum(value) range_low range_med range_high target
          ",,,"custom_viz_bullet_graph"
tbd,,,,"inputlookup sales_data.csv","lookup sales_goals_by_metric.csv","simple_xml_examples",,"
            | inputlookup sales_data.csv | stats sum(value) by metric 
            | lookup sales_goals_by_metric.csv metric | fields metric sum(value) range_low range_med range_high target
          ",,,"custom_viz_bullet_graph"
tbd,,,,"inputlookup 2015.RideStartsByHour.csv",,"simple_xml_examples",,"|inputlookup 2015.RideStartsByHour.csv | timechart span=1d sum(Casual) sum(Member)",,,"custom_viz_calendar_heatmap"
tbd,,,,"inputlookup 2015.RideStartsByHour.csv",,"simple_xml_examples",,"|inputlookup 2015.RideStartsByHour.csv | timechart span=1d sum(Casual) sum(Member)",,,"custom_viz_calendar_heatmap"
tbd,,,,"inputlookup 2015.RideStartsByHour.csv",,"simple_xml_examples",,"|inputlookup 2015.RideStartsByHour.csv | timechart span=1h sum(Casual) sum(Member)",,,"custom_viz_calendar_heatmap"
tbd,,,,"inputlookup 2015.Dec31.Starts12h.csv",,"simple_xml_examples",,"| inputlookup 2015.Dec31.Starts12h.csv | timechart span=1m sum(rides)",,,"custom_viz_calendar_heatmap"
tbd,,,,"inputlookup 2015.RideStartsByHour.csv",,"simple_xml_examples",,"|inputlookup 2015.RideStartsByHour.csv | timechart span=1d sum(Casual) sum(Member)",,,"custom_viz_calendar_heatmap"
tbd,,,,"inputlookup stocks.csv",,"simple_xml_examples",,"| inputlookup stocks.csv | eval _time = strptime(date, ""%Y-%m-%d"")| timechart useother=""f"" span=1d limit=10 latest(open) by ticker_symbol | filldown",,,"custom_viz_horizon_chart"
tbd,,,,"inputlookup stocks.csv",,"simple_xml_examples",,"| inputlookup stocks.csv | eval _time = strptime(date, ""%Y-%m-%d"")| timechart useother=""f"" span=1d limit=10 latest(open) by ticker_symbol | filldown",,,"custom_viz_horizon_chart"
tbd,,,,"inputlookup firewall_data.csv",,"simple_xml_examples",,"| inputlookup firewall_data.csv   | eval _time = timestamp  | eval bytes_out = -bytes_out                                         | timechart span=5s sum(bytes_in) sum(bytes_out) by host      | table _time *host111 *host18 *host19 *host2 *host241 *host248 *host254 *host8",,,"custom_viz_horizon_chart"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 60 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 90 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup horseshoe_data.csv",,"simple_xml_examples",,"| inputlookup horseshoe_data.csv | head 1 | stats count",,,"custom_viz_horseshoe"
tbd,,,,"inputlookup locations.csv",,"simple_xml_examples",,"| inputlookup locations.csv | table _time latitude longitude user | sort -_time",,,"custom_viz_location_tracker"
tbd,,,,"inputlookup locations.csv",,"simple_xml_examples",,"| inputlookup locations.csv | table _time latitude longitude user | sort -_time",,,"custom_viz_location_tracker"
tbd,,,,"inputlookup locations.csv",,"simple_xml_examples",,"| inputlookup locations.csv | eval icon=""ambulance"" | table _time latitude longitude user icon | sort -_time",,,"custom_viz_location_tracker"
tbd,,,,"inputlookup locations.csv",,"simple_xml_examples",,"| inputlookup locations.csv | table _time latitude longitude user | sort -_time",,,"custom_viz_location_tracker"
tbd,,,,"inputlookup locations.csv",,"simple_xml_examples",,"| inputlookup locations.csv | table _time latitude longitude user | sort -_time",,,"custom_viz_location_tracker"
tbd,,,,"inputlookup nutrients.csv",,"simple_xml_examples",,"| inputlookup nutrients.csv | head 1500 | stats count by group calories ""protein (g)"" ""water (g)"" | fields - count",,,"custom_viz_parallel_coordinates"
tbd,,,,"inputlookup nutrients.csv",,"simple_xml_examples",,"| inputlookup nutrients.csv | head 1500 | stats count by calories group ""protein (g)"" ""water (g)"" | fields - count",,,"custom_viz_parallel_coordinates"
tbd,,,,"inputlookup nutrients.csv",,"simple_xml_examples",,"| inputlookup nutrients.csv | head 1500 | stats count by name calories ""protein (g)"" ""water (g)"" | fields - count",,,"custom_viz_parallel_coordinates"
tbd,,,,"inputlookup nutrients.csv",,"simple_xml_examples",,"| inputlookup nutrients.csv | head 2500 | stats count by calories group ""protein (g)"" ""water (g)"" | fields - count",,,"custom_viz_parallel_coordinates"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| stats count by date_hour date_wday",,,"custom_viz_punchcard"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| stats count by date_hour date_wday",,,"custom_viz_punchcard"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| eval duration = duration_ms/60000
| stats count median(duration) by date_hour date_wday",,,"custom_viz_punchcard"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| eval duration = duration_ms/60000
| eval is_member = if(member_type==""Registered"", 1, 0)
| eval is_casual = if(member_type==""Casual"", 1, 0)
| stats avg(duration_ms) sum(is_member) as sum_m, sum(is_casual) as sum_c by date_hour date_wday
| eval prevalent_member_type = if(sum_m &gt; sum_c, ""Member"", ""Casual"")
| fields - sum_m, sum_c",,,"custom_viz_punchcard"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| search member_type=""Registered""
| stats count by date_hour start_station
| sort 400 start_station",,,"custom_viz_punchcard"
tbd,,,,"inputlookup bikeshare.csv",,"simple_xml_examples",,"| inputlookup bikeshare.csv
| search member_type=""Casual""
| stats count by date_hour start_station
| sort 400 start_station",,,"custom_viz_punchcard"
tbd,,,,"inputlookup webstore_requests.csv",,"simple_xml_examples",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path",,,"custom_viz_sankey"
tbd,,,,"inputlookup webstore_requests.csv",,"simple_xml_examples",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path",,,"custom_viz_sankey"
tbd,,,,"inputlookup webstore_requests.csv",,"simple_xml_examples",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path  | where count &gt; 10",,,"custom_viz_sankey"
tbd,,,,"inputlookup status_indicator_data.csv",,"simple_xml_examples",,"| inputlookup status_indicator_data.csv | head 1 | table value icon_value color_value",,,"custom_viz_status_indicator"
tbd,,,,"inputlookup status_indicator_data.csv",,"simple_xml_examples",,"| inputlookup status_indicator_data.csv | eval count=1 | accum count | where count &gt;= 2 | head 1 | table value icon_value color_value",,,"custom_viz_status_indicator"
tbd,,,,"inputlookup status_indicator_data.csv",,"simple_xml_examples",,"| inputlookup status_indicator_data.csv | eval count=1 | accum count | where count &gt;= 3 | head 1 | table value icon_value color_value",,,"custom_viz_status_indicator"
tbd,,,,"inputlookup status_indicator_data.csv",,"simple_xml_examples",,"| inputlookup status_indicator_data.csv | head 1",,,"custom_viz_status_indicator"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=pipeline | stats max(cpu_seconds) as cpu_seconds by
                    processor | sort - cpu_seconds
                ","source=*metrics.log",,"custom_viz_tag_cloud"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=pipeline | stats max(cpu_seconds) as cpu_seconds by processor","source=*metrics.log",,"custom_viz_tag_cloud"
tbd,,,,"inputlookup outages.csv",,"simple_xml_examples",,"| inputlookup outages.csv | where Year = 2006 | head 20 | rename ""Date Event Began"" AS start_date   | rename ""Date of Restoration"" AS end_date   | rename ""Time Event Began"" AS start_time   | rename ""Time of Restoration"" AS end_time   | eval _time = strptime(start_date."" "".start_time, ""%m/%d/%Y %H:%M %p"") | eval end_time = strptime(end_date."" "".end_time, ""%m/%d/%Y %H:%M %p"") | eval duration = (end_time - _time) * 1000 * (random()%5) | eval duration = IF(duration &lt; 86400000, 0, duration) | stats count by _time, duration, ""NERC Region"", ""Event Description"" | table _time ""NERC Region"" duration",,,"custom_viz_timeline"
tbd,,,,"inputlookup outages.csv",,"simple_xml_examples",,"| inputlookup outages.csv | where Year = 2006 | head 20 | rename ""Date Event Began"" AS start_date   | rename ""Date of Restoration"" AS end_date   | rename ""Time Event Began"" AS start_time   | rename ""Time of Restoration"" AS end_time   | eval _time = strptime(start_date."" "".start_time, ""%m/%d/%Y %H:%M %p"") | eval end_time = strptime(end_date."" "".end_time, ""%m/%d/%Y %H:%M %p"") | eval duration = (end_time - _time) * 1000 * (random()%5) | eval duration = IF(duration &lt; 86400000, 0, duration) | stats count by _time, duration, ""NERC Region"", ""Event Description"" | table _time ""NERC Region"" ""Event Description"" duration",,,"custom_viz_timeline"
tbd,,,,"inputlookup outages.csv",,"simple_xml_examples",,"| inputlookup outages.csv | where Year = 2006 | tail 20 | rename ""Date Event Began"" AS start_date   | rename ""Date of Restoration"" AS end_date   | rename ""Time Event Began"" AS start_time   | rename ""Time of Restoration"" AS end_time   | eval _time = strptime(start_date."" "".start_time, ""%m/%d/%Y %H:%M %p"") | eval end_time = strptime(end_date."" "".end_time, ""%m/%d/%Y %H:%M %p"")  | eval duration = (end_time - _time) * 1000 * (random()%5) | eval duration = IF(duration &lt; 86400000, 0, duration) | stats count by _time, duration, ""NERC Region"", ""Number of Customers Affected"" | rename ""Number of Customers Affected"" as num_affected | eval num_affected = abs(num_affected) | table _time ""NERC Region"" num_affected duration | fillnull value=0",,,"custom_viz_timeline"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal source=*metrics.log group=pipeline | stats max(cpu_seconds) as cpu_seconds sum(executes) as executes sum(cumulative_hits) as cumulative_hits by processor","source=*metrics.log",,"custom_viz_tokens"
tbd,,,,"inputlookup treemap_dataset.csv",,"simple_xml_examples",,"| inputlookup treemap_dataset.csv | stats sum(amount) as Dollars by card_provider, response_code",,,"custom_viz_treemap"
tbd,,,,"inputlookup splunk_files.csv",,"simple_xml_examples",,"| inputlookup splunk_files.csv  | rex field=path ""\./(?&lt;level1&gt;[A-Za-z0-9]*)/(?&lt;level2&gt;[A-Za-z0-9]*)""  | rex field=path "".+\.(?&lt;extension&gt;[A-Za-z0-9]+)$"" | fillnull value=""Other"" extension| fillnull value=""-"" level2  | stats sum(size), count as size by level1, level2",,,"custom_viz_treemap"
tbd,,,,"inputlookup splunk_files.csv",,"simple_xml_examples",,"| inputlookup splunk_files.csv  | rex field=path ""\./(?&lt;level1&gt;[A-Za-z0-9]*)/(?&lt;level2&gt;[A-Za-z0-9]*)""  | rex field=path "".+\.(?&lt;extension&gt;[A-Za-z0-9]+)$"" | fillnull value=""Other"" extension| fillnull value=""-"" level2  | stats sum(size), count as size by level1, level2",,,"custom_viz_treemap"
tbd,,,,"inputlookup splunk_files.csv
inputlookup splunk_files.csv",,"simple_xml_examples",,"| inputlookup splunk_files.csv   | rex field=path ""\./(?&lt;level1&gt;[A-Za-z0-9]*)/(?&lt;level2&gt;[A-Za-z0-9]*)""  | rex field=path "".+\.(?&lt;extension&gt;[A-Za-z][A-Za-z0-9]+)$""  | join extension type=left [| inputlookup splunk_files.csv  | rex field=path ""\./(?&lt;level1&gt;[A-Za-z0-9]*)/(?&lt;level2&gt;[A-Za-z0-9]*)""  | rex field=path "".+\.(?&lt;extension&gt;[A-Za-z][A-Za-z0-9]+)$"" | top $num_categories$ extension | eval is_top_extension=""true""| fields extension,is_top_extension]  | eval extension=if(is_top_extension==""true"", extension, ""OTHER"")  | stats $agg_fn$ by level1, level2, extension",,,"custom_viz_treemap"
tbd,,,,"inputlookup savedsearch_details.csv
inputlookup dashboard_details.csv
inputlookup mitre_not_possible_ttps.csv",,"CyberSentry_Training",,"| inputlookup savedsearch_details.csv 
| eval foo = replace(mitre, "" "", """")
| eval foo = replace(foo, ""\["", """")
| eval foo = replace(foo, ""]"", """")
| eval bar = split(foo, "","")
| mvexpand bar

| stats values(title) as description count by bar
| rename bar as id
| append [ | inputlookup dashboard_details.csv
| eval foo = replace(mitre, "" "", """")
| eval foo = replace(foo, ""\["", """")
| eval foo = replace(foo, ""]"", """")
| eval bar = split(foo, "","")
| mvexpand bar

| stats values(id) as description count by bar
| rename bar as id

| table id, count, description]
| append [ | inputlookup mitre_not_possible_ttps.csv | rename mitre as id | table id, count, description]
| table id, count, description",,,"dashboard_saved_searches_mitre_mapping"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | search search_group=dmc_group_indexer
        ",,,"data_quality"
tbd,,,"index=*
index=_*",,,"splunk_monitoring_console",,"
      (index=* OR index=_*) host=$host$ source=$source$ sourcetype=$sourcetype$ _index_earliest=$time.earliest$ _index_latest=$latest_time$
      | search splunk_server=*
      | eval event_size = len(_raw)
      | eval time_disparity = (_indextime - _time) / 36000
      | stats count by linecount, event_size, time_disparity
    ",,,"data_quality"
tbd,,"host = if",,,,"splunk_monitoring_console",,"
            <![CDATA[
            `dmc_set_index_internal` search_group=dmc_group_indexer splunk_server=$splunk_server$ source=*splunkd.log*
            (component=AggregatorMiningProcessor OR component=LineBreakingProcessor OR component=DateParserVerbose OR component=MetricSchemaProcessor OR component=MetricsProcessor)
            (log_level=WARN OR log_level=ERROR)
            | rex field=event_message ""Context: source(::|=)(?<context_source>[^\|]*?)\|host(::|=)(?<context_host>[^\|]*?)\|(?<context_sourcetype>[^\|]*?)\|""
            | eval data_source = if(isnull(data_source) AND isnotnull(context_source), context_source, data_source)
            | eval data_host = if(isnull(data_host) AND isnotnull(context_host), context_host, data_host)
            | eval data_sourcetype = if(isnull(data_sourcetype) AND isnotnull(context_sourcetype), context_sourcetype, data_sourcetype)
            | stats
              count(eval(component==""LineBreakingProcessor"" OR component==""DateParserVerbose"" OR component==""AggregatorMiningProcessor"" OR component==""MetricSchemaProcessor"" OR component==""MetricsProcessor"")) as total_issues
              dc(data_host) AS ""Host Count""
              dc(data_source) AS ""Source Count""
              count(eval(component==""LineBreakingProcessor"")) AS ""Line Breaking Issues""
              count(eval(component==""DateParserVerbose"")) AS ""Timestamp Parsing Issues""
              count(eval(component==""AggregatorMiningProcessor"")) AS ""Aggregation Issues""
              count(eval(component==""MetricSchemaProcessor"")) AS ""Metric Schema Issues""
              count(eval(component==""MetricsProcessor"")) AS ""Metrics Issues"" by data_sourcetype
            | sort - total_issues
            | rename
              data_sourcetype as Sourcetype
              total_issues as ""Total Issues""
            ]]>
          ","source=*splunkd.log*
source = if","sourcetype = if","data_quality"
tbd,,"host = if",,,,"splunk_monitoring_console",,"
            <![CDATA[
            `dmc_set_index_internal` search_group=dmc_group_indexer splunk_server=$splunk_server$ source=*splunkd.log*
            (component=AggregatorMiningProcessor OR component=LineBreakingProcessor OR component=DateParserVerbose OR component=MetricSchemaProcessor OR component=MetricsProcessor)
            (log_level=WARN OR log_level=ERROR)
            | rex field=event_message ""Context: source(::|=)(?<context_source>[^\|]*?)\|host(::|=)(?<context_host>[^\|]*?)\|(?<context_sourcetype>[^\|]*?)\|""
            | eval data_source = if(isnull(data_source) AND isnotnull(context_source), context_source, data_source)
            | eval data_host = if(isnull(data_host) AND isnotnull(context_host), context_host, data_host)
            | eval data_sourcetype = if(isnull(data_sourcetype) AND isnotnull(context_sourcetype), context_sourcetype, data_sourcetype)
            | search data_sourcetype=$sourcetype$
            | stats
              count(eval(component==""LineBreakingProcessor"" OR component==""DateParserVerbose"" OR component==""AggregatorMiningProcessor"" OR component==""MetricSchemaProcessor"" OR component=""MetricsProcessor"")) as total_issues
              count(eval(component==""LineBreakingProcessor"")) AS ""Line Breaking Issues""
              count(eval(component==""DateParserVerbose"")) AS ""Timestamp Parsing Issues""
              count(eval(component==""AggregatorMiningProcessor"")) AS ""Aggregation Issues""
              count(eval(component==""MetricSchemaProcessor"")) AS ""Metric Schema Issues""
              count(eval(component==""MetricsProcessor"")) AS ""Metrics Issues"" by data_host, data_source
            | sort - total_issues
            | rename
              data_host as Host
              data_source as Source
              total_issues as ""Total Issues""
            ]]>
          ","source=*splunkd.log*
source = if","sourcetype = if","data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats sum(count) as ""Event Count"" by linecount
            | rename linecount as ""Line Count""
          ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | bin event_size span=log2
            | stats sum(count) as ""Event Count"" by event_size
            | rename event_size as ""Event Size (bytes)""
            ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | bin time_disparity span=1 start=-24 end=-24
            | stats sum(count) as ""Event Count"" by time_disparity
            | rename time_disparity as ""Observed Latency (hours)""
          ",,,"data_quality"
tbd,,,,,,"Splunk_SA_CIM",,"sort 18 - size | table datamodel,size(MB)",,,"datamodel_audit"
tbd,,,,,,"Splunk_SA_CIM",,"sort 18 - runDuration | table datamodel,runDuration",,,"datamodel_audit"
tbd,,,,,,"Splunk_SA_CIM",,"sort 100 + datamodel | fieldformat earliest=strftime(earliest, ""%m/%d/%Y %H:%M:%S"") | fieldformat latest=strftime(latest, ""%m/%d/%Y %H:%M:%S"") | table datamodel,app,cron,retention(days),earliest,latest,is_inprogress,complete(%),size(MB),runDuration(s),last_error",,,"datamodel_audit"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role | h$)` | search search_group!=""dmc_group_*""",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/search/distributed/peers
          | join type=outer title, splunk_server [
          | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/distributed
          | where title != ""per_searchhead_metrics"" AND title != ""window_metrics""
          ]
          | fields status, health_status, splunk_server, peerName
        ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            where status!=""Up""
            | stats dc(splunk_server) as down_count, values(splunk_server) as splunk_server
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            where health_status!=""Healthy""
            | stats dc(splunk_server) as unhealthy_count, values(splunk_server) as splunk_server
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/distributed/window_metrics
            | eval avg_baseline_mb = if(isNotNull(average_baseline_file_size), round(average_baseline_file_size /1024 / 1024, 2), ""Unknown"")
            | eval avg_sec = round(average_msecs / 1000, 2)
            | eval bundle_file_count = if(isNotNull(bundle_file_count), bundle_file_count, count)
            | fields splunk_server, bundle_file_count, avg_baseline_mb, avg_sec
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/search/distributed/peers
              | stats count as peer_count by splunk_server
            ]
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/dispatch/Bundle_Directory_Reaper
              | eval Bundle_Directory_Reaper_Average_Time(ms) = round('Bundle_Directory_Reaper_Average_Time(ms)')
              | eval Bundle_Directory_Reaper_Max_Time(ms) = round('Bundle_Directory_Reaper_Max_Time(ms)')
              | fields splunk_server, ""Bundle_Directory_Reaper_Average_Time(ms)"", ""Bundle_Directory_Reaper_Max_Time(ms)""
            ]
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/dispatch/Dispatch_Directory_Reaper
              | eval Dispatch_Directory_Reaper_Average_Time(ms) = round('Dispatch_Directory_Reaper_Average_Time(ms)')
              | eval Dispatch_Directory_Reaper_Max_Time(ms) = round('Dispatch_Directory_Reaper_Max_Time(ms)')
              | fields splunk_server, ""Dispatch_Directory_Reaper_Average_Time(ms)"", ""Dispatch_Directory_Reaper_Max_Time(ms)""
            ]
            | $instance_fields_filter$
            | rename splunk_server as Instance, peer_count as ""Peer Count"", bundle_file_count as ""Knowledge Bundle Replication Count"", avg_baseline_mb as ""Average Size of Baseline Knowledge Bundle (MB)"", avg_sec as ""Average Time Spent on Knowledge Bundle Replication (sec)"", ""Bundle_Directory_Reaper_Average_Time(ms)"" as ""Average Time to Reap Knowledge Bundle Directory (ms)"", ""Bundle_Directory_Reaper_Max_Time(ms)"" as ""Max Time to Reap Knowledge Bundle Directory (ms)"", ""Dispatch_Directory_Reaper_Average_Time(ms)"" as ""Average Time to Reap Dispatch Directory (ms)"", ""Dispatch_Directory_Reaper_Max_Time(ms)"" as ""Max Time to Reap Dispatch Directory (ms)""
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat`
              avg(get_auth_$peer_heartbeat_no_split_agg$_ms) as get_auth_$peer_heartbeat_no_split_agg$_ms
              avg(get_bundleList_$peer_heartbeat_no_split_agg$_ms) as get_bundleList_$peer_heartbeat_no_split_agg$_ms
              avg(get_serverInfo_$peer_heartbeat_no_split_agg$_ms) as get_serverInfo_$peer_heartbeat_no_split_agg$_ms
            | eval get_auth_$peer_heartbeat_no_split_agg$_ms = round(get_auth_$peer_heartbeat_no_split_agg$_ms)
            | eval get_bundleList_$peer_heartbeat_no_split_agg$_ms = round(get_bundleList_$peer_heartbeat_no_split_agg$_ms)
            | eval get_serverInfo_$peer_heartbeat_no_split_agg$_ms = round(get_serverInfo_$peer_heartbeat_no_split_agg$_ms)
            | rename get_auth_$peer_heartbeat_no_split_agg$_ms as ""Get Authentication"",
                get_bundleList_$peer_heartbeat_no_split_agg$_ms as ""Get Bundle List"",
                get_serverInfo_$peer_heartbeat_no_split_agg$_ms as ""Get Peer Info""
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | where Peer_Count > $peer_count_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_count_agg$(Peer_Count) as peer_count by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | where get_$peer_heartbeat_metric$_$peer_heartbeat_split_agg$_ms > $peer_heartbeat_metric_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_heartbeat_split_agg$(get_$peer_heartbeat_metric$_$peer_heartbeat_split_agg$_ms) as metric by host
            | eval metric = round(metric)
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
          | `dmc_timechart_for_metrics_log` sum(total_count) as total_count,
            sum(success_count) as success_count, sum(failure_count) as failure_count,
            sum(baseline_count) as baseline_count, sum(delta_count) as delta_count,
            sum(already_present_count) as already_present_count, sum(unattempted_count) as unattempted_count,
            sum(total_msec_spent), as total_msec_spent,
            sum(baseline_msec_spent) as baseline_msec_spent, sum(delta_msec_spent) as delta_msec_spent,
            sum(total_bytes) as total_bytes,
            sum(baseline_bytes) as baseline_bytes, sum(delta_bytes) as delta_bytes
        ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, baseline_count, delta_count
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_size_mb = round(baseline_bytes / 1024 / 1024, 2)
            | eval delta_size_mb = round(delta_bytes / 1024 / 1024, 2)
            | fields _time, baseline_size_mb, delta_size_mb
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_sec_spent = round(baseline_msec_spent / 1000, 2)
            | eval delta_sec_spent = round(delta_msec_spent / 1000, 2)
            | fields _time, baseline_sec_spent, delta_sec_spent
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_count) as count by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | eval $bundle_rep_mode$_mb = round($bundle_rep_mode$_bytes / 1024 / 1024, 2)
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_mb) as size by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | eval $bundle_rep_mode$_sec_spent = round($bundle_rep_mode$_msec_spent / 1000, 2)
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_sec_spent) as msec_spent by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(dispatch_dir_reaper_mean_ms) as dispatch_dir_reaper_mean_ms, max(dispatch_dir_reaper_max_ms) as dispatch_dir_reaper_max_ms
            | eval dispatch_dir_reaper_mean_ms = round(dispatch_dir_reaper_mean_ms)
            | eval dispatch_dir_reaper_max_ms = round(dispatch_dir_reaper_max_ms)
            | rename dispatch_dir_reaper_mean_ms as Average, dispatch_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(bundle_dir_reaper_mean_ms) as bundle_dir_reaper_mean_ms, max(bundle_dir_reaper_max_ms) as bundle_dir_reaper_max_ms
            | eval bundle_dir_reaper_mean_ms = round(bundle_dir_reaper_mean_ms)
            | eval bundle_dir_reaper_max_ms = round(bundle_dir_reaper_max_ms)
            | rename bundle_dir_reaper_mean_ms as Average, bundle_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper search_group=$role | h$ search_group=""$group$""
            | where dispatch_dir_reaper_$reaper_agg$_ms > $dispatch_dir_filter$
            | eval dispatch_dir_reaper_$reaper_agg$_ms = round(dispatch_dir_reaper_$reaper_agg$_ms)
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $reaper_agg$(dispatch_dir_reaper_$reaper_agg$_ms) as dispatch_dir_reaper_ms by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper search_group=$role | h$ search_group=""$group$""
            | where bundle_dir_reaper_$reaper_agg$_ms > $bundle_dir_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $reaper_agg$(bundle_dir_reaper_$reaper_agg$_ms) as bundle_dir_reaper_$reaper_agg$_ms by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | where search_group=""$role$""
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/search/distributed/peers
      | join type=outer title [
        | rest splunk_server=$splunk_server$ /services/server/introspection/search/distributed
        | where title != ""per_searchhead_metrics"" AND title != ""window_metrics""
        | fields title, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures
        | eval get_auth_max_ms = round(get_auth_max_ms, 0)
        | eval get_auth_mean_ms = round(get_auth_mean_ms, 0)
        | eval get_bundleList_max_ms = round(get_bundleList_max_ms, 0)
        | eval get_bundleList_mean_ms = round(get_bundleList_mean_ms, 0)
        | eval get_serverInfo_max_ms = round(get_serverInfo_max_ms, 0)
        | eval get_serverInfo_mean_ms = round(get_serverInfo_mean_ms, 0)
      ]
    ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ host=$host$
      | `dmc_timechart_for_metrics_log` sum(total_count) as total_count, sum(success_count) as success_count, sum(failure_count) as failure_count, sum(baseline_count) as baseline_count, sum(delta_count) as delta_count, sum(already_present_count) as already_present_count, sum(unattempted_count) as unattempted_count, sum(total_msec_spent), as total_msec_spent, sum(baseline_msec_spent) as baseline_msec_spent, sum(delta_msec_spent) as delta_msec_spent, sum(total_bytes) as total_bytes, sum(baseline_bytes) as baseline_bytes, sum(delta_bytes) as delta_bytes
    ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where status!=""Up""
            | stats count as down_count, values(peerName) as peer_name
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where health_status!=""Healthy""
            | stats count as unhealthy_count, values(peerName) as peer_name
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval mem_gb = round(physicalMemoryMB / 1024, 2)
            | eval disabled = if(disabled == 1, ""yes"", ""no"")
            | `dmc_time_format(startup_time)`
            | fields peerName, status, status_details, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures, replicationStatus, startup_time, disabled
            | rename peerName as Peer, status as Status, status_details as ""Status Details"", get_auth_max_ms as ""Authentication Max Time (ms)"", get_auth_mean_ms as ""Authentication Mean Time (ms)"", get_bundleList_max_ms as ""Get Bundle List Max Time (ms)"", get_bundleList_mean_ms as ""Get Bundle List Mean Time (ms)"", get_serverInfo_max_ms as ""Get Peer Info Max Time (ms)"", get_serverInfo_mean_ms as ""Get Peer Info Mean Time (ms)"", health_status as ""Health Condition"", health_check_failures as ""Health Details"", replicationStatus as ""Replication Status"", startup_time as ""Startup Time"", disabled as Disabled, version as Version, numberOfCores as ""CPU Cores"", mem_gb as ""Physical Memory (GB)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search peerName = $peer_drilldown$
            | fields - _timediff, peerName, status, status_details, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures, replicationStatus, startup_time, disabled, server_roles, peerType, title, author, published, updated, remote_session, splunk_server, eai*
            | transpose
            | rename column as Property, ""row 1"" as Value
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/distributed/window_metrics
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval bundle_file_count = if(isNotNull(bundle_file_count), bundle_file_count, count)
            | fields bundle_file_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval avg_mb = if(isNotNull(average_baseline_file_size), round(average_baseline_file_size /1024 / 1024, 2), ""Unknown"")
            | fields avg_mb
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval avg_sec = average_msecs / 1000
            | fields avg_sec
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/dispatch/Bundle_Directory_Reaper
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Bundle_Directory_Reaper_Average_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Bundle_Directory_Reaper_Max_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/dispatch/Dispatch_Directory_Reaper
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Dispatch_Directory_Reaper_Average_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Dispatch_Directory_Reaper_Max_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat host=$host$
          | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_heartbeat_agg$(Peer_Count) as peer_count,
              $peer_heartbeat_agg$(get_auth_$peer_heartbeat_agg$_ms) as get_auth_$peer_heartbeat_agg$_ms,
              $peer_heartbeat_agg$(get_bundleList_$peer_heartbeat_agg$_ms) as get_bundleList_$peer_heartbeat_agg$_ms,
              $peer_heartbeat_agg$(get_serverInfo_$peer_heartbeat_agg$_ms) as get_serverInfo_$peer_heartbeat_agg$_ms,
          | eval get_auth_$peer_heartbeat_agg$_ms = round(get_auth_$peer_heartbeat_agg$_ms)
          | eval get_bundleList_$peer_heartbeat_agg$_ms = round(get_bundleList_$peer_heartbeat_agg$_ms)
          | eval get_serverInfo_$peer_heartbeat_agg$_ms = round(get_serverInfo_$peer_heartbeat_agg$_ms)
        ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, peer_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, get_auth_$peer_heartbeat_agg$_ms, get_bundleList_$peer_heartbeat_agg$_ms, get_serverInfo_$peer_heartbeat_agg$_ms
            | rename get_auth_$peer_heartbeat_agg$_ms as ""Get Authentication"",
                get_bundleList_$peer_heartbeat_agg$_ms as ""Get Bundle List"",
                get_serverInfo_$peer_heartbeat_agg$_ms as ""Get Peer Info""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, baseline_count, delta_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_size_mb = round(baseline_bytes / 1024 / 1024, 2)
            | eval delta_size_mb = round(delta_bytes / 1024 / 1024, 2)
            | fields _time, baseline_size_mb, delta_size_mb
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_sec_spent = round(baseline_msec_spent / 1000, 2)
            | eval delta_sec_spent = round(delta_msec_spent / 1000, 2)
            | fields _time, baseline_sec_spent, delta_sec_spent
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper host=$host$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(dispatch_dir_reaper_mean_ms) as dispatch_dir_reaper_mean_ms, max(dispatch_dir_reaper_max_ms) as dispatch_dir_reaper_max_ms
            | eval dispatch_dir_reaper_mean_ms = round(dispatch_dir_reaper_mean_ms)
            | eval dispatch_dir_reaper_max_ms = round(dispatch_dir_reaper_max_ms)
            | rename dispatch_dir_reaper_mean_ms as Average, dispatch_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper host=$host$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(bundle_dir_reaper_mean_ms) as bundle_dir_reaper_mean_ms, max(bundle_dir_reaper_max_ms) as bundle_dir_reaper_max_ms
            | eval bundle_dir_reaper_mean_ms = round(bundle_dir_reaper_mean_ms)
            | eval bundle_dir_reaper_max_ms = round(bundle_dir_reaper_max_ms)
            | rename bundle_dir_reaper_mean_ms as Average, bundle_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_dns src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$""

| bin span=5m _time 

| stats  count as eventCount by _time  qtype_name
| eventstats perc$perc$(eventCount) as perc95 by  qtype_name
| eval diff = eventCount-perc95
| chart avg(diff) by _time, qtype_name",,"sourcetype=corelight_dns","dns_log_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_dns src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$"" 
| stats count earliest(_time) as firstSeenTime by dest_ip
| iplocation dest_ip 
| eval _time = firstSeenTime
| table dest_ip, count, _time, Country, City,",,"sourcetype=corelight_dns","dns_log_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_dns src_ip=""192.168.1.156"" AND dest_ip=""*"" 

| stats count as eventCount earliest(_time) as firstSeenTime by query
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_dns","dns_log_anomalies"
tbd,,,,"inputlookup analytics_info",,"lame_analytic_documentation","outputlookup analytics_info","| inputlookup analytics_info | search _key = $the_key$
| eval description = ""$description$""
| outputlookup analytics_info append=true",,,"document_analytics"
tbd,,,,"inputlookup index_info",,"lame_analytic_documentation","outputlookup index_info","| inputlookup index_info | search _key = $the_key$
| eval description = ""description""
| outputlookup index_info append=true",,,"document_indexes"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_analytics"
tbd,,,,,,"CyberSentry_Training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_analytics"
tbd,,,,,,"CyberSentry_Training","outputlookup Sourcetype_Analytics","| makeresults
| eval index = ""$idx$""
| eval sourcetype = ""$src_type$""
| eval metric_description = ""$q_description$""
| eval metric_query = ""$query$""
| table index, sourcetype, metric_description, metric_query
| where metric_description != ""Change Me"" 
| outputlookup Sourcetype_Analytics append=true",,,"document_sourcetype_analytics"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_analytics"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_analytics"
tbd,,,,,,"lame_training","outputlookup Sourcetype_Analytics","| makeresults
| eval sourcetype = ""$src_type$""
| eval metric_description = ""$q_description$""
| eval metric_query = ""$query$""
| table sourcetype, metric_description, metric_query
| where metric_description != ""Change Me"" 
| outputlookup Sourcetype_Analytics append=true",,,"document_sourcetype_analytics"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training",,"index=$idx$ sourcetype=$src_type$ | head 100
| fieldsummary | fields field",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training","outputlookup SourcetypeInfo","| makeresults
| eval foo = ""bar""
| eval sourcetype = ""$src_type$""
| eval rationale = ""$rationale$""
| eval fieldname = ""$fld$""
| eval fieldvalue = ""$fldvalue$""
| table foo  sourcetype, rationale, fieldname, fieldvalue
| where rationale != ""Change Me""

| outputlookup SourcetypeInfo append=true",,,"document_sourcetype_fields"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_fields"
tbd,,,,,,"CyberSentry_Training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_fields"
tbd,,,,,,"CyberSentry_Training",,"index=$idx$ sourcetype=$src_type$ | head 100
| fieldsummary | fields field",,,"document_sourcetype_fields"
tbd,,,,,,"CyberSentry_Training","outputlookup SourcetypeInfo","| makeresults
| eval index = ""$idx$""
| eval sourcetype = ""$src_type$""
| eval rationale = ""$rationale$""
| eval fieldname = ""$fld$""
| eval fieldvalue = ""$fldvalue$""
| table index,  sourcetype, rationale, fieldname, fieldvalue
| where rationale != ""Change Me""

| outputlookup SourcetypeInfo append=true",,,"document_sourcetype_fields"
tbd,,,,"inputlookup sourcetype_info",,"lame_analytic_documentation","outputlookup sourcetype_info","| inputlookup sourcetype_info | search _key = $the_key$
| eval description = ""description""
| outputlookup sourcetype_info append=true",,,"document_sourcetypes"
tbd,,,,"inputlookup source_info",,"lame_analytic_documentation","outputlookup source_info","| inputlookup source_info | search _key = $the_key$
| eval description = ""description""
| outputlookup source_info append=true",,,"document_summary_indexes"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=10 
| streamstats count as from 
| eval type=case (from==1, ""usergroups"", from==2, ""users"", from==3, ""server"", from==4, ""database"",from==5, ""desktop"",from==6, ""laptop"",from==7, ""printer"",from==8, ""harddrive"",from==9,""wifi"",from==10, ""folder"") 
| eval to=case(from==2, ""usergroups"", from==3, ""users"", from==4, ""server"", from==5, ""database"",from==6, ""desktop"",from==7, ""laptop"",from==8, ""printer"",from==9, ""harddrive"", from==10, ""wifi"") 
| eval from=type 
| eval color = random()%4 
| eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"") 
| eval value = ""NODE_ID_"" . tostring(random()%40)
| eval linkText=random()%500+1
| table from, to, value, type, color, linkText",,,"drill_down_config"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=10 
                 | streamstats count as from 
                 | eval type=case (from==1, ""usergroups"", from==2, ""users"", from==3, ""server"", from==4, ""database"",from==5, ""desktop"",from==6, ""laptop"",from==7, ""printer"",from==8, ""harddrive"",from==9,""wifi"",from==10, ""folder"") 
                 | eval to=case(from==2, ""usergroups"", from==3, ""users"", from==4, ""server"", from==5, ""database"",from==6, ""desktop"",from==7, ""laptop"",from==8, ""printer"",from==9, ""harddrive"", from==10, ""wifi"") 
                 | eval from=type, color = random()%4, eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red""), value = ""NODE_ID_"" . tostring(random()%40), linkText=random()%500+1
                 | table from, to, value, type, color, linkText",,,"drill_down_config"
tbd,,"host=Load
host=from",,,,"network-diagram-viz",,"| gentimes start=-7 end=0 increment=8h 
| eval host=""Load Balancer,Web 1,Web 2,Web 3,App Server 1,App Server 2,Database Server"" 
| makemv delim="","" host 
| mvexpand host 
| eval Mem=random()%20 + 60 
| eval CPU=random()%20 + 60 
| rename starttime as _time 
| fields _time, CPU,Mem,host 
| append 
    [| makeresults count=12 
    | eval redServer=random()%7+1 
    | eval redServer=case(redServer=1,""Load Balancer"",redServer=2,""Web 1"",redServer=3,""Web 2"",redServer=4,""Web 3"",redServer=5,""App Server 1"",redServer=6,""App Server 2"",redServer=7,""Database Server"") 
    | eventstats first(redServer) as redServer 
    | streamstats count as id 
    | eval from=case(id=1,""Load Balancer"",id=2,""Load Balancer"",id=3,""Load Balancer"", id=4,""Web 1"",id=5,""Web 1"", id=6, ""Web 2"",id=7,""Web 2"", id=8,""Web 3"",id=9,""Web 3"",id=10,""App Server 1"",id=11,""App Server 2"",id=12, ""Database Server"") 
    | eval to=case(id=1,""Web 1"",id=2,""Web 2"",id=3,""Web 3"", id=4,""App Server 1"",id=5,""App Server 2"", id=6, ""App Server 1"",id=7,""App Server 2"", id=8,""App Server 1"",id=9,""App Server 2"",id=10,""Database Server"",id=11,""Database Server"",id=12, """") 
    | eval value=case(id=1,""Load Balancer"",id=2,""Load Balancer"",id=3,""Load Balancer"", id=4,""Web 1"",id=5,""Web 1"", id=6, ""Web 2"",id=7,""Web 2"", id=8,""Web 3"",id=9,""Web 3"",id=10,""App Server 1"",id=11,""App Server 2"",id=12, ""Database Server"") 
    | eval type=case(id=1,""sitemap"",id=4,""server"", id=6, ""server"",id=8,""server"",id=10,""server"",id=11,""server"",id=12, ""database"") 
    | eval host=from 
    | eval color=if(host==redServer,""red"",""green"") 
    | eval Mem=if(host==redServer,random()%9 + 90,random()%5+50) 
    | eval CPU=if(host==redServer,random()%8 + 90,random()%15+50) 
    | eval _time = now() 
    | eval linkColor = if(to=redServer OR from=redServer, ""red"",""green"")
    | eval linkWidth=random()%11
    | eval linkText=linkWidth . ""MBps""
    | fields _time, from, to, value, type, color, CPU,Mem,host, linkWidth, linkColor, linkText]
    
    
    
    ",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| search to=* OR from=* | table from, to, value, type, color, linkColor, $showLinkWidth$ $showLinkText$",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| search host=$nd_node_token|s$ | timechart avg(CPU) as CPU | filldown",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| search host=$nd_node_token|s$
| timechart avg(Mem) as Mem | filldown",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| makeresults 
| eval linkspeed=$nd_value_token|s$ , fromnode=$nd_node_token|s$ 
| eval linkspeed=if(linkspeed=fromnode, ""N/A"", linkspeed) 
| table linkspeed",,,"drill_down_example"
tbd,,"host=Load
host=from",,,,"network-diagram-viz",,"| gentimes start=-7 end=0 increment=8h 
| eval host=""Load Balancer,Web 1,Web 2,Web 3,App Server 1,App Server 2,Database Server""
| makemv delim="","" host
| mvexpand host
| eval Mem=random()%20 + 60
| eval CPU=random()%20 + 60 
| rename starttime as _time
| fields _time, CPU,Mem,host


| append[

| makeresults count=12
| eval redServer=random()%7+1
| streamstats count as id
| eval from=case(id=1,""Load Balancer"",id=2,""Load Balancer"",id=3,""Load Balancer"", id=4,""Web 1"",id=5,""Web 1"", id=6, ""Web 2"",id=7,""Web 2"", id=8,""Web 3"",id=9,""Web 3"",id=10,""App Server 1"",id=11,""App Server 2"",id=12, ""Database Server"")
| eval to=case(id=1,""Web 1"",id=2,""Web 2"",id=3,""Web 3"", id=4,""App Server 1"",id=5,""App Server 2"", id=6, ""App Server 1"",id=7,""App Server 2"", id=8,""App Server 1"",id=9,""App Server 2"",id=10,""Database Server"",id=11,""Database Server"",id=12, """")
| eval value=case(id=1,""Load Balancer"",id=2,""Load Balancer"",id=3,""Load Balancer"", id=4,""Web 1"",id=5,""Web 1"", id=6, ""Web 2"",id=7,""Web 2"", id=8,""Web 3"",id=9,""Web 3"",id=10,""App Server 1"",id=11,""App Server 2"",id=12, ""Database Server"")
| eval type=case(id=1,""sitemap"",id=4,""server"", id=6, ""server"",id=8,""server"",id=10,""server"",id=11,""server"",id=12, ""database"")
| eval color=if(id==redServer,""red"",""green"")
| eval Mem=if(id==redServer,99,random()%50+20)
| eval CPU=if(id==redServer,99,random()%50+20)
| eval _time = now()
| eval host=from
]
| table _time, host, CPU, Mem
| search host=$nd_node_token|s$",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| gentimes start=-7| eval requests=random()%400 + 200 | rename starttime as  _time | table _time, requests",,,"drill_down_example"
tbd,,,,,,"network-diagram-viz",,"| gentimes start=-7| eval requests=random()%100 + 10 | rename starttime as  _time | table _time, requests",,,"drill_down_example"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_files tx_hosts=""$src_ip$"" AND rx_hosts=""$dest_ip$"" 
| stats count earliest(_time) as firstSeenTime by rx_hosts
| iplocation rx_hosts 
| eval _time = firstSeenTime
| table rx_hosts, count, _time, Country, City,",,"sourcetype=corelight_files","files_logs_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_files tx_hosts=""192.168.1.156"" AND rx_hosts=""*"" 

| stats count as eventCount earliest(_time) as firstSeenTime by mime_type
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_files","files_logs_anomalies"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_files tx_hosts=""192.168.1.156"" AND rx_hosts=""*"" 

| stats count as eventCount earliest(_time) as firstSeenTime by md5
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_files","files_logs_anomalies"
tbd,,,"index=wekan",,"lookup wekan_lists.csv
lookup wekan_users.csv","lame_wekan_pm",,"index=wekan ""$wildcard_search$""
| eval checklistTitleWithStatus = case(sourcetype=""wekan_checklistitems"" AND isFinished=""true"", title + "" - Complete"", sourcetype=""wekan_checklistitems"" AND isFinished=""false"", title + "" - Pending"" )
| eval cardId = case(sourcetype=""wekan_cards"", id, 1=1, cardId)
| eval dateLastActivityTime = case(sourcetype=""wekan_cards"", endAt)
| eval cardTitle = case(sourcetype=""wekan_cards"", title)
| eval dueAt = strftime(dueAt, ""%Y-%m-%d"")
| eval endAt = strftime(endAt, ""%Y-%m-%d"")
| eval startAt = strftime(startAt, ""%Y-%m-%d"")
| eval modifiedAt = strftime(modifiedAt, ""%Y-%m-%d"")
| lookup wekan_lists.csv id as listId output title as listTitle
| lookup wekan_users.csv id as assignees_0 output username as assignees_0
| eval _time = dateLastActivityTime
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, cardId
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""
| sort startAt, Task )",,"sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_cards
sourcetype=wekan_cards","find_a_card"
tbd,,,"index=summary",,"lookup dashboard_details","CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| search urlField=""*$c-input$*""  OR datasources=""*$c-input$*"" OR eai.data=""*$c-input$*""
| fillnull value=""N/A"" datasources
| table  urlField, datasources, myapp  author, eai:acl.sharing, count, dc_user","source=dashboard_views",,"find_a_dashboard"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
| inputlookup dmc_forwarder_assets
| makemv delim="" "" avg_tcp_kbps_sparkline
| eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
| eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
| eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
| eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
| `dmc_rename_forwarder_type(forwarder_type)`
| `dmc_time_format(last_connected)`
    ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
| search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
    ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/saved/searches/DMC%20Forwarder%20-%20Build%20Asset%20Table/history
      | eval endtime = strptime(updated, ""%Y-%m-%dT%H:%M:%S"")
      | sort 1 -endtime
      | fields updated
      | rename updated AS last_run_time
      | eval last_run_time = if(isnotnull(last_run_time), "" - As of "".replace(last_run_time, ""T"", "" ""), "" "")
    ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as ""count"" by $forwarderCountSplitBy$",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as count",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as ""count"" by $forwarderCountSplitBy$",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as count",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
| $forwarderNameFilter$
| $forwarderStatusFilter$
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
 search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
| $forwarderNameFilter$
| $forwarderStatusFilter$
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_get_forwarder_tcpin`
          | eval forwarder = guid
          | eval receiver = host."":"".destPort
          | stats dc(forwarder) as count_forwarder, dc(receiver) as count_receiver
        ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
          `dmc_get_forwarder_tcpin`
           | eval forwarder = guid
           | eval receiver = host."":"".destPort
           | search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
           | stats dc(forwarder) as count_forwarder, dc(receiver) as count_receiver
        ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_forwarder",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_forwarder",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_receiver",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin`
| eval connection = hostname."":"".sourcePort.""->"".host."":"".destPort
| eval event_count = tcp_eps * 30
| `dmc_timechart_for_metrics_log` dc(connection) as connection_count, per_second($funcForwarderCountOverlay$) as $funcForwarderCountOverlay$
| rename connection_count as ""Connections"", $funcForwarderCountOverlay$ as $forwarderCountOverlayLabel$
          ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| eval source_uri = hostname."":"".sourcePort
| eval dest_uri = host."":"".destPort
| eval connection = source_uri.""->"".dest_uri
| stats values(fwdType) as fwdType, values(sourceIp) as sourceIp, latest(version) as version,  values(os) as os, values(arch) as arch, dc(dest_uri) as dest_count, dc(connection) as connection_count, avg(tcp_KBps) as avg_tcp_kbps, avg(tcp_eps) as avg_tcp_eps by hostname, guid
| eval avg_tcp_kbps = round(avg_tcp_kbps, 2)
| eval avg_tcp_eps = round(avg_tcp_eps, 2)
| `dmc_rename_forwarder_type(fwdType)`
| rename hostname as Instance, fwdType as ""Forwarder Type"", sourceIp as IP, version as ""Splunk Version"", os as OS, arch as Architecture, guid as GUID, dest_count as ""Receiver Count"", connection_count as ""Connection Count"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| eval dest_uri = host."": "".destPort
| stats count by dest_uri
| fields dest_uri
| rename dest_uri as ""Receivers""
          ",,,"forwarder_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| `dmc_timechart_for_metrics_log` $funcVolume$(eval(tcp_KBps)) as ""KB/s"", $funcVolume$(tcp_eps) as ""Events/s""
          ",,,"forwarder_instance"
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| stats count by date_hour date_wday",,,gallery
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| stats count by date_hour date_wday",,,gallery
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| eval duration = duration_ms/60000
| stats count median(duration) by date_hour date_wday",,,gallery
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| eval duration = duration_ms/60000
| eval is_member = if(member_type==""Registered"", 1, 0)
| eval is_casual = if(member_type==""Casual"", 1, 0)
| stats avg(duration_ms) sum(is_member) as sum_m, sum(is_casual) as sum_c by date_hour date_wday
| eval prevalent_member_type = if(sum_m &gt; sum_c, ""Member"", ""Casual"")
| fields - sum_m, sum_c",,,gallery
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| search member_type=""Registered""
| stats count by date_hour start_station
| sort 400 start_station",,,gallery
tbd,,,,"inputlookup bikeshare.csv",,"punchcard_app",,"| inputlookup bikeshare.csv
| search member_type=""Casual""
| stats count by date_hour start_station
| sort 400 start_station",,,gallery
tbd,,,,"inputlookup webstore_requests.csv",,"sankey_diagram_app",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path",,,gallery
tbd,,,,"inputlookup webstore_requests.csv",,"sankey_diagram_app",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path",,,gallery
tbd,,,,"inputlookup webstore_requests.csv",,"sankey_diagram_app",,"|inputlookup webstore_requests.csv  | rex field=referer ""https?://.*(?&lt;referer_path&gt;/.*)\?.*""  | stats count, avg(bytes) by referer_path uri_path  | where count &gt; 10",,,gallery
tbd,,,,,,"network-diagram-viz",,"|makeresults
| eval raw=""from=splunkd, to=\""file_monitor_input\"", color=green, type= type=indexer, value= ### from=splunkd, to=\""index_processor\"", color=green, type= type=indexer, value=  ### from=splunkd, to=\""search_scheduler\"", color=green, type= type=indexer, value=  ### from=splunkd, to=\""workload_management\"", color=green, type= type=indexer, value=  ### from=\""file_monitor_input\"", to=\""batchreader-0\"", color=green, type= type=folder, value=  ### from=\""file_monitor_input\"", to=\""tailreader-0\"", color=, type= type=folder, value=  ### from=\""index_processor\"", to=buckets, color=, type= type=folder, value=  ### from=\""index_processor\"", to=\""disk_space\"",color=green, type= type=folder, value=  ### from=\""index_processor\"", to=\""index_optimization\"", color=green, type= type=folder, value=  ### from=\""search_scheduler\"", to=\""search_lag\"",linkcolor=green, color=yellow, type= type=folder, value=  ### from=\""search_scheduler\"", to=\""searches_delayed\"",linkcolor=red, color=yellow, type= type=folder, value=  ### from=\""search_scheduler\"", to=\""searches_skipped\"",linkcolor=green, color=yellow, type= type=folder, value=  ### from=\""workload_management\"", to=\""configuration_check\"", color=green , type= type=folder, value=  ### from=\""workload_management\"", to=\""system_check\"", color=green, type= type=folder, value=  ### from=\""batchreader-0\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=buckets, to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""configuration_check\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""disk_space\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""index_optimization\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""search_lag\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""searches_delayed\"", to=, color=red, type=\""times-circle\"", value=\""Status: red at \"" ### from=\""searches_skipped\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""system_check\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### from=\""tailreader-0\"", to=, color=green, type=\""check-circle\"", value=\""Status: green at \"" ### ""
|makemv delim=""###"" raw
| mvexpand raw
| rename raw as _raw
| extract
| eval time = strftime(now(),""%d %b, %H:%M:%S"")| eval value = value ."" "". time",,,"general_examples"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=5
|streamstats count as id
| eval from=""Source"", to=""Destination""
| eval linktext = ""Line #"" + id
| eval linklength = 100 * id
| table from, to, linktext, linklength
|  append [ 
 |  makeresults 
 |  eval raw=""from=\""Source\"", x=\""-382\"", y=\""17\"" ### from=\""Destination\"", x=\""-369\"", y=\""-166\""""
 |  makemv delim=""###"" raw 
 |  mvexpand raw 
 |  rename raw  as  _raw 
 |  extract 
 |  table from, x, y]",,,"general_examples"
tbd,,,,,,"network-diagram-viz",,"|makeresults
| eval raw = ""from=\""10x.152.100.10\"", to=\""20x.115.136.144\"", nodetext=\""10x.152.100.10\"", linkcolor=red, linktext=\""4.39 ms\"" ~ from=\""10x.152.100.10\"", to=\""20x.115.136.59\"", nodetext=\""10x.152.100.10\"", linkcolor=green, linktext=\""3.07 ms\"" ~ from=\""10x.152.100.10\"", to=\""6x.115.191.0\"", nodetext=\""10x.152.100.10\"", linkcolor=green, linktext=\""0.28 ms\"" ~ from=\""10x.152.100.10\"", to=, nodetext=\""10x.152.100.10\"",  linktext=, value=\""10x.152.100.10\"", type=\""network-wired\"" ~ from=\""10x.152.100.12\"", to=\""20x.115.136.144\"", nodetext=\""10x.152.100.12\"", linkcolor=red, linktext=\""4.39 ms\"" ~ from=\""10x.152.100.12\"", to=\""20x.115.136.59\"", nodetext=\""10x.152.100.12\"", linkcolor=green, linktext=\""3.07 ms\"" ~ from=\""10x.152.100.12\"", to=\""6x.115.191.0\"", nodetext=\""10x.152.100.12\"", linkcolor=green, linktext=\""0.28 ms\"" ~ from=\""10x.152.100.12\"", to=, nodetext=\""10x.152.100.12\"",  linktext=, value=\""10x.152.100.12\"", type=\""network-wired\"" ~ from=\""10x.152.100.14\"", to=\""20x.115.136.144\"", nodetext=\""10x.152.100.14\"", linkcolor=red, linktext=\""4.39 ms\"" ~ from=\""10x.152.100.14\"", to=\""20x.115.136.59\"", nodetext=\""10x.152.100.14\"", linkcolor=green, linktext=\""3.07 ms\"" ~ from=\""10x.152.100.14\"", to=\""6x.115.191.0\"", nodetext=\""10x.152.100.14\"", linkcolor=green, linktext=\""0.28 ms\"" ~ from=\""10x.152.100.14\"", to=, nodetext=\""10x.152.100.14\"",  linktext=, value=\""10x.152.100.14\"", type=\""network-wired\"" ~ from=\""10x.152.100.8\"", to=\""20x.115.136.144\"", nodetext=\""10x.152.100.8\"", linkcolor=red, linktext=\""4.39 ms\"" ~ from=\""10x.152.100.8\"", to=\""20x.115.136.59\"", nodetext=\""10x.152.100.8\"", linkcolor=green, linktext=\""3.07 ms\"" ~ from=\""10x.152.100.8\"", to=\""6x.115.191.0\"", nodetext=\""10x.152.100.8\"", linkcolor=green, linktext=\""0.28 ms\"" ~ from=\""10x.152.100.8\"", to=, nodetext=\""10x.152.100.8\"",  linktext=, value=\""10x.152.100.8\"", type=\""network-wired\"" ~ from=\""20x.182.219.253\"", to=\""10x.152.100.10\"", nodetext=\""20x.182.219.253\"", linkcolor=green, linktext=\""0.19 ms\"" ~ from=\""20x.182.219.253\"", to=\""10x.152.100.12\"", nodetext=\""20x.182.219.253\"", linkcolor=green, linktext=\""1.41 ms\"" ~ from=\""20x.182.219.253\"", to=\""10x.152.100.14\"", nodetext=\""20x.182.219.253\"", linkcolor=green, linktext=\""0.44 ms\"" ~ from=\""20x.182.219.253\"", to=\""10x.152.100.8\"", nodetext=\""20x.182.219.253\"", linkcolor=green, linktext=\""0.33 ms\"" ~ from=\""20x.182.219.253\"", to=, nodetext=\""20x.182.219.253\"",  linktext=, value=\""20x.182.219.253\"", type=\""network-wired\"" ~ from=\""20x.182.219.254\"", to=\""10x.152.100.10\"", nodetext=\""20x.182.219.254\"", linkcolor=green, linktext=\""0.19 ms\"" ~ from=\""20x.182.219.254\"", to=\""10x.152.100.12\"", nodetext=\""20x.182.219.254\"", linkcolor=green, linktext=\""1.41 ms\"" ~ from=\""20x.182.219.254\"", to=\""10x.152.100.14\"", nodetext=\""20x.182.219.254\"", linkcolor=green, linktext=\""0.44 ms\"" ~ from=\""20x.182.219.254\"", to=\""10x.152.100.8\"", nodetext=\""20x.182.219.254\"", linkcolor=green, linktext=\""0.33 ms\"" ~ from=\""20x.182.219.254\"", to=, nodetext=\""20x.182.219.254\"",  linktext=, value=\""20x.182.219.254\"", type=\""network-wired\"" ~ from=\""6x.115.191.0\"", to=\""19x.229.225.133\"", nodetext=\""6x.115.191.0\"", linkcolor=green, linktext=\""2.90 ms\"" ~ from=\""6x.115.191.0\"", to=\""19x.229.227.133\"", nodetext=\""6x.115.191.0\"", linkcolor=green, linktext=\""2.23 ms\"" ~ from=\""6x.115.191.0\"", to=\""20x.115.136.144\"", nodetext=\""6x.115.191.0\"", linkcolor=red, linktext=\""4.39 ms\"" ~ from=\""6x.115.191.0\"", to=\""20x.115.136.59\"", nodetext=\""6x.115.191.0\"", linkcolor=green, linktext=\""3.07 ms\"" ~ from=\""6x.115.191.0\"", to=, nodetext=\""6x.115.191.0\"",  linktext=, value=\""6x.115.191.0\"", type=\""network-wired\"" ~ from=\""origin.example.com\"", to=\""20x.182.219.253\"", nodetext=\""origin.example.com\"", linkcolor=green, linktext=\""0.93 ms\"" ~ from=\""origin.example.com\"", to=\""20x.182.219.254\"", nodetext=\""origin.example.com\"", linkcolor=green, linktext=\""0.37 ms\"" ~""
| makemv delim=""~"" raw
| mvexpand raw 
| rename raw  as  _raw 
| extract",,,"general_examples"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=25
| streamstats count as from 
| eval type=case(from==1, ""usergroups"", from==2, ""users"", from==3, ""server"", from==4, ""database"",from==5, ""desktop"",from==6, ""laptop"",from==7, ""printer"",from==8, ""harddrive"",from==9,""wifi"",from==10, ""code"",from==11, ""money"",from==12, ""bell""
,from==13, ""shopping-cart"",from==14, ""comment"",from==15, ""exclamation-circle"",from==16, ""file"",from==17, ""globe"",from==18, ""sitemap"",from==19, ""image"",from==20, ""info"", from==21, ""cloud"",from==22,""envelope"",from==23,""star"",from==24,""code-branch"",from==25,""folder"")

| eval to=case(from==2, ""usergroups"", from==3, ""users"", from==4, ""server"", from==5, ""database"",from==6, ""desktop"",from==7, ""laptop"",from==8, ""printer"",from==9, ""harddrive"",from==10,""wifi"",from==11, ""code"",from==12, ""money"",from==13, ""bell""
,from==14, ""shopping-cart"",from==15, ""comment"",from==16, ""exclamation-circle"",from==17, ""file"",from==18, ""globe"",from==19, ""sitemap"",from==20, ""image"",from==21, ""info"", from==22, ""cloud"",from==23,""envelope"",from==24,""star"",from==25,""code-branch"")

| eval from=type

|  eval color = random()%4 | eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"")

| eval value = type
| table from, to, value, type, color
| append[|makeresults | eval from=""Network Diagram Viz"", nodeText="" "", type=""customimage"", customurl=""/static/app/network-diagram-viz/customimages/network-diagram-viz.png"" | table from, nodeText, type, customurl]
      

 | eval nodeText=from . "" "" . to . "" "" . value . "" "" . type . "" "" . color",,,"general_examples"
tbd,,,,,,"lame_training",,"| rest services/data/models
| stats count by title",,,"help_document_datamodel_analytics"
tbd,,,,"inputlookup Datamodel_Analytics.csv",,"lame_training",,"| inputlookup Datamodel_Analytics.csv
| search datamodel=""$dm$""
| table metric_description, metric_query",,,"help_document_datamodel_analytics"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | stats count by index | fields index | sort index",,,"help_document_sourcetype_analytics"
tbd,,,"index=*","inputlookup base_analytics.csv",,"lame_training",,"| inputlookup base_analytics.csv
| makemv delim="","" index
| makemv delim="","" sourcetype
| search index=*
| stats count by sourcetype",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup SourcetypeInfo.csv",,"lame_training",,"| inputlookup SourcetypeInfo.csv
| search sourcetype=$st$
| table fieldname, rationale",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup Sourcetype_Analytics.csv",,"lame_training",,"| inputlookup Sourcetype_Analytics.csv
| search sourcetype=$st$
| table metric_description, metric_query",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup autonumberAnalytics.csv","lookup analyticsV2.csv","lame_training",,"| inputlookup autonumberAnalytics.csv
| makemv delim="","" index
| makemv delim="","" sourcetype
| search index=$idx$ sourcetype=$st$
| lookup analyticsV2.csv BaseAnalytic as BaseAnalytic Output queries, myapp
| stats values(myapp) as app, values(BaseAnalytic) as BaseAnalytic count by queries
| fields - count",,,"help_document_sourcetype_analytics"
tbd,,,,,"lookup dashboard_details.csv","CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* 
| lookup dashboard_details.csv id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre

| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""
| search urlField=""$id$""
  
| rex field=eai:data (?i)""sourcetype=\""?(?&lt;sourcetypes&gt;.*?)\""?\s"" max_match=0
| rex field=eai:data (?i)""source=\""?(?&lt;sources&gt;.*?)\""?\s"" max_match=0
| rex field=eai:data (?i)""eventtype=\""?(?&lt;eventtypes&gt;.*?)\""?\s"" max_match=0
| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes))
| rename eai:acl.app as app eai:acl.owner as owner
| table datasources urlField app owner details mitre usecase
| transpose
| rename column as ""Field Names"" 
| rename ""row 1"" as ""Descriptions""",,,"help_page"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 60 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 90 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 30 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv | head 1 | stats count",,,"horseshoe_meter_gallery"
tbd,,,,"inputlookup horseshoe_data.csv",,"horseshoe_meter_app",,"| inputlookup horseshoe_data.csv
                        | head 100
                        | eval rnd = random() % 10
                        | rangemap field=rnd ""Site 1""=0-2 ""Site 2""=3-5 ""Site 3""=6-8 default=""Site 4""
                        | stats avg(value) as count by range
                        | eval count = round(count)",,,"horseshoe_meter_gallery"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
      | timechart minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
      | eval data_received=data_received/pow(1024, 2) 
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval valid_requests_total = requests_total
    ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
      | bin _time minspan=1m 
      | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time
      | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
      | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval data_received=data_received/pow(1024, 2) 
      | eval valid_requests_total = requests_total
      | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
    ",,,"http_event_collector_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
      | bin _time minspan=1m 
      | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time, host
      | join host [ 
      | inputlookup dmc_assets 
      | mvcombine search_group 
      | search search_group=$role$ OR search_group=""$group$"" ]
      | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
      | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval data_received=data_received/pow(1024, 2) 
      | eval valid_requests_total = requests_total
      | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
      | stats sum(events_total) as events_total, sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received, sum(data_indexed) as data_indexed
      | appendcols [search `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ | stats count as event_count]
    ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
        | `dmc_get_groups_containing_role($role$)` 
        | search search_group!=""dmc_group_*""
      ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
        | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/data/inputs/http 
        | eval token_name=substr('title', 8)
      ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(events_total) as events_total by _time
          | stats latest(events_total) as event_latest 
          | eval event_throughput = event_latest/60 
          | fields event_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(valid_requests_total) as valid_requests_total by _time
          | stats latest(valid_requests_total) as valid_request_latest 
          | eval valid_requests_throughput=valid_request_latest/60 
          | fields valid_requests_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(invalid_requests_total) as invalid_requests_total by _time
          | stats latest(invalid_requests_total) as invalid_request_latest 
          | eval invalid_requests_throughput=invalid_request_latest/60 
          | fields invalid_requests_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_received) as data_received_latest 
          | eval data_received_throughput = round(data_received_latest/60, 3) 
          | fields data_received_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_indexed) as data_indexed_latest 
          | eval data_indexed_throughput = round(data_indexed_latest/60, 3) 
          | fields data_indexed_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          timechart minspan=""1m"" sum(valid_requests_total) as ""Valid Requests"" sum(invalid_requests_total) as ""Invalid Requests"" sum(data_received) as ""Data Received""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" /services/data/inputs/http 
          | search title=""http://$token_name$"" 
          | dedup splunk_server                          
          | fields splunk_server                          
          | join type=left splunk_server [|`dmc_get_instance_roles` | rename serverName as splunk_server]                                              
          | fields host, splunk_server, role                          
          | join type=left splunk_server [| rest /services/server/status/resource-usage/hostwide
          | eval mem_used=round(mem_used / mem * 100, 0)  
          | eval cpu_usage=cpu_system_pct + cpu_user_pct
          | fields splunk_server, mem_used, cpu_usage]
          | eval port = if(isnotnull(port), port, 8088)                                 
          | fields host, cpu_usage, mem_used, role, port
          | rename host as ""Instance"", cpu_usage as ""CPU Usage (%)"", mem_used as ""Memory Usage (%)"", role as Role, port as ""HTTP Event Collector Port Number""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval events_total = if(event_count==0, 0, events_total)
          | fields events_total
          | rename events_total as ""Event Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval valid_requests_total = if(event_count==0, 0, valid_requests_total)
          | fields valid_requests_total
          | rename valid_requests_total as ""Valid Request Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval invalid_requests_total = if(event_count==0, 0, invalid_requests_total)
          | fields invalid_requests_total
          | rename invalid_requests_total as ""Invalid Request Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_received = if(event_count==0, 0, data_received)
          | fields data_received
          | rename data_received as ""Data Received""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_indexed = if(event_count==0, 0, data_indexed)
          | fields data_indexed
          | rename data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token"" $token_clause$      
          | bin _time minspan=1m                     
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, values(data.token_name) as token_name by _time
          | eval valid_requests_total = requests_total
          | timechart partial=f minspan=1m sum($event_type$) as $event_type$ by token_name
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_auth_failures, num_of_requests_to_disabled_token, num_of_requests_to_incorrect_url, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_requests_to_disabled_token, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token"" $token_clause$
          | bin _time minspan=1m
          | timechart partial=f minspan=1m sum(data.$invalid_request_reason$) as $invalid_request_reason$ by data.token_name
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token""
          | bin _time minspan=1m 
          | stats sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received, values(data.token_name) as token_name by _time
          | eval data_received=data_received/pow(1024, 2)
          | eval data_indexed=data_indexed/pow(1024, 2)
          | timechart partial=f minspan=1m sum($data_type$) by token_name
        ",,,"http_event_collector_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | bin _time minspan=1m 
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_errors) as error_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time, host
          | join host [ 
          | inputlookup dmc_assets 
          | mvcombine search_group 
          | search search_group=$role$ OR search_group=""$group$"" ]
          | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
          | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
          | eval data_indexed=round(data_indexed/pow(1024, 2), 2)
          | eval data_received=round(data_received/pow(1024, 2), 2)
          | eval valid_requests_total = requests_total
          | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
          | stats sum(events_total) as events_total, sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received, sum(data_indexed) as data_indexed by host
          | fields host, events_total, valid_requests_total, invalid_requests_total, data_received, data_indexed
          | rename host as Instance, events_total as ""Event Count"", valid_requests_total as ""Valid Request Count"", invalid_requests_total as ""Invalid Request Count"", data_received as ""Data Received (MB)"", data_indexed as ""Data Indexed (MB)""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
    `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$
    | bin _time span=1m
    | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time
    | eval incorrect_url_total=if(isnotnull(incorrect_url_total), incorrect_url_total, 0)
    | eval auth_fail_total=if(isnotnull(auth_fail_total), auth_fail_total, 0)
    | eval data_indexed=data_indexed/pow(1024, 2)
    | eval data_received=data_received/pow(1024, 2)
    | eval valid_requests_total = requests_total
    | eval invalid_requests_total = auth_fail_total + disabled_token_total + incorrect_url_total
  ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
    `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$
    | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_parser_errors) as parser_errors_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
    | eval requests_to_incorrect_url_total = if(isnotnull(requests_to_incorrect_url_total), requests_to_incorrect_url_total, 0)
    | eval auth_failures_total = if(isnotnull(auth_failures_total), auth_failures_total, 0)
    | eval data_received=data_received/pow(1024, 2) 
    | eval data_indexed=data_indexed/pow(1024, 2)
    | appendcols [search `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$ | stats count as event_count]
  ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
        | rest splunk_server=* /services/data/inputs/http 
        | dedup splunk_server 
        | fields splunk_server 
        | join type=left splunk_server [|`dmc_get_instance_roles` | rename serverName as splunk_server]
      ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/inputs/http 
      | eval token_name=substr('title', 8)
    ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(events_total) as event_latest
          | eval event_throughput = event_latest/60
          | fields event_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(valid_requests_total) as valid_request_latest
          | eval valid_requests_throughput=valid_request_latest/60
          | fields valid_requests_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(invalid_requests_total) as invalid_request_latest
          | eval invalid_requests_throughput=invalid_request_latest/60
          | fields invalid_requests_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_received) as data_received_latest
          | eval data_received_throughput = round(data_received_latest/60, 3)
          | fields data_received_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_indexed) as data_indexed_latest
          | eval data_indexed_throughput = round('data_indexed_latest'/60, 3)
          | fields data_indexed_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          timechart minspan=""1m"" sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received
          | rename valid_requests_total as ""Valid Requests"", invalid_requests_total as ""Invalid Requests"", data_received as ""Data Received""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval events_total = if(event_count==0, 0, events_total)
          | fields events_total
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval valid_request_count_total = if(event_count==0, 0, requests_total)
          | fields valid_request_count_total 
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval invalid_request_count_total = if(event_count==0, 0, auth_failures_total + requests_to_disabled_token_total + requests_to_incorrect_url_total)
          | fields invalid_request_count_total
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_received = if(event_count==0, 0, data_received)
          | fields data_received
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_indexed = if(event_count==0, 0, data_indexed)
          | fields data_indexed
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token"" $token_clause$      
          | bin _time minspan=1m                     
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, values(data.token_name) as token_name by _time
          | eval valid_requests_total = requests_total
          | timechart partial=f minspan=1m sum($event_type$) as $event_type$ by token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_auth_failures, num_of_requests_to_disabled_token, num_of_requests_to_incorrect_url, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_requests_to_disabled_token, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token"" $token_clause$ 
          | bin _time minspan=1m
          | timechart partial=f minspan=1m sum(data.$invalid_request_reason$) as $invalid_request_reason$ by data.token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token""
          | bin _time minspan=1m 
          | stats sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received, values(data.token_name) as token_name by _time
          | eval data_received=data_received/pow(1024, 2)
          | eval data_indexed=data_indexed/pow(1024, 2)
          | timechart partial=f minspan=1m sum($data_type$) by token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/data/inputs/http
          | eval enabled = if(disabled == 1, ""No"", ""Yes"")
          | eval Token=substr(title, 8)
          | fields Token, enabled
          | join type=""left"" Token [search http_event_collector_token `dmc_set_index_introspection` host=$host$
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_errors_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as requests_with_auth_failures, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by data.token_name
          | eval data_indexed=data_indexed/pow(1024, 2)
          | eval data_received=data_received/pow(1024, 2)
          | rename ""Data Indexed"" as ""Data Indexed (MB)"" ""Data Received"" as ""Data Received (MB)""
          | eval valid_requests_total = requests_total
          | eval invalid_request_count_total = requests_to_disabled_token_total + requests_with_auth_failures + requests_to_incorrect_url
          | fields data.token_name, events_total, valid_requests_total, invalid_request_count_total, data_received, data_indexed
          | rename data.token_name as ""Token""]
          | eval events_total=if(isnull(events_total), 0, events_total)
          | eval invalid_request_count_total=if(isnull(invalid_request_count_total), 0, invalid_request_count_total)
          | eval valid_requests_total=if(isnull(valid_requests_total), 0, valid_requests_total)
          | eval data_received=if(isnull(data_received), 0, data_received)
          | eval data_indexed=if(isnull(data_indexed), 0, data_indexed)
          | fields Token, enabled, data_indexed, data_received, events_total, invalid_request_count_total, valid_requests_total
          | rename enabled as ""Enabled"", data_indexed as ""Data Indexed (MB)"", data_received as ""Data Received (MB)"", events_total as ""Event Count"", invalid_request_count_total as ""Invalid Request Count"", valid_requests_total as ""Valid Request Count""
        ",,,"http_event_collector_instance"
tbd,,,"index=bro",,,"log_analysis_made_easy",,"index=bro sourcetype=corelight_http src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$""

| bin span=5m _time 

| stats  count as eventCount by _time method
| eventstats perc$perc$(eventCount) as perc95 by method
| eval diff = eventCount-perc95
| chart avg(diff) by _time, method",,"sourcetype=corelight_http","http_log_anomalies"
tbd,,,"index=bro",,,"log_analysis_made_easy",,"index=bro sourcetype=corelight_http src_ip=""$src_ip$"" AND dest_ip=""$dest_ip$"" 
| stats count earliest(_time) as firstSeenTime by dest_ip
| iplocation dest_ip 
| eval _time = firstSeenTime
| table dest_ip, count, _time, Country, City,",,"sourcetype=corelight_http","http_log_anomalies"
tbd,,,"index=bro",,,"log_analysis_made_easy",,"index=bro sourcetype=corelight_http src_ip=""192.168.1.156"" AND dest_ip=""*"" 

| stats count as eventCount earliest(_time) as firstSeenTime by url
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_http","http_log_anomalies"
tbd,,,"index=bro",,,"log_analysis_made_easy",,"index=bro sourcetype=corelight_http src_ip=""192.168.1.156"" AND dest_ip=""*"" 

| stats count as eventCount earliest(_time) as firstSeenTime by uri
| eval _time = firstSeenTime
| fields - firstSeenTime",,"sourcetype=corelight_http","http_log_anomalies"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes/$index$
            | join title splunk_server type=outer [| rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes-extended/$index$]
            | eval bucketCount = coalesce(total_bucket_count, 0)
            | eval eventCount = coalesce(totalEventCount, 0)
            | eval coldBucketSize = coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size', 0)
            | eval coldBucketSizeGB = round(coldBucketSize/ 1024, 2)
            | eval coldBucketMaxSizeGB = if(isnull('coldPath.maxDataSizeMB') OR 'coldPath.maxDataSizeMB' = 0, ""unlimited"", round('coldPath.maxDataSizeMB' / 1024, 2))
            | eval coldBucketUsageGB = coldBucketSizeGB."" / "".coldBucketMaxSizeGB
            | eval homeBucketSizeGB = coalesce(round((total_size - coldBucketSize) / 1024, 2), 0.00)
            | eval homeBucketMaxSizeGB = round('homePath.maxDataSizeMB' / 1024, 2)
            | eval homeBucketMaxSizeGB = if(homeBucketMaxSizeGB > 0, homeBucketMaxSizeGB, ""unlimited"")
            | eval homeBucketUsageGB = homeBucketSizeGB."" / "".homeBucketMaxSizeGB
            | eval dataAgeDays = coalesce(round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0), 0)
            | eval frozenTimePeriodDays = round(frozenTimePeriodInSecs / 86400, 0)
            | eval frozenTimePeriodDays = if(frozenTimePeriodDays > 0, frozenTimePeriodDays, ""unlimited"")
            | eval freezeRatioDays = dataAgeDays."" / "".frozenTimePeriodDays
            | eval indexSizeGB = if(currentDBSizeMB >= 1 AND totalEventCount >=1, round(currentDBSizeMB/1024, 2), 0.00)
            | eval maxTotalDataSizeGB = round(maxTotalDataSizeMB / 1024, 2)
            | eval indexMaxSizeGB = if(maxTotalDataSizeGB > 0, maxTotalDataSizeGB, ""unlimited"")
            | eval indexSizeUsageGB = indexSizeGB."" / "".indexMaxSizeGB
            | eval indexSizeUsagePerc = if(isNum(indexMaxSizeGB) AND (indexMaxSizeGB > 0), round(indexSizeGB / indexMaxSizeGB * 100, 2).""%"", ""N/A"")
            | eval total_raw_size = coalesce(total_raw_size, 0)
        ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    | `dmc_get_groups_containing_role(dmc_group_indexer)`
                    | where search_group!=""dmc_group_indexer""
                ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes $datatype$
                    | `dmc_exclude_indexes`
                    | stats count by title
                    | fields - count
                ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) as instances
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalIndexSizeGB
                        | eval totalIndexSizeGB = totalIndexSizeGB."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        search title=$index$
                        | stats sum(eval(total_raw_size / 1024)) as totalRawSizeGB
                        | eval totalRawSizeGB = round(totalRawSizeGB, 2)."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalIndexSizeGB
                        sum(eval(total_raw_size / 1024)) as totalRawSizeGB
                        | eval compressionRatio = round(totalRawSizeGB / totalIndexSizeGB, 2)."":1""
                        | fields compressionRatio
                        | eval compressionRatio = if(isnotnull(compressionRatio), compressionRatio, ""N/A"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(eventCount) as totalEventCount
                        | eval totalEventCount = toString(totalEventCount, ""commas"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats min(minTime) as oldestEvent
                        | eval oldestEvent = replace(oldestEvent, ""T"", "" "")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats max(maxTime) as newestEvent
                        | eval newestEvent = replace(newestEvent, ""T"", "" "")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(indexSizeGB) as avgIndexSizeGB
                        | eval avgIndexSizeGB = round(avgIndexSizeGB, 2)."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(eventCount) as avgEventCount
                        | eval avgEventCount = toString(floor(avgEventCount), ""commas"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        eval dataAgeDays = if(isnotnull(dataAgeDays), dataAgeDays, 0)
                        | stats median(dataAgeDays) as medianDataAge
                        | eval medianDataAge = medianDataAge."" days""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(bucketCount) as avgBucketCount
                        | eval avgBucketCount = floor(avgBucketCount)
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        eval avgBucketSize = round(indexSizeGB / bucketCount, 2)
                        | fields splunk_server, freezeRatioDays, indexSizeUsageGB, homeBucketUsageGB, coldBucketUsageGB, eventCount, bucketCount, avgBucketSize
                        | rename splunk_server as ""Indexer""
                        freezeRatioDays as ""Data Age vs Frozen Age (days)""
                        indexSizeUsageGB as ""Index Usage (GB)""
                        homeBucketUsageGB as ""Home Path Usage (GB)""
                        coldBucketUsageGB as ""Cold Path Usage (GB)""
                        eventCount as ""Total Event Count""
                        bucketCount as ""Total Bucket Count""
                        avgBucketSize as ""Average Bucket Size (GB)""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=hosts index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields host, totalCount
                        | sort - totalCount
                        | rename host as Host, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=sources index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields source, totalCount
                        | sort - totalCount
                        | rename source as Source, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=sourcetypes index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields sourcetype, totalCount
                        | sort - totalCount
                        | rename sourcetype as Sourcetype, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | `dmc_set_bin_for_disk_usage`
                        | stats latest(data.total_size) as totalSize by host _time
                        | `dmc_timechart_for_disk_usage` $funcDiskSizeUsage$(eval(totalSize /1024)) as ""Index Size""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval pctDiskUsage = 'data.total_size' / 'data.total_capacity'
                        | bin _time minspan=10min
                        | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
                        | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
                        | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
                        | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval pctDiskUsage = 'data.total_size' / 'data.total_capacity'
                        | bin _time minspan=10min
                        | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
                        | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
                        | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
                        | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval data_birth_date = if(isnotnull('data.bucket_dirs.cold.event_min_time'), 'data.bucket_dirs.cold.event_min_time', 'data.bucket_dirs.home.event_min_time')
                        | eval data_age_days = round((_time - data_birth_date) / 86400, 0)
                        | `dmc_timechart_for_disk_usage` $funcDataAge$(data_age_days) as ""Data Age""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/data/indexes $datatype$
  | join type=outer title [
    | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
  ]
| `dmc_exclude_indexes`
| eval warm_bucket_size = coalesce('bucket_dirs.home.warm_bucket_size', 'bucket_dirs.home.size')
| eval cold_bucket_size = coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
| eval hot_bucket_size = if(isnotnull(cold_bucket_size), total_size - cold_bucket_size - warm_bucket_size, total_size - warm_bucket_size)
| eval thawed_bucket_size = coalesce('bucket_dirs.thawed.bucket_size', 'bucket_dirs.thawed.size')
| eval warm_bucket_size_gb = coalesce(round(warm_bucket_size / 1024, 2), 0.00)
| eval hot_bucket_size_gb = coalesce(round(hot_bucket_size / 1024, 2), 0.00)
| eval cold_bucket_size_gb = coalesce(round(cold_bucket_size / 1024, 2), 0.00)
| eval thawed_bucket_size_gb = coalesce(round(thawed_bucket_size / 1024, 2), 0.00)

| eval warm_bucket_count = coalesce('bucket_dirs.home.warm_bucket_count', 0)
| eval hot_bucket_count = coalesce('bucket_dirs.home.hot_bucket_count', 0)
| eval cold_bucket_count = coalesce('bucket_dirs.cold.bucket_count', 0)
| eval thawed_bucket_count = coalesce('bucket_dirs.thawed.bucket_count', 0)
| eval home_event_count = coalesce('bucket_dirs.home.event_count', 0)
| eval cold_event_count = coalesce('bucket_dirs.cold.event_count', 0)
| eval thawed_event_count = coalesce('bucket_dirs.thawed.event_count', 0)

| eval home_bucket_size_gb = coalesce(round((warm_bucket_size + hot_bucket_size) / 1024, 2), 0.00)
| eval homeBucketMaxSizeGB = coalesce(round('homePath.maxDataSizeMB' / 1024, 2), 0.00)
| eval home_bucket_capacity_gb = if(homeBucketMaxSizeGB > 0, homeBucketMaxSizeGB, ""unlimited"")
| eval home_bucket_usage_gb = home_bucket_size_gb."" / "".home_bucket_capacity_gb
| eval cold_bucket_capacity_gb = coalesce(round('coldPath.maxDataSizeMB' / 1024, 2), 0.00)
| eval cold_bucket_capacity_gb = if(cold_bucket_capacity_gb > 0, cold_bucket_capacity_gb, ""unlimited"")
| eval cold_bucket_usage_gb = cold_bucket_size_gb."" / "".cold_bucket_capacity_gb

| eval currentDBSizeGB = round(currentDBSizeMB / 1024, 2)
| eval maxTotalDataSizeGB = if((maxTotalDataSizeMB > 0) AND isNull(remotePath), round(maxTotalDataSizeMB / 1024, 2), ""unlimited"")
| eval disk_usage_gb = currentDBSizeGB."" / "".maxTotalDataSizeGB

| eval currentTimePeriodDay = coalesce(round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0), 0)
| eval frozenTimePeriodDay = coalesce(round(frozenTimePeriodInSecs / 86400, 0), 0)
| eval frozenTimePeriodDay = if(frozenTimePeriodDay > 0, frozenTimePeriodDay, ""unlimited"")
| eval freeze_period_viz_day = currentTimePeriodDay."" / "".frozenTimePeriodDay

| eval total_bucket_count = toString(coalesce(total_bucket_count, 0), ""commas"")
| eval totalEventCount = toString(coalesce(totalEventCount, 0), ""commas"")
| eval total_raw_size_gb = round(total_raw_size / 1024, 2)
| eval avg_bucket_size_gb = round(currentDBSizeGB / total_bucket_count, 2)
| eval compress_ratio = round(total_raw_size_gb / currentDBSizeGB, 2)."" : 1""

| fields title, datatype
    currentDBSizeGB, totalEventCount, total_bucket_count,  avg_bucket_size_gb,
    total_raw_size_gb, compress_ratio, minTime, maxTime
    freeze_period_viz_day, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb,
    hot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb,
    hot_bucket_count,   warm_bucket_count,   cold_bucket_count,   thawed_bucket_count,
    home_event_count,   cold_event_count,    thawed_event_count,
    homePath, homePath_expanded, coldPath, coldPath_expanded, thawedPath, thawedPath_expanded, summaryHomePath_expanded, tstatsHomePath, tstatsHomePath_expanded,
    maxTotalDataSizeMB, frozenTimePeriodInSecs, homePath.maxDataSizeMB, coldPath.maxDataSizeMB,
    maxDataSize, maxHotBuckets, maxWarmDBCount
    ",,,"index_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
      index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes data.name=$single_index_name$
      | eval data_birth_date = if(isnotnull('data.bucket_dirs.cold.event_min_time'), 'data.bucket_dirs.cold.event_min_time', 'data.bucket_dirs.home.event_min_time')
      | eval data_age_days = round((_time - data_birth_date) / 86400, 0)
      | eval data.total_capacity = if(isnotnull('data.total_capacity'), 'data.total_capacity', 500000)
      | eval disk_usage = round('data.total_size' / 1024, 2)
      | eval disk_capacity = round('data.total_capacity' / 1024, 2)
      | `dmc_timechart_for_disk_usage` $funcDataAge$(data_age_days) as data_age_days, $funcDiskUsage$(disk_usage) as disk_usage, $funcDiskUsage$(disk_capacity) as disk_capacity
    ",,"sourcetype=splunk_disk_objects","index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"| eval index_data_type_combined = title."":"".datatype | stats count by title index_data_type_combined
        ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields currentDBSizeGB | eval currentDBSizeGB = currentDBSizeGB."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields total_raw_size_gb | eval total_raw_size_gb = total_raw_size_gb."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields compress_ratio",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields total_bucket_count",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields avg_bucket_size_gb | eval avg_bucket_size_gb = avg_bucket_size_gb."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields totalEventCount",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields minTime | eval minTime = replace(minTime, ""T"", "" "")",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields maxTime | eval maxTime = replace(maxTime, ""T"", "" "")",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
search title=$single_index_name$
| fields freeze_period_viz_day, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb
| rename freeze_period_viz_day as ""Data Age vs Frozen Age (days)"", disk_usage_gb as ""Index Usage (GB)"", home_bucket_usage_gb as ""Home Path Usage (GB)"", cold_bucket_usage_gb as ""Cold Path Usage (GB)""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
            | rest splunk_server=$splunk_server$ /services/data/indexes/$single_index_name$
            | `dmc_exclude_indexes`
            | rex field=homePath ""volume:(?<home_vol_name>[^/\\\]*)(?:/|\\\)""
            | rex field=coldPath ""volume:(?<cold_vol_name>[^/\\\]*)(?:/|\\\)""
            | fields home_vol_name cold_vol_name
            | eval home_vol_name = if(isnull(home_vol_name), ""N/A"", home_vol_name)
            | eval cold_vol_name = if(isnull(cold_vol_name), ""N/A"", cold_vol_name)
            | rename home_vol_name as ""home"" cold_vol_name as ""cold""
            | transpose | rename column as ""Index Directory"" ""row 1"" as title

            | join type=outer title [
              | rest splunk_server=$splunk_server$ /services/data/index-volumes
              | fields title, total_size, max_size, volume_path
              | `dmc_exclude_volumes`
              | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
              | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
              | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
              | eval remaining_capacity_pct = (max_size_gb - total_size_gb) / max_size_gb
              | fields title, disk_usage_gb, remaining_capacity_pct
            ]

            | eval disk_usage_gb = if(isnull(disk_usage_gb), ""N/A"", disk_usage_gb)
            | eval ""Volume Freezing Due to Size"" = if(isnull(remaining_capacity_pct), ""N/A"", if(remaining_capacity_pct < 0.05, ""Yes"", ""No""))
            | rename title as ""Volume Name"" disk_usage_gb as ""Volume Usage / Capacity (GB)""
            | fields - remaining_capacity_pct
            ]]>
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
search title=$single_index_name$
| fields hot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb
| rename hot_bucket_size_gb as Hot, warm_bucket_size_gb as Warm, cold_bucket_size_gb as Cold, thawed_bucket_size_gb as Thawed
| eval bucket_dir = """"
| fields bucket_dir, Hot, Warm, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields home_event_count, cold_event_count, thawed_event_count
            | rename home_event_count as Home, cold_event_count as Cold, thawed_event_count as Thawed
            | eval bucket_dir = """"
            | fields bucket_dir, Home, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields hot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count
            | rename hot_bucket_count as Hot, warm_bucket_count as Warm, cold_bucket_count as Cold, thawed_bucket_count as Thawed
            | eval bucket_dir = """"
            | fields bucket_dir, Hot, Warm, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=hosts index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields host, totalCount
| sort - totalCount
| rename host as Host, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=sources index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields source, totalCount
| sort - totalCount
| rename source as Source, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=sourcetypes index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields sourcetype, totalCount
| sort - totalCount
| rename sourcetype as Sourcetype, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields homePath, homePath_expanded, coldPath, coldPath_expanded, thawedPath, thawedPath_expanded, summaryHomePath_expanded, tstatsHomePath, tstatsHomePath_expanded
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields maxTotalDataSizeMB, frozenTimePeriodInSecs, homePath.maxDataSizeMB, coldPath.maxDataSizeMB
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields maxDataSize, maxHotBuckets, maxWarmDBCount
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, disk_usage $capacityOverlay$",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, data_age_days",,,"index_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes data.name=$single_index_name$

            | eval warm_bucket_total_size       = if(isnotnull('data.bucket_dirs.home.warm_bucket_size'), 'data.bucket_dirs.home.warm_bucket_size', 'data.bucket_dirs.home.size')
            | eval cold_bucket_total_size       = if(isnotnull('data.bucket_dirs.cold.bucket_size'), 'data.bucket_dirs.cold.bucket_size', 'data.bucket_dirs.cold.size')
            | eval thawed_bucket_total_size     = if(isnotnull('data.bucket_dirs.thawed.bucket_size'), 'data.bucket_dirs.thawed.bucket_size', 'data.bucket_dirs.thawed.size')
            | eval hot_bucket_total_size        = if(isnotnull(cold_bucket_total_size), 'data.total_size' - cold_bucket_total_size - warm_bucket_total_size, 'data.total_size' - warm_bucket_total_size)

            | eval warm_bucket_total_size_gb    = round(warm_bucket_total_size / 1024, 2)
            | eval cold_bucket_total_size_gb    = round(cold_bucket_total_size / 1024, 2)
            | eval thawed_bucket_total_size_gb  = round(thawed_bucket_total_size / 1024, 2)
            | eval hot_bucket_total_size_gb     = round(hot_bucket_total_size / 1024, 2)

            | eval warm_bucket_count            = 'data.bucket_dirs.home.warm_bucket_count'
            | eval hot_bucket_count             = 'data.bucket_dirs.home.hot_bucket_count'
            | eval cold_bucket_count            = 'data.bucket_dirs.cold.bucket_count'
            | eval thawed_bucket_count          = 'data.bucket_dirs.thawed.bucket_count'

            | eval home_event_count             = 'data.bucket_dirs.home.event_count'
            | eval cold_event_count             = 'data.bucket_dirs.cold.event_count'
            | eval thawed_event_count           = 'data.bucket_dirs.thawed.event_count'

            | eval hot_bucket_avg_size_gb       = round(hot_bucket_total_size_gb / hot_bucket_count, 2)
            | eval warm_bucket_avg_size_gb      = round(warm_bucket_total_size_gb / warm_bucket_count, 2)
            | eval cold_bucket_avg_size_gb      = round(cold_bucket_total_size_gb / cold_bucket_count, 2)
            | eval thawed_bucket_avg_size_gb    = round(thawed_bucket_total_size_gb / thawed_bucket_count, 2)

            | fields
            hot_bucket_total_size_gb, warm_bucket_total_size_gb, cold_bucket_total_size_gb, thawed_bucket_total_size_gb,
            hot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count,
            home_event_count, cold_event_count, thawed_event_count,
            hot_bucket_avg_size_gb, warm_bucket_avg_size_gb, cold_bucket_avg_size_gb, thawed_bucket_avg_size_gb

            | `dmc_timechart_for_disk_usage` $funcBucket$(*_$single_index_metric$) as ""* bucket""
          ",,"sourcetype=splunk_disk_objects","index_detail_instance"
tbd,,,,"inputlookup index_info",,"lame_analytic_documentation",,"| inputlookup index_info
| rename _key as the_key
| table the_key, index, description, usegroup",,,"index_information"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_indexer_cluster_groups`",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_indexer, Cluster*)` search_group=""$group$""
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_indexer, Cluster*)` punct=""$warningErrorPunct$"" search_group=""$group$""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/buckets?filter=search_state=PendingSearchable""
          | fields title, index, peers*server_name
          | eval peers = """"
          | foreach peers.*.server_name [eval peers = mvappend(peers, """", '&lt;&lt;FIELD&gt;&gt;')]
          | fields title, index, peers
        ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"stats count by index",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            $fixupProgressIndexFilter$
            | rename title as Bucket, index as Index, peers as Peers
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes
            | fields title
            | rename title as index
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=search_factor""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=replication_factor""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=generation""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes
            | fields title, buckets_with_excess_copies, buckets_with_excess_searchable_copies, total_excess_bucket_copies, total_excess_searchable_copies
            | search (buckets_with_excess_copies > 0) OR (buckets_with_excess_searchable_copies > 0) OR (total_excess_bucket_copies > 0) OR (total_excess_searchable_copies > 0)
            | rename title as Index, buckets_with_excess_copies as ""Buckets with Excess Copies"", buckets_with_excess_searchable_copies as ""Buckets with Excess Searchable Copies"", total_excess_bucket_copies as ""Total Excess Copies"", total_excess_searchable_copies as ""Total Excess Searchable Copies""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=cmmaster_service search_group=dmc_group_cluster_master search_group=""$group$""
| fields - to_fix_total to_fix_added to_fix_removed
| `dmc_timechart_for_metrics_log` median(to_fix_*) as to_fix_*
| rename to_fix_sync AS Sync to_fix_data_safety AS ""Data safety"" to_fix_gen AS Generation to_fix_rep_factor AS ""Replication factor"" to_fix_search_factor AS ""Search factor"" to_fix_streaming AS Streaming
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=executor name=cmmaster_executor search_group=dmc_group_cluster_master search_group=""$group$""
| eval backlog_change = jobs_added - jobs_finished
| `dmc_timechart_for_metrics_log` sum(jobs_added) AS ""Jobs added"" sum(jobs_finished) AS ""Jobs finished"" sum(backlog_change) AS ""Backlog change""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=jobs name=cmmaster search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(CMRepJob) AS ""Bucket replication"" sum(CMChangeBucketJob_build) AS ""Make bucket searchable"" sum(CMChangeBucketJob_makePrimary) AS ""Make bucket primary"" sum(CMChangeBucketJob_removePrimary) AS ""Removing primary bucket site"" sum(CMTruncJob) AS ""Truncating size of bucket"" sum(CMSyncM2PJob) AS ""Syncing buckets for peers and master"" sum(CMSyncP2PJob) AS ""Syncing bucket between peers"" sum(CMBucketFrozenJob) AS ""Notifying peer that bucket is frozen""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_seconds name=cmmaster_endpoints search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(cluster*) as cluster*
| rename clustermaster*_* AS ""/services/cluster/master/*:*"" clusterconfig_* AS ""/services/cluster/config:*""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=cmmaster_endpoints search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(clustermaster*) as clustermaster*
| rename clustermaster*_* AS ""/services/cluster/master/*:*"" clusterconfig_* AS ""/services/cluster/config:*""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_indexer_cluster_groups`",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"| rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/peers",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"| rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/generation/master
      | fields pending_last_reason, search_factor_met, replication_factor_met
      | eval all_data_searchable_icon = if(isnull(pending_last_reason) or pending_last_reason=="""", ""icon-check"", ""icon-alert"")
      | eval all_data_searchable = if (isnull(pending_last_reason) or pending_last_reason=="""", ""All Data Searchable"", ""Some Data Not Searchable"")
      | eval search_factor_met_icon = if(search_factor_met == 1 or search_factor_met == ""1"", ""icon-check"", ""icon-alert"")
      | eval search_factor_met = if (search_factor_met == 1 or search_factor_met == ""1"", ""Search Factor Met"", ""Search Factor Not Met"")
      | eval replication_factor_met_icon = if(replication_factor_met == 1 or replication_factor_met == ""1"", ""icon-check"", ""icon-alert"")
      | eval replication_factor_met = if (replication_factor_met == 1 or replication_factor_met == ""1"", ""Replication Factor Met"", ""Replication Factor Not Met"")
    ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/server/info
      | stats count
      | eval cluster_master_icon = if(count==1, ""icon-check"", ""icon-alert"")
      | eval cluster_master_indicator = if(count==1, ""Cluster Manager Reachable"", ""Cluster Manager Not Reachable"")
    ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 1 or is_searchable == ""1""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 0 or is_searchable == ""0""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 1 or is_searchable == ""1""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 0 or is_searchable == ""0""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count by is_searchable
            | eval is_searchable = if(is_searchable == 1 or is_searchable == ""1"", ""Yes"", ""No"")
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by site",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            fields label, is_searchable, status, site, bucket_count, host_port_pair, last_heartbeat, replication_port, base_generation_id, title, bucket_count_by_index.*
            | eval is_searchable = if(is_searchable == 1 or is_searchable == ""1"", ""Yes"", ""No"")
            | `dmc_time_format(last_heartbeat)`
            | sort - last_heartbeat
            | $peerNameFilter$
            | $peerSearchableFilter$
            | $peerStatusFilter$
            | $peerSiteFilter$
            | fields label, is_searchable, status, site, bucket_count
            | rename label as Peer, is_searchable as ""Fully Searchable"", status as Status, site as Site, bucket_count as Buckets
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            search label = $peerDrilldown$
            | `dmc_time_format(last_heartbeat)`
            | fields host_port_pair, last_heartbeat, replication_port, base_generation_id, title
            | rename host_port_pair as Location, last_heartbeat as ""Last Heartbeat"", replication_port as ""Replication Port"", base_generation_id as ""Base Generation ID"", title as GUID
            | transpose
            | rename column as Property, ""row 1"" as Value
          ",,,"indexer_clustering_status"
tbd,,,"Index=replace",,,"splunk_monitoring_console",,"
            search label = $peerDrilldown$
            | fields bucket_count_by_index.*
            | transpose
            | rename column as Index, ""row 1"" as Buckets
            | eval Index=replace(Index, ""bucket_count_by_index."", """")
            | where isnotnull(Buckets) AND isnotnull(Index)
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count by is_searchable
            | eval is_searchable = if((is_searchable == 1) or (is_searchable == ""1""), ""Yes"", ""No"")
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, is_searchable, replicated_copies_tracker*, searchable_copies_tracker*, num_buckets, index_size
            | rename replicated_copies_tracker.*.* as rp**, searchable_copies_tracker.*.* as sb**
            | eval replicated_data_copies = """"
            | foreach rp*actual_copies_per_slot [eval replicated_data_copies = replicated_data_copies."" "".rp&lt;&lt;MATCHSTR&gt;&gt;actual_copies_per_slot.""/"".rp&lt;&lt;MATCHSTR&gt;&gt;expected_total_per_slot]
            | makemv replicated_data_copies
            | eval searchable_data_copies = """"
            | foreach sb*actual_copies_per_slot [eval searchable_data_copies = searchable_data_copies."" "".sb&lt;&lt;MATCHSTR&gt;&gt;actual_copies_per_slot.""/"".sb&lt;&lt;MATCHSTR&gt;&gt;expected_total_per_slot]
            | makemv searchable_data_copies
            | eval is_searchable = if((is_searchable == 1) or (is_searchable == ""1""), ""Yes"", ""No"")
            | eval index_size = round(index_size / 1024 / 1024 / 1024, 2)."" GB""
            | fields title, is_searchable, searchable_data_copies, replicated_data_copies, num_buckets, index_size
            | $indexNameFilter$
            | $indexSearchableFilter$
            | rename title as ""Index Name"", is_searchable as ""Fully Searchable"", searchable_data_copies as ""Searchable Data Copies"", replicated_data_copies as ""Replicated Data Copies"", num_buckets as Buckets, index_size as ""Cumulative Raw Data Size""
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/searchheads
          | fields label, status, site, host_port_pair, title
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by site",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            $searchHeadNameFilter$
            | $searchHeadStatusFilter$
            | $searchHeadSiteFilter$
            | rename label as ""Search Head"", status as Status, site as Site, host_port_pair as URI, title as GUID
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
                    | `dmc_get_groups_containing_role(dmc_group_indexer)`
                    | where search_group!=""dmc_group_indexer""
                ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes $datatype$
            | join title splunk_server type=outer [rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes-extended $datatype$]
            | `dmc_exclude_indexes`
            | eval elapsedTime = now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")
            | eval dataAge = ceiling(elapsedTime / 86400)
            | eval indexSizeGB = if(currentDBSizeMB >= 1 AND totalEventCount >=1, currentDBSizeMB/1024, null())
            | eval maxSizeGB = maxTotalDataSizeMB / 1024
            | eval sizeUsagePerc = indexSizeGB / maxSizeGB * 100
        ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(title) as numIndexes
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(eval(if(isnotnull(indexSizeGB), title, null()))) as nonEmptyIndexes
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) AS instances
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalSize
                        | eval totalSize = if(isNum(totalSize), round(totalSize, 2)."" GB"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalSize
                        dc(splunk_server) as instances
                        | eval averageAggregateIndexesSizePerInstance = round(totalSize / instances, 2)."" GB""
                        | fields averageAggregateIndexesSizePerInstance
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats median(dataAge) AS medianDataAge
                        | eval medianDataAge = if(isNum(medianDataAge), medianDataAge."" days"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats max(dataAge) AS oldestDataAge
                        | eval oldestDataAge = if(isNum(oldestDataAge), oldestDataAge."" days"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(title) as count",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) AS Instances
                        count(indexSizeGB) as ""Non-Empty Instances""
                        sum(indexSizeGB) AS totalSize
                        avg(indexSizeGB) as averageSize
                        avg(sizeUsagePerc) as averageSizePerc
                        perc90(sizeUsagePerc) as ninetyPercentileSizePerc
                        count(eval(sizeUsagePerc > 95)) as instancesFreezingDueToSize
                        median(dataAge) as medianDataAge
                        max(dataAge) as oldestDataAge
                        count(eval(elapsedTime > frozenTimePeriodInSecs)) as instancesFreezingDueToAge
                        sum(frozenTimePeriodInSecs) as infiniteFreezingFlag
                        by title, datatype
                        | eval totalSize = if(isnotnull(totalSize), round(totalSize, 2), 0)
                        | eval averageSize = if(isnotnull(averageSize), round(averageSize, 2), 0)
                        | eval averageSizePerc = if(isnotnull(averageSizePerc), round(averageSizePerc, 2).""%"", ""N/A"")
                        | eval ninetyPercentileSizePerc = if(isnotnull(ninetyPercentileSizePerc), round(ninetyPercentileSizePerc, 2).""%"", ""N/A"")
                        | eval instancesFreezingDueToSize = if(averageSizePerc != ""N/A"", instancesFreezingDueToSize, ""N/A"")
                        | eval medianDataAge = if(isNum(medianDataAge), medianDataAge, ""N/A"")
                        | eval oldestDataAge = if(isNum(oldestDataAge), oldestDataAge, ""N/A"")
                        | eval instancesFreezingDueToAge = if(infiniteFreezingFlag > 0, instancesFreezingDueToAge, ""N/A"")
                        | rename title as ""Index""
                        datatype as ""Data Type""
                        totalSize as ""Total Size (GB)""
                        averageSize as ""Average Size (GB)""
                        averageSizePerc as ""Average Usage (%)""
                        ninetyPercentileSizePerc as ""90th Percentile Usage (%)""
                        instancesFreezingDueToSize as ""Instances Freezing Due To Size*""
                        medianDataAge as ""Median Data Age (days)""
                        oldestDataAge as ""Oldest Data Age (days)""
                        instancesFreezingDueToAge as ""Instances Freezing Due to Age""
                        | fields - infiniteFreezingFlag
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" services/data/index-volumes
                        | `dmc_exclude_volumes`
                        | eval volumeSizeGB = if(total_size > 1, round(total_size / 1024, 2), null())
                        | eval sizeUsagePerc = total_size / max_size * 100
                        | stats dc(splunk_server) as Instances
                        count(eval(total_size > 1)) as ""Non-Empty Instances""
                        sum(volumeSizeGB) as totalSize
                        avg(volumeSizeGB) as avgSize
                        avg(sizeUsagePerc) as avgSizePerc
                        perc90(sizeUsagePerc) as ninetyPercentileSizePerc
                        count(eval(total_size > max_size)) as volumesFreezingDueToSize
                        by title
                        | eval totalSize = if(isnotnull(totalSize), totalSize, 0)
                        | eval avgSize = if(isnotnull(avgSize), round(avgSize, 2), 0)
                        | eval avgSizePerc = if(isnotnull(avgSizePerc), round(avgSizePerc, 2).""%"", ""N/A"")
                        | eval ninetyPercentileSizePerc = if(isnotnull(ninetyPercentileSizePerc), round(ninetyPercentileSizePerc, 2).""%"", ""N/A"")
                        | eval volumesFreezingDueToSize = if(avgSizePerc != ""N/A"", volumesFreezingDueToSize, ""N/A"")
                        | rename title as ""Volume""
                        totalSize as ""Total Size (GB)""
                        avgSize as ""Average Size (GB)""
                        avgSizePerc as ""Average Usage (%)""
                        ninetyPercentileSizePerc as ""90th Percentile Usage (%)""
                        volumesFreezingDueToSize as ""Volumes Freezing Due To Size""
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/data/indexes $datatype$
  | join title type=outer [
  | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
  | eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
  | fields title, cold_bucket_size, total_size, total_bucket_count]
| `dmc_exclude_indexes`
| fields title datatype maxTotalDataSizeMB currentDBSizeMB frozenTimePeriodInSecs minTime coldPath.maxDataSizeMB homePath.maxDataSizeMB, homePath, coldPath, cold_bucket_size, total_size, total_bucket_count, totalEventCount
| eval currentDBSizeGB = if(isnotnull(currentDBSizeMB), round(currentDBSizeMB / 1024, 2), 0)
| eval maxTotalDataSizeGB = if((maxTotalDataSizeMB == 0) OR isnull(maxTotalDataSizeMB), ""unlimited"", round(maxTotalDataSizeMB / 1024, 2))
| eval disk_usage_gb = currentDBSizeGB."" / "".maxTotalDataSizeGB
| eval currentTimePeriodDay = round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0)
| eval currentTimePeriodDay = if(isnull(currentTimePeriodDay), 0, currentTimePeriodDay)
| eval frozenTimePeriodDay = round(frozenTimePeriodInSecs / 86400, 0)
| eval frozenTimePeriodDay = if(isnull(frozenTimePeriodDay) OR frozenTimePeriodDay == 0, ""unlimited"", frozenTimePeriodDay)
| eval freeze_period_viz = currentTimePeriodDay."" / "".frozenTimePeriodDay
| eval total_bucket_count = if(isnotnull(total_bucket_count), total_bucket_count, 0)
| eval totalEventCount = if(isnotnull(totalEventCount), totalEventCount, 0)
| eval home_bucket_size_gb = round((total_size - if(isnull(cold_bucket_size), 0, cold_bucket_size)) / 1024, 2)
| eval home_bucket_size_gb = if(isnull(home_bucket_size_gb), 0, home_bucket_size_gb)
| eval home_bucket_capacity_gb = if(isnull('homePath.maxDataSizeMB') OR 'homePath.maxDataSizeMB' = 0, ""unlimited"", round('homePath.maxDataSizeMB' / 1024, 2))
| eval home_bucket_usage_gb = home_bucket_size_gb."" / "".home_bucket_capacity_gb
| eval cold_bucket_size_gb = if(isnull(cold_bucket_size), 0, round(cold_bucket_size / 1024, 2))
| eval cold_bucket_capacity_gb = if(isnull('coldPath.maxDataSizeMB') OR 'coldPath.maxDataSizeMB' = 0, ""unlimited"", round('coldPath.maxDataSizeMB' / 1024, 2))
| eval cold_bucket_usage_gb = cold_bucket_size_gb."" / "".cold_bucket_capacity_gb
| fields title, datatype, freeze_period_viz, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb, total_bucket_count, totalEventCount, currentDBSizeGB,
      cold_bucket_size_gb, home_bucket_size_gb, homePath, coldPath
    ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(currentDBSizeGB) as total_gb
            | eval total_gb = case(
                total_gb > 1000, round(total_gb / 1024, 2)."" TB"",
                1 = 1, total_gb."" GB""
              )
            ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(totalEventCount) as totalEventCount
            | eval totalEventCount = tostring(totalEventCount, ""commas"")
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(total_bucket_count) as total_bucket_count
            | eval total_bucket_count = tostring(total_bucket_count, ""commas"")
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, datatype, freeze_period_viz, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb, totalEventCount, total_bucket_count
            | eval total_bucket_count=tostring(total_bucket_count, ""commas"")
            | eval totalEventCount=tostring(totalEventCount, ""commas"")
            | rename title as Index, datatype as ""Data Type"", disk_usage_gb as ""Index Usage (GB)"", freeze_period_viz as ""Data Age vs Frozen Age (days)"", home_bucket_usage_gb as ""Home Path Usage (GB)"", cold_bucket_usage_gb as ""Cold Path Usage (GB)"", total_bucket_count as ""Total Bucket Count"", totalEventCount as ""Total Event Count""
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/data/index-volumes
            | fields title, total_size, max_size, volume_path
            | `dmc_exclude_volumes`
            | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
            | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
            | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
            | fields title, disk_usage_gb, max_size_gb, volume_path
            | rename title as Volume, disk_usage_gb as ""Volume Usage (GB)"", max_size_gb as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
fields title, cold_bucket_size_gb, home_bucket_size_gb, homePath, coldPath
| rex field=homePath ""volume:(?&lt;home_vol_name&gt;[^/\\\]*)(?:/|\\\)""
| rex field=coldPath ""volume:(?&lt;cold_vol_name&gt;[^/\\\]*)(?:/|\\\)""
| eval dir = ""home,cold""
| makemv dir delim="",""
| mvexpand dir
| eval dir_size = case(dir == ""home"", home_bucket_size_gb, dir == ""cold"", cold_bucket_size_gb)
| eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
| fields title dir dir_size vol_name
| where isnotnull(vol_name) AND isnotnull(dir_size) AND dir_size > 0
| eval index_dir_name = title."":"".dir
| chart sum(dir_size) AS dir_size over vol_name by index_dir_name
          ",,,"indexes_and_volumes_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes
            | `dmc_exclude_indexes`
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | `dmc_timechart_for_disk_usage` $funcIndex$($index_metric$) as total by data.name
          ",,"sourcetype=splunk_disk_objects","indexes_and_volumes_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Volumes
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | `dmc_timechart_for_disk_usage` $funcVolume$(data.total_size) as total_size by data.name
          ",,"sourcetype=splunk_disk_objects","indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/introspection/queues
      | eval current_fill_perc = round(current_size_bytes / max_size_bytes * 100, 0)
      | fields title, current_fill_perc
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      eval a = ""to transpose table""
      | where title==""parsingQueue"" OR title==""aggQueue"" OR title==""typingQueue"" OR title==""indexQueue""
      | chart values(current_fill_perc) over a by title
      | fields parsingQueue, aggQueue, typingQueue, indexQueue
      | eval parsingQueue = if(isnotnull(parsingQueue), parsingQueue, ""N/A"")
      | eval aggQueue = if(isnotnull(aggQueue), aggQueue, ""N/A"")
      | eval typingQueue = if(isnotnull(typingQueue), typingQueue, ""N/A"")
      | eval indexQueue = if(isnotnull(indexQueue), indexQueue, ""N/A"")
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/introspection/indexer
            | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
            | fields splunk_server, average_KBps, status
            | eval average_KBps = round(average_KBps, 0)
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/pipelineSetSelectionPolicy
              | fields value
              | rename value as pipeline_set_selection_policy]
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines]
            | fields splunk_server, value, pipeline_set_selection_policy, average_KBps, status
            | rename splunk_server as Instance, value as ""Pipeline Set Count"", pipeline_set_selection_policy as ""Pipeline Set Selection Policy"", average_KBps as ""Indexing Rate (KB/s)"", status as Status, reason as Reason
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=pipeline
            | stats count by ingest_pipe
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" group=dutycycle
            | eval dutycycle_ratio_perc=ratio*100
            | `dmc_timechart_for_metrics_log` avg(dutycycle_ratio_perc) by thread limit=30
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" group=dutycycle name=ingest
            | fillnull value=0 ingest_pipe
            | search ingest_pipe=$log_pipe_scope$
            | eval dutycycle_ratio_perc=ratio*100
            | chart limit=30 avg(dutycycle_ratio_perc) over ingest_pipe by thread
            | rename ingest_pipe as ""Pipeline Set""
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | chart limit=30 count(busiest_thread_name) over ingest_pipe by busiest_thread_name
            | rename ingest_pipe as ""Pipeline Set""
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | eval dutycycle_ratio_perc=dutycycle_ratio*100
            | `dmc_timechart_for_metrics_log` max(dutycycle_ratio_perc) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | eval pipeline_set_selection_perc=share*100
            | `dmc_timechart_for_metrics_log` max(pipeline_set_selection_perc) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | `dmc_timechart_for_metrics_log` max(requests_last_period) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=pipeline NOT processor=sendout
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` per_second(eval(cpu_seconds*100)) AS pctCPU by processor useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=pipeline name=indexerpipe processor=indexer
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` sum(write_cpu_seconds) AS ""raw data write"" sum(service_cpu_seconds) AS ""index service""
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=subtask_seconds
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | fields replicate_semislice, sync_hotBkt, throttle_optimize, flushBlockSig, retryMove_1hotBkt, size_hotBkt, roll_hotBkt, chillOrFreeze, update_checksums, fork_recovermetadata, rebuild_metadata, update_bktManifest, service_volumes, service_maxSizes, service_externProc
            | `dmc_timechart_for_metrics_log` sum(*) AS *
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=per_$groupTypeCpuProfValue$_$processorTypeCpuProfValue$_cpu*
          | eval cpu_time_ms = if(isnotnull(cpu_time_ms), cpu_time_ms, cpu)
          | eval avg_cpu_time_per_event_ms = if(isnotnull(avg_cpu_time_per_event_ms), avg_cpu_time_per_event_ms, cpupe)
          | eval event_count = if(isnotnull(event_count), event_count, ev)
          | bin _time minspan=30s
          | stats $functionQueueCpuProf$(cpu_time_ms) AS cpu_time_ms $functionQueueCpuProf$(avg_cpu_time_per_event_ms) AS avg_cpu_time_per_event_ms $functionQueueCpuProf$(bytes) AS bytes $functionQueueCpuProf$(event_count) AS event_count by series, _time
        ","source=*",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(cpu_time_ms) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(avg_cpu_time_per_event_ms) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(bytes) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(event_count) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/server/introspection/queues
        | search title=parsingQueue* OR title=aggQueue* OR title=typingQueue* OR title=indexQueue*
        | eval fill_perc=round(current_size_bytes / max_size_bytes * 100,2)
        | fields splunk_server, title, fill_perc
        | rex field=title ""(?<queue_name>^\w+)(?:\.(?<pipeline_number>\d+))?""
        | eval fill_perc = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".fill_perc, fill_perc)
        | chart values(fill_perc) over splunk_server by queue_name
        | eval pset_count = mvcount(parsingQueue)
        | join type=outer splunk_server [
          | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/server/introspection/indexer
          | eval average_KBps = round(average_KBps, 0)
          | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
          | fields splunk_server, average_KBps, status]
        | fields splunk_server pset_count average_KBps status parsingQueue aggQueue typingQueue indexQueue
        | sort -average_KBps
      ]]>
    ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(average_KBps)",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
stats avg(average_KBps) as avg_indexing_rate
| eval avg_indexing_rate = round(avg_indexing_rate, 0)
          ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats median(average_KBps)",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            rename splunk_server as Instance, pset_count as ""Pipeline Set Count"", average_KBps as ""Indexing Rate (KB/s)"", status as ""Status"", parsingQueue as ""Parsing Queue Fill Ratio (%)"", aggQueue as ""Aggregation Queue Fill Ratio (%)"", ""typingQueue"" as ""Typing Queue Fill Ratio (%)"", indexQueue as ""Indexing Queue Fill Ratio (%)""
          ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
                    | `dmc_timechart_for_metrics_log` partial=f limit=0 per_second(kb) AS kbps by host
                    | untable _time host kbps
                    | eval kbps = round(kbps,0)
                    | `dmc_indexing_rate_rangemap_and_timechart`
                ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
| `dmc_timechart_for_metrics_log` partial=f limit=0 per_second(kb) AS kbps by host
| untable _time host kbps
| eval kbps = round(kbps,0)
| `dmc_indexing_rate_rangemap_and_timechart`
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_indexing_rate(""$group$"", $drilldown_indexing_rate_metric$)`",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
| `dmc_timechart_for_metrics_log` partial=f per_second(kb) AS kbps by host
| eval kbps = round(kbps, 0)
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput 
| `dmc_timechart_for_metrics_log` partial=f per_second(kb) AS kbps dc(host) AS num_hosts 
| eval kbps = kbps / $aggrIdxRateDivider$ 
| eval kbps = round(kbps, 0)
| rename kbps as ""KB/s""
| fields - num_hosts
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
                    | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
                    | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
                    | eval fill_perc=round((curr/max)*100,2)
                    | bin _time minspan=30s
                    | stats $instanceWideFillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
                    | `dmc_queue_fill_ratio_rangemap_and_timechart`
                ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| bin _time minspan=30s
| stats $instanceWideFillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
| `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_queue_fill_ratio(""$group$"", $instanceWideFillRatioQueueType$, $instanceWideFillRatioAggrFunc$, $drilldown_queue_fill_ratio_metric$)`",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` partial=f limit=25 $instanceWideFillRatioAggrFunc$(fill_perc) AS fill_percentage by host
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$deploymentWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` partial=f $deploymentWideFillRatioAggrFunc$(fill_perc) AS fill_percentage
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/introspection/queues
      | eval current_fill_perc = round(current_size_bytes / max_size_bytes * 100, 0)
      | fields title, current_fill_perc
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      eval a = ""to transpose table""
      | where title==""parsingQueue"" OR title==""aggQueue"" OR title==""typingQueue"" OR title==""indexQueue""
      | chart values(current_fill_perc) over a by title
      | fields parsingQueue, aggQueue, typingQueue, indexQueue
      | eval parsingQueue = if(isnotnull(parsingQueue), parsingQueue, ""N/A"")
      | eval aggQueue = if(isnotnull(aggQueue), aggQueue, ""N/A"")
      | eval typingQueue = if(isnotnull(typingQueue), typingQueue, ""N/A"")
      | eval indexQueue = if(isnotnull(indexQueue), indexQueue, ""N/A"")
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/introspection/indexer
            | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
            | fields splunk_server, average_KBps, status
            | eval average_KBps = round(average_KBps, 0)
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/pipelineSetSelectionPolicy
              | fields value
              | rename value as pipeline_set_selection_policy]
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines]
            | fields splunk_server, value, pipeline_set_selection_policy, average_KBps, status
            | rename splunk_server as Instance, value as ""Pipeline Set Count"", pipeline_set_selection_policy as ""Pipeline Set Selection Policy"", average_KBps as ""Indexing Rate (KB/s)"", status as Status, reason as Reason
          ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
            search title=""parsingQueue.*"" OR title=""aggQueue.*"" OR title=""typingQueue.*"" OR title=""indexQueue.*""
            | rex field=title ""(?<queue_name>^\w+)\.(?<pipeline_number>\d+)""
            | chart values(current_fill_perc) over pipeline_number by queue_name
            | fields pipeline_number, parsingQueue, aggQueue, typingQueue, indexQueue
            | rename pipeline_number as ""Pipeline Number"", parsingQueue as ""Parsing Queue Fill Ratio (%)"", aggQueue as ""Aggregator Queue Fill Ratio (%)"", typingQueue as ""Typing Queue Fill Ratio (%)"", indexQueue as ""Index Queue Fill Ratio (%)""
            ]]>
          ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=pipeline
            | stats count by ingest_pipe
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=per_$groupTypeIdxPerf$_thruput
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` per_second(kb) by series useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=queue $queues$
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
| search ingest_pipe=$log_pipe_scope$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` $functionQueue$(fill_perc) by name useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,search,,"| rest splunk_server=$splunk_server$ /services/server/info
| fields splunk_server version
| join type=outer splunk_server [rest splunk_server=$splunk_server$ /services/server/status/installed-file-integrity
    | fields splunk_server check_ready check_failures.fail]
| eval check_status = case(isnull('check_failures.fail') AND isnotnull(check_ready), ""enabled"", 'check_failures.fail' == ""check_disabled"", ""disabled"", isnull(check_ready), ""feature unavailable"")
| eval check_ready = if(check_status == ""enabled"", check_ready, ""N/A"")
| fields version check_status check_ready
| rename version AS ""Splunk version"" check_status AS ""Check status"" check_ready AS ""Results ready?""",,,"integrity_check_of_installed_files"
tbd,,,,,,search,,"| rest splunk_server=$splunk_server$ /services/server/status/installed-file-integrity
| fields check_failures.*
| untable splunk_server file_path check_result
| replace ""check_failures.*"" WITH ""*"" IN file_path
| fields file_path check_result
| rename file_path AS ""File path"" check_result AS ""Check result""",,,"integrity_check_of_installed_files"
tbd,,,,"inputlookup append",,"log_analysis_made_easy","outputlookup ip_inventory.csv","| makeresults 
| eval ip = ""$ip$""
| eval hostname = ""$hostname$""
| eval purpose = ""$purpose$""
| eval other = ""$other$""
| eval model = ""$model$""
| eval os = ""$os$""
| eval protect = case(hostname="""" AND purpose="""" AND other="""" AND model="""" AND os="""", ""false"", 1=1, ""true"")

| where protect = ""true""

| table protect ip, hostname, purpose, other, model, os
| inputlookup append=t ip_inventory.csv
| eval foo = case(ip=""$ip$"", now(), 1=1, now() - 100)
| sort -foo
| dedup ip
| table protect foo ip, hostname, purpose, other, model, os
| fields - protect foo
| outputlookup ip_inventory.csv",,,"ip_inventory_editor"
tbd,,,,,,search,,"
            | rest /services/search/jobs/$sid$ splunk_server=local
        ",,,"job_details_dashboard"
tbd,,,,,,search,,"fields runDuration",,,"job_details_dashboard"
tbd,,,,,,search,,"fields scanCount",,,"job_details_dashboard"
tbd,,,,,,search,,"fields eventCount",,,"job_details_dashboard"
tbd,,,,,,search,,"fields resultCount",,,"job_details_dashboard"
tbd,,,,,,search,,"eval eventsScannedPerSec=scanCount / exact(runDuration) | fields eventsScannedPerSec",,,"job_details_dashboard"
tbd,,,,,,search,,"fields dispatchState | eval dispatchState=lower(dispatchState)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields author",,,"job_details_dashboard"
tbd,,,,,,search,,"eval searchMode=if (isnull('custom.display.page.search.mode'), ""smart"", 'custom.display.page.search.mode') | fields searchMode",,,"job_details_dashboard"
tbd,,,,,,search,,"fields splunk_server",,,"job_details_dashboard"
tbd,,,,,,search,,"fields search",,,"job_details_dashboard"
tbd,,,,,,search,,"fields optimizedSearch",,,"job_details_dashboard"
tbd,,,,,,search,,"fields remoteSearch",,,"job_details_dashboard"
tbd,,,,,,search,,"fields reduceSearch | eval reduceSearch=if (isnull(reduceSearch), ""N/A"", reduceSearch)",,,"job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval fields=mvzip(command, duration, "","") | mvexpand fields | rex field=fields ""(?&lt;Command&gt;[^,]+),(?&lt;Duration&gt;.+)$""
                        | table Command, Duration",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,,,,search,,"fields performance.dispatch.stream.remote.*.duration_secs, performance.dispatch.stream.local.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats max(Duration)",,,"job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields reduceSearch | rex field=reduceSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | eval Command=if(like(Command, ""%stats%""), ""stats"", Command) | eval Command=if(like(Command, ""%bin%""), ""prebin"", Command) | join Command [| search index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | stats sum(Duration)",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields remoteSearch | rex field=remoteSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | join Command [| search index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | table Command, Duration",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields reduceSearch | rex field=reduceSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | eval Command=if(like(Command, ""%stats%""), ""stats"", Command) | eval Command=if(like(Command, ""%bin%""), ""prebin"", Command) | join Command [| search index=""_introspection"" sourcetype=search_telemetry source=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | table Command, Duration","source=*","sourcetype=search_telemetry","job_details_dashboard"
tbd,,,,,,search,,"fields performance.startup.handoff.duration_secs",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance.dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats count(Indexer)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance.dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats avg(Duration)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance.dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as ""Duration (seconds)"" | rex field=Indexer ""performance.dispatch.stream.remote.(?&lt;Indexer&gt;.*).duration_secs""",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance.dispatch.stream.remote.$indexer$.* | transpose | rename column as Field ""row 1"" as Value | rex field=Field ""performance.dispatch.stream.remote.$indexer$.(?&lt;Field&gt;.*)$""",,,"job_details_dashboard"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) | table owner
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
	| rename eai:acl.owner as owner
	| table owner]
| append [
    | rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
	| rename eai:data as xml eai:acl.owner as owner
    ``` include only dashboards with customjs ```
    | regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
    ``` filter out dashboards with version=""1.1"" explicitly set ```
    | regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
	| table owner]
| dedup owner",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load)
| fields app
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
	| rename eai:acl.app as app
	| table app]
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
	| rename eai:data as xml eai:acl.app as app
    ``` include only dashboards with customjs ```
    | regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
    ``` filter out dashboards with version=""1.1"" explicitly set ```
    | regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
	| table app]
| dedup app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]
| strcat app_label "" ("" app "")"" app_dropdown",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) owner=$field2$ app=$field3$ | timechart count",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) owner=$field2$ app=$field3$
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]
| stats count by view_name, owner, app, app_label
| sort - count",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
| rename eai:data as xml
  title as view_name
  eai:acl.app as app
  eai:acl.owner as owner
| search owner=$field2$ app=$field3$
``` include only dashboards with customjs ```
| regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
``` filter out dashboards with version=""1.1"" explicitly set ```
| regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
| stats count",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
| rename eai:data as xml
  title as view_name
  eai:acl.app as app
  eai:acl.owner as owner
| search owner=$field2$ app=$field3$
``` include only dashboards with customjs ```
| regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
``` filter out dashboards with version=""1.1"" explicitly set ```
| regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
| table view_name, owner, app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
          | rename eai:acl.app as app eai:acl.owner as owner
          | search owner=$field2$ app=$field3$
          | stats count",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
| rename title as view_name eai:acl.app as app eai:acl.owner as owner
| search owner=$field2$ app=$field3$
| table view_name, owner, app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]",,,"jquery_upgrade"
tbd,,,,,,"lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views 
| stats count by eai:acl.app
| fields eai:acl.app
| sort by eai:acl.app",,,"knowledge_objects_wo_custom_descriptions"
tbd,,,,,"lookup saved_search_details.csv","lame_documentation",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=""$app$"" AND author!=""nobody"" 
| lookup saved_search_details.csv title as title output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre, details
| search details=""TBD""
| table title, eai:acl.app, actions, cron_schedule, action.summary_index._name, details, mitre, usecase",,,"knowledge_objects_wo_custom_descriptions"
tbd,,,,,"lookup dashboard_details.csv","lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search author!=""nobody"" eai:acl.app=""$app$""
| lookup dashboard_details.csv id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre, details
| search details=""TBD""
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""
| eval httpAddress=""https://127.0.0.1:8089/servicesNS/nobody/""
| table  urlField eai:acl.app eai:acl.owner details, mitre, usecase, httpAddress
| rename eai:acl.app as app eai:acl.owner as owner",,,"knowledge_objects_wo_custom_descriptions"
tbd,,,,,"lookup macros_details.csv","lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/admin/macros
| search eai:acl.app=""$app$"" AND author!=""nobody""
| lookup macros_details.csv title as title output Details
| fillnull value=""TBD"" Details
| search Details=""TBD""
| table title, args, eai:acl.app, definition Details
| rename eai:acl.app as app",,,"knowledge_objects_wo_custom_descriptions"
tbd,,,,,"lookup eventtype_details.csv","lame_documentation",,"| rest servicesNS/-/-/saved/eventtypes
          | search eai:acl.app=""$app$""
| rename eai:acl.app as app
| lookup eventtype_details.csv title as title output details
| eval disabled=case(disabled=0, ""true"", disabled=1, ""false"")
| fillnull value=""TBD"" details
| search details=""TBD""
| table app, disabled, title, tags, search, details",,,"knowledge_objects_wo_custom_descriptions"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_kv_store)`
          | where search_group!=""dmc_group_kv_store""
        ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_kv_store, KVStore*)` search_group=""$group$""
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_kv_store, KVStore*)` punct=""$warningErrorPunct$"" search_group=""$group$""
          ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | eval server = host
                    | rename data.opcounters.command as c, data.opcounters.update as u, data.opcounters.queries as q, data.opcountes.deletes as d, data.opcounters.getmore as g, data.opcounters.inserts as i | eval commands=if(isNotNull('c'), 'c',0)
                    | eval totalops=if(isNotNull('c'), 'c',0)+if(isNotNull('u'), 'u', 0)+if(isNotNull('q'), 'q', 0)+if(isNotNull('d'), 'd', 0)+if(isNotNull('g'), 'g', 0)+if(isNotNull('i'), 'i', 0)
                    | bin _time minspan=30s
                    | stats latest(totalops) AS ops latest(data.extra_info.page_faults) AS pf by server _time
                    | eval  percent=if(ops==0, 0, round(abs(pf/ops), 2))
                    | bin _time minspan=30s
                    | stats $countPageFaultsFunc$(percent) as percent by server _time
                    | rangemap field=percent ""0-0.7""=0-0.7 ""0.7-1.3""=0.7001-1.3 ""1.3+""=1.3001-999999 default=abnormal
                    | timechart minspan=30s partial=f dc(server) as server_count by range
                    | fields _time, ""1.3+"", ""0.7-1.3"", ""0-0.7""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_page_faults(""$group$"", $countPageFaultsFunc$, $drilldown_page_fault_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_network(""$group$"")`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | eval source=host
                    | eval ratio='data.mem.virtual'/'data.mem.mappedWithJournal'
                    | bin _time minspan=30s
                    | stats avg(ratio) AS myratio by source _time
                    | eval myratio = round(myratio, 2)
                    | rangemap field=myratio ""0-2x""=0-2 ""2-3x""=2.001-3 "">3x""=3.001-10000 default=abnormal
                    | timechart minspan=30s partial=f dc(source) as server_count by range
                    | fields _time "">3x"", ""2-3x"", ""0-2x""
                ","source=host",,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_memory_ratio(""$group$"", $drilldown_mapped_memory_ratio_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstorereplicasetstats data.replSetStats.myState=1
                    | spath data.replSetStats.members{}.name output=name
                    | spath data.replSetStats.members{}.optimeDate output=optimeDate
                    | spath data.replSetStats.members{}.stateStr output=stateStr
                    | eval prop_key_val=mvzip(mvzip(name, optimeDate, ""---""), stateStr, ""---"")
                    | fields _time, prop_key_val
                    | mvexpand prop_key_val
                    | eval name=mvindex(split(prop_key_val, ""---""), 0)
                    | eval optimeDate=mvindex(split(prop_key_val, ""---""), 1)
                    | eval stateStr=mvindex(split(prop_key_val, ""---""), 2)
                    | where stateStr=""SECONDARY""
                    | join _time
                    [ search `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstorereplicasetstats data.replSetStats.myState=1
                       | spath data.replSetStats.members{}.name output=name
                       | spath data.replSetStats.members{}.optimeDate output=optimeDate
                       | spath data.replSetStats.members{}.stateStr output=stateStr
                       | eval prop_key_val=mvzip(mvzip(name, optimeDate, ""---""), stateStr, ""---"")
                       | fields _time, prop_key_val
                       | mvexpand prop_key_val
                       | eval name=mvindex(split(prop_key_val, ""---""), 0)
                       | eval optimeDate=mvindex(split(prop_key_val, ""---""), 1)
                       | eval stateStr=mvindex(split(prop_key_val, ""---""), 2)
                       | where stateStr=""PRIMARY""
                       | stats max(optimeDate) as primary by _time]
                    | eval difference=(primary-optimeDate)/1000
                    | bin _time minspan=1m
                    | stats avg(difference) AS lag by _time name
                    | eval lag = round(lag, 2)
                    | `dmc_replication_lag_rangemap`
                    | timechart minspan=1m partial=f dc(name) as server_count by range
                    | fields _time "">30s"", ""10-30s"", ""0-10s""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_replication_lag(""$group$"", $drilldown_rep_latency_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_primary_oplog_window(""$group$"")`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | bin _time minspan=30s
                    | stats range(data.backgroundFlushing.total_ms)  AS diff by _time host
                    | timechart minspan=30s per_minute(diff) as ms_per_min by host
                    | untable _time host ms_per_min
                    | eval percent = round(ms_per_min / (60 * 1000) * 100, 2)
                    | `dmc_background_flush_rangemap`
                    | timechart minspan=30s partial=f dc(host) as server_count by range
                    | fields _time, ""50-100%"", ""10-50%"", ""0-10%""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_background_flush(""$group$"", $drilldown_background_flush_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_get_groups_containing_role(dmc_group_kv_store)`
            | search search_group!=""dmc_group_*""
          ",,,"kv_store_instance"
tbd,,,,,,"lame_training",,"
      | makeresults | eval foo=""bar""
      | table foo
    ",,,"lame_channel_timebasetokens"
tbd,,,,,,"lame_training",,"$data_source$ 
| eval src_ip = coalesce(src_ip, src)
| eval dest_ip = coalesce(dest_ip, dest)
| table sourcetype, src_ip, dest_ip",,,"lame_channel_timebasetokens"
tbd,,,"index=lame_training",,"lookup ip_inventory.csv","lame_training",,"
    index=lame_training sourcetype=lame_conn src_ip=$ip$ | lookup ip_inventory.csv ip as src_ip output hostname, os | head 1
| table src_ip, hostname, os
  ",,"sourcetype=lame_conn","lame_channel_token_usage"
tbd,,,,,,"splunk_monitoring_console",,"
      |rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/saved/searches/DMC%20License%20Usage%20Data%20Cube | fields auto_summarize | eval no_acceleration=if(auto_summarize == 1, NULL, ""yes"")
    ",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
					|  `dmc_get_instance_info(""dmc_group_license_master"")`
					| fields host, serverName
				",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval name=pool | eval value=""pool=\"""". pool . ""\"""" | table name value
        ",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$daily_usage_search$($splunk_server$, $size_search$, $host$, ""$pool$"", ""$split_by_field_name$"")` $overlay_remove$",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$daily_usage_pct_search$($splunk_server$, $sz_clause$, $host$, ""$split_by_field_name$"")`",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$max_avg_search$($splunk_server$, ""$split_by_field_name$"", ""$split_by$"")`",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
					|  `dmc_get_instance_info(""dmc_group_license_master"")`
					| fields host, serverName
				",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | join type=outer stack_id [rest splunk_server=$splunk_server$ /services/licenser/stacks | eval stack_id=title | eval stack_quota=quota | fields stack_id stack_quota] | stats sum(used_bytes) as used max(stack_quota) as total | eval usedGB=round(used/1024/1024/1024,3) | eval totalGB=round(total/1024/1024/1024,3) | eval gauge_base=0 | eval gauge_danger=totalGB*0.8 | eval gauge_top=totalGB+0.001 | gauge usedGB gauge_base gauge_danger totalGB gauge_top
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval ""Used""=round(used_bytes/1024/1024/1024, 3) | eval ""Quota""=round(quota/1024/1024/1024, 3) | fields Pool ""Used"" ""Quota""
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval ""% used""=round(used_bytes/quota*100,2) | fields Pool ""% used""
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/messages | where (category==""license_window"" OR category==""pool_over_quota"") AND create_time >= now() - (30 * 86400) | rename pool_id AS pool | eval warning_day=if(category==""pool_over_quota"",""("".strftime(create_time,""%B %e, %Y"")."")"",strftime(create_time-43200,""%B %e, %Y"")) | fields pool warning_day | join outer pool [rest splunk_server=$splunk_server$ /services/licenser/slaves | mvexpand active_pool_ids | eval slave_name=label | eval pool=active_pool_ids | fields pool slave_name | stats values(slave_name) as ""members"" by pool] | join outer pool [rest splunk_server=$splunk_server$ /services/licenser/pools | eval pool=title | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval quotaGB=round(quota/1024/1024/1024,3) | fields pool stack_id, quotaGB] | stats first(pool) as ""Pool"" first(stack_id) as ""Stack ID"" first(members) as ""Current Members"" first(quotaGB) as ""Curent Quota (GB)"" values(warning_day) AS ""Warning Days - (Soft)/Hard"" by pool | fields - pool
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/slaves | mvexpand active_pool_ids | where warning_count>0 | eval pool=active_pool_ids | join type=outer pool [rest splunk_server=$splunk_server$ /services/licenser/pools | eval pool=title | fields pool stack_id] | eval in_violation=if(warning_count>4 OR (warning_count>2 AND stack_id==""free""),""yes"",""no"") | fields label, title, pool, warning_count, in_violation | fields - _timediff | rename label as ""Slave"" title as ""GUID"" pool as ""Pool"" warning_count as ""Hard Warnings"" in_violation AS ""In Violation?""
          ",,,"license_usage_today"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"link_switcher"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"link_switcher"
tbd,,,,"inputlookup geomaps_data.csv",,"simple_xml_examples",,"| inputlookup geomaps_data.csv | iplocation device_ip | geostats latfield=lat longfield=lon count by method",,,"link_switcher"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"link_switcher"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | stats count",,"sourcetype=splunkd","link_switcher"
tbd,,,,,,"lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views 
| stats count by eai:acl.app
   | fields eai:acl.app
   | sort by eai:acl.app",,,"list_of_dashboards"
tbd,,,,"inputlookup dashboard_details.csv",,"lame_documentation",,"| inputlookup dashboard_details.csv | stats count by usecase 
| fields usecase",,,"list_of_dashboards"
tbd,,,,,"lookup dashboard_details.csv","lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=$app$ AND author!=""nobody""
| lookup dashboard_details.csv id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| search (usecase=$category$ OR usecase=""TBD"") AND mitre=$ttp$
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""
   | table  urlField eai:acl.app eai:acl.owner details, mitre, usecase
   | rename eai:acl.app as app eai:acl.owner as owner",,,"list_of_dashboards"
tbd,,,,,,"CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views 
| stats count by eai:acl.app
   | fields eai:acl.app
   | sort by eai:acl.app",,,"list_of_dashboarrds"
tbd,,,,"inputlookup dashboard_details.csv",,"CyberSentry_Training",,"| inputlookup dashboard_details.csv | stats count by usecase 
| fields usecase",,,"list_of_dashboarrds"
tbd,,,,,"lookup dashboard_details.csv","CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=$app$ AND author!=""nobody""
| lookup dashboard_details.csv id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| search (usecase=$category$ OR usecase=""TBD"") AND mitre=$ttp$
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""
   | table  urlField eai:acl.app eai:acl.owner details, mitre, usecase
   | rename eai:acl.app as app eai:acl.owner as owner",,,"list_of_dashboarrds"
tbd,,,,"inputlookup index_details.csv",,"CyberSentry_Training",,"| inputlookup index_details.csv | stats count by area | table area",,,"list_of_indexes"
tbd,,,"index=*",,"lookup index_details.csv","CyberSentry_Training",,"| eventcount summarize=false index=*  
| lookup index_details.csv index as index output details, area
| fillnull value=""TBD"" area
| search area=TBD OR area=$area$
| stats values(details) as details, values(area) as area sum(count) as LogCount by index  | sort - LogCount",,,"list_of_indexes"
tbd,,,,"inputlookup index_details.csv",,"lame_documentation",,"| inputlookup index_details.csv | stats count by area | table area",,,"list_of_indexes"
tbd,,,"index=*",,"lookup index_details.csv","lame_documentation",,"| eventcount summarize=false index=*  
| lookup index_details.csv index as index output details, area
| fillnull value=""TBD"" area
| search area=TBD OR area=$area$
| stats values(details) as details, values(area) as area sum(count) as LogCount by index  | sort - LogCount",,,"list_of_indexes"
tbd,,,,,"lookup CS_Lookup_Info.csv","CyberSentry_Training",,"| rest/servicesNS/-/-/data/lookup-table-files | search eai:acl.app=""CyberSentry_Analysis"" | table title
| lookup CS_Lookup_Info.csv lookup as title Output static, description",,,"list_of_lookups"
tbd,,,,,"lookup CS_Lookup_Info.csv","CyberSentry_Training",,"| rest/servicesNS/-/-/storage/collections/config | search eai:acl.app=""CyberSentry_Analysis"" | table title | lookup CS_Lookup_Info.csv lookup as title Output static, description",,,"list_of_lookups"
tbd,,,,,,"CyberSentry_Training",,"| inputlookup $lookup$",,,"list_of_lookups"
tbd,,,,,,"CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/admin/macros 
        
| stats count by eai:acl.app
| sort eai:acl.app
| fields eai:acl.app",,,"list_of_macros"
tbd,,,,,"lookup app_macros.csv","CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/admin/macros
| search eai:acl.app=$app$ AND author!=""nobody""
| lookup app_macros.csv Name as title output Details
| table title, args, eai:acl.app, definition Details",,,"list_of_macros"
tbd,,,,,,"CyberSentry_Training",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| stats count by eai:acl.app
| sort eai:acl.app
| fields eai:acl.app",,,"list_of_saved_searches"
tbd,,,,"inputlookup savedsearch_details.csv",,"CyberSentry_Training",,"| inputlookup savedsearch_details.csv | stats count by usecase
| fields usecase",,,"list_of_saved_searches"
tbd,,,,,"lookup saved_search_details.csv","CyberSentry_Training",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=$app$ AND author!=""nobody"" 
| lookup saved_search_details.csv title as title output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| search usecase=""$category$""
| table title, eai:acl.app, actions, cron_schedule, action.summary_index._name, details, mitre, usecase",,,"list_of_saved_searches"
tbd,,,,,,"lame_documentation",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| stats count by eai:acl.app
| sort eai:acl.app
| fields eai:acl.app",,,"list_of_saved_searches"
tbd,,,,"inputlookup saved_search_details.csv",,"lame_documentation",,"| inputlookup saved_search_details.csv | stats count by usecase
| fields usecase",,,"list_of_saved_searches"
tbd,,,,,"lookup saved_search_details.csv","lame_documentation",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=$app$ AND author!=""nobody"" 
| lookup saved_search_details.csv title as title output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| search usecase=""$category$""
| table title, eai:acl.app, actions, cron_schedule, action.summary_index._name, details, mitre, usecase",,,"list_of_saved_searches"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=*  | stats count by index  | fields index",,,"list_of_sources"
tbd,,,,"inputlookup source_details.csv",,"CyberSentry_Training",,"| inputlookup source_details.csv | stats count by usegroup | sort - usegroup
| fields usegroup",,,"list_of_sources"
tbd,,,,,"lookup source_details.csv","CyberSentry_Training",,"| metadata index=$index$ type=sources
| lookup source_details.csv source as source output description as description usegroup as usegroup
| fillnull value=""N/A"" usegroup
| search usegroup = ""$myType$""
| table source, totalCount, description, usegroup",,,"list_of_sources"
tbd,,,"index=*",,,"lame_documentation",,"| eventcount summarize=false index=*  | stats count by index  | sort index",,,"list_of_sourcetypes"
tbd,,,,"inputlookup sourcetype_details.csv",,"lame_documentation",,"| inputlookup sourcetype_details.csv | stats count by usegroup
| sort - usegroup
| fields usegroup",,,"list_of_sourcetypes"
tbd,,,,,"lookup sourcetype_details.csv","lame_documentation",,"| metadata index=$index$ type=sourcetypes
| table sourcetype, totalCount
| lookup sourcetype_details.csv sourcetype as sourcetype Output description as desc usegroup as usegroup
| fillnull value=""N/A"" usegroup
| search usegroup = $mytype$
| table sourcetype, totalCount, desc,  usegroup",,,"list_of_sourcetypes"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=*  | stats count by index  | sort index",,,"list_of_sourcetypes"
tbd,,,,"inputlookup sourcetype_details.csv",,"CyberSentry_Training",,"| inputlookup sourcetype_details.csv | stats count by usegroup
| sort - usegroup
| fields usegroup",,,"list_of_sourcetypes"
tbd,,,,,"lookup sourcetype_details.csv","CyberSentry_Training",,"| metadata index=$index$ type=sourcetypes
| table sourcetype, totalCount
| lookup sourcetype_details.csv sourcetype as sourcetype Output description as desc usegroup as usegroup
| fillnull value=""N/A"" usegroup
| search usegroup = $mytype$
| table sourcetype, totalCount, desc,  usegroup",,,"list_of_sourcetypes"
tbd,,,"index=*",,,"lame_documentation",,"| eventcount summarize=false index=*  | stats count by index  | fields index",,,"list_of_summary_indexes_"
tbd,,,,"inputlookup source_details.csv",,"lame_documentation",,"| inputlookup source_details.csv | stats count by usegroup | sort - usegroup
| fields usegroup",,,"list_of_summary_indexes_"
tbd,,,,,"lookup source_details.csv","lame_documentation",,"| metadata index=$index$ type=sources
| lookup source_details.csv source as source output description as description usegroup as usegroup
| fillnull value=""N/A"" usegroup
| search usegroup = ""$myType$""
| table source, totalCount, description, usegroup",,,"list_of_summary_indexes_"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) $severity$ | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)"" | fillnull severity value=""UNDEFINED"" | timechart count(severity) as count by severity",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)"" | fillnull value=""undefined"" vendor_severity | stats sparkline count by severity | sort -count",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) $severity$
          | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)""
          | sort -_time
          | eval time=_time
          | convert ctime(time)
          | table time severity message",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,,,,"lookup_editor",,"| stats count as value | eval value=""Offline"" | append [rest /services/data/lookup_edit/ping | fields value] | stats last(value) as status | eval range=if(status==""Offline"", ""severe"", ""low"")",,,"lookup_editor_status"
tbd,,,,,,"lookup_editor",,"| stats count as value | eval value=""Offline"" | append [rest /services/data/lookup_backup/ping | fields value] | stats last(value) as status | eval range=if(status==""Offline"", ""severe"", ""low"")",,,"lookup_editor_status"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal sourcetype=lookup_editor_rest_handler | timechart count",,"sourcetype=lookup_editor_rest_handler","lookup_editor_status"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal sourcetype=lookup_backups_rest_handler | timechart count",,"sourcetype=lookup_backups_rest_handler","lookup_editor_status"
tbd,,,,,,search,,"| mstats avg(""cpu_metric.pctIdle"") prestats=true WHERE ""index""=""itsi_im_metrics"" $mstats_span$ BY host
| timechart avg(""cpu_metric.pctIdle"") $timechart_span$ useother=false BY host WHERE max in top5
| fields - _span*",,,"metrics_dashboard"
tbd,,,,,"lookup LAME_Alert_Whitelist","log_analysis_made_easy",,"| tstats count from datamodel=Intrusion_Detection groupby IDS_Attacks.signature, IDS_Attacks.src, IDS_Attacks.dest
| rename IDS_Attacks.signature as alert, IDS_Attacks.src as src_ip, IDS_Attacks.dest as dest_ip
| lookup LAME_Alert_Whitelist src_ip as src_ip dest_ip as dest_ip output rationale
| fillnull value=""no match"" rationale

| sort - count",,,"nonwhitelisted_alerts"
tbd,,,,,,"simple_xml_examples",,"$index_switcher$ |  top sourcetype",,,"null_search_swapper"
tbd,,,,,,"network-diagram-viz",,"| makeresults count=$numNodes$
| streamstats count as id
| eval from=id
| eval id= id % 1359 +1
| eval type=case(id=1,""500px"", id=2,""accessible-icon"", id=3,""accusoft"", id=4,""acquisitions-incorporated"", id=5,""ad"", id=6,""address-book"", id=7,""address-card"", id=8,""adjust"", id=9,""adn"", id=10,""adobe"", id=11,""adversal"", id=12,""affiliatetheme"", id=13,""air-freshener"", id=14,""airbnb"", id=15,""algolia"", id=16,""align-center"", id=17,""align-justify"", id=18,""align-left"", id=19,""align-right"", id=20,""alipay"", id=21,""allergies"", id=22,""amazon"", id=23,""amazon-pay"", id=24,""ambulance"", id=25,""american-sign-language-interpreting"", id=26,""amilia"", id=27,""anchor"", id=28,""android"", id=29,""angellist"", id=30,""angle-double-down"", id=31,""angle-double-left"", id=32,""angle-double-right"", id=33,""angle-double-up"", id=34,""angle-down"", id=35,""angle-left"", id=36,""angle-right"", id=37,""angle-up"", id=38,""angry"", id=39,""angrycreative"", id=40,""angular"", id=41,""ankh"", id=42,""app-store"", id=43,""app-store-ios"", id=44,""apper"", id=45,""apple"", id=46,""apple-alt"", id=47,""apple-pay"", id=48,""archive"", id=49,""archway"", id=50,""arrow-alt-circle-down"", id=51,""arrow-alt-circle-left"", id=52,""arrow-alt-circle-right"", id=53,""arrow-alt-circle-up"", id=54,""arrow-circle-down"", id=55,""arrow-circle-left"", id=56,""arrow-circle-right"", id=57,""arrow-circle-up"", id=58,""arrow-down"", id=59,""arrow-left"", id=60,""arrow-right"", id=61,""arrow-up"", id=62,""arrows-alt"", id=63,""arrows-alt-h"", id=64,""arrows-alt-v"", id=65,""artstation"", id=66,""assistive-listening-systems"", id=67,""asterisk"", id=68,""asymmetrik"", id=69,""at"", id=70,""atlas"", id=71,""atlassian"", id=72,""atom"", id=73,""audible"", id=74,""audio-description"", id=75,""autoprefixer"", id=76,""avianex"", id=77,""aviato"", id=78,""award"", id=79,""aws"", id=80,""baby"", id=81,""baby-carriage"", id=82,""backspace"", id=83,""backward"", id=84,""bacon"", id=85,""balance-scale"", id=86,""ban"", id=87,""band-aid"", id=88,""bandcamp"", id=89,""barcode"", id=90,""bars"", id=91,""baseball-ball"", id=92,""basketball-ball"", id=93,""bath"", id=94,""battery-empty"", id=95,""battery-full"", id=96,""battery-half"", id=97,""battery-quarter"", id=98,""battery-three-quarters"", id=99,""battle-net"", id=100,""bed"", id=101,""beer"", id=102,""behance"", id=103,""behance-square"", id=104,""bell"", id=105,""bell-slash"", id=106,""bezier-curve"", id=107,""bible"", id=108,""bicycle"", id=109,""bimobject"", id=110,""binoculars"", id=111,""biohazard"", id=112,""birthday-cake"", id=113,""bitbucket"", id=114,""bitcoin"", id=115,""bity"", id=116,""black-tie"", id=117,""blackberry"", id=118,""blender"", id=119,""blender-phone"", id=120,""blind"", id=121,""blog"", id=122,""blogger"", id=123,""blogger-b"", id=124,""bluetooth"", id=125,""bluetooth-b"", id=126,""bold"", id=127,""bolt"", id=128,""bomb"", id=129,""bone"", id=130,""bong"", id=131,""book"", id=132,""book-dead"", id=133,""book-medical"", id=134,""book-open"", id=135,""book-reader"", id=136,""bookmark"", id=137,""bootstrap"", id=138,""bowling-ball"", id=139,""box"", id=140,""box-open"", id=141,""boxes"", id=142,""braille"", id=143,""brain"", id=144,""bread-slice"", id=145,""briefcase"", id=146,""briefcase-medical"", id=147,""broadcast-tower"", id=148,""broom"", id=149,""brush"", id=150,""btc"", id=151,""buffer"", id=152,""bug"", id=153,""building"", id=154,""bullhorn"", id=155,""bullseye"", id=156,""burn"", id=157,""buromobelexperte"", id=158,""bus"", id=159,""bus-alt"", id=160,""business-time"", id=161,""buysellads"", id=162,""calculator"", id=163,""calendar"", id=164,""calendar-alt"", id=165,""calendar-check"", id=166,""calendar-day"", id=167,""calendar-minus"", id=168,""calendar-plus"", id=169,""calendar-times"", id=170,""calendar-week"", id=171,""camera"", id=172,""camera-retro"", id=173,""campground"", id=174,""canadian-maple-leaf"", id=175,""candy-cane"", id=176,""cannabis"", id=177,""capsules"", id=178,""car"", id=179,""car-alt"", id=180,""car-battery"", id=181,""car-crash"", id=182,""car-side"", id=183,""caret-down"", id=184,""caret-left"", id=185,""caret-right"", id=186,""caret-square-down"", id=187,""caret-square-left"", id=188,""caret-square-right"", id=189,""caret-square-up"", id=190,""caret-up"", id=191,""carrot"", id=192,""cart-arrow-down"", id=193,""cart-plus"", id=194,""cash-register"", id=195,""cat"", id=196,""cc-amazon-pay"", id=197,""cc-amex"", id=198,""cc-apple-pay"", id=199,""cc-diners-club"", id=200,""cc-discover"", id=201,""cc-jcb"", id=202,""cc-mastercard"", id=203,""cc-paypal"", id=204,""cc-stripe"", id=205,""cc-visa"", id=206,""centercode"", id=207,""centos"", id=208,""certificate"", id=209,""chair"", id=210,""chalkboard"", id=211,""chalkboard-teacher"", id=212,""charging-station"", id=213,""chart-area"", id=214,""chart-bar"", id=215,""chart-line"", id=216,""chart-pie"", id=217,""check"", id=218,""check-circle"", id=219,""check-double"", id=220,""check-square"", id=221,""cheese"", id=222,""chess"", id=223,""chess-bishop"", id=224,""chess-board"", id=225,""chess-king"", id=226,""chess-knight"", id=227,""chess-pawn"", id=228,""chess-queen"", id=229,""chess-rook"", id=230,""chevron-circle-down"", id=231,""chevron-circle-left"", id=232,""chevron-circle-right"", id=233,""chevron-circle-up"", id=234,""chevron-down"", id=235,""chevron-left"", id=236,""chevron-right"", id=237,""chevron-up"", id=238,""child"", id=239,""chrome"", id=240,""chromecast"", id=241,""church"", id=242,""circle"", id=243,""circle-notch"", id=244,""city"", id=245,""clinic-medical"", id=246,""clipboard"", id=247,""clipboard-check"", id=248,""clipboard-list"", id=249,""clock"", id=250,""clone"", id=251,""closed-captioning"", id=252,""cloud"", id=253,""cloud-download-alt"", id=254,""cloud-meatball"", id=255,""cloud-moon"", id=256,""cloud-moon-rain"", id=257,""cloud-rain"", id=258,""cloud-showers-heavy"", id=259,""cloud-sun"", id=260,""cloud-sun-rain"", id=261,""cloud-upload-alt"", id=262,""cloudscale"", id=263,""cloudsmith"", id=264,""cloudversify"", id=265,""cocktail"", id=266,""code"", id=267,""code-branch"", id=268,""codepen"", id=269,""codiepie"", id=270,""coffee"", id=271,""cog"", id=272,""cogs"", id=273,""coins"", id=274,""columns"", id=275,""comment"", id=276,""comment-alt"", id=277,""comment-dollar"", id=278,""comment-dots"", id=279,""comment-medical"", id=280,""comment-slash"", id=281,""comments"", id=282,""comments-dollar"", id=283,""compact-disc"", id=284,""compass"", id=285,""compress"", id=286,""compress-arrows-alt"", id=287,""concierge-bell"", id=288,""confluence"", id=289,""connectdevelop"", id=290,""contao"", id=291,""cookie"", id=292,""cookie-bite"", id=293,""copy"", id=294,""copyright"", id=295,""couch"", id=296,""cpanel"", id=297,""creative-commons"", id=298,""creative-commons-by"", id=299,""creative-commons-nc"", id=300,""creative-commons-nc-eu"", id=301,""creative-commons-nc-jp"", id=302,""creative-commons-nd"", id=303,""creative-commons-pd"", id=304,""creative-commons-pd-alt"", id=305,""creative-commons-remix"", id=306,""creative-commons-sa"", id=307,""creative-commons-sampling"", id=308,""creative-commons-sampling-plus"", id=309,""creative-commons-share"", id=310,""creative-commons-zero"", id=311,""credit-card"", id=312,""critical-role"", id=313,""crop"", id=314,""crop-alt"", id=315,""cross"", id=316,""crosshairs"", id=317,""crow"", id=318,""crown"", id=319,""crutch"", id=320,""css3"", id=321,""css3-alt"", id=322,""cube"", id=323,""cubes"", id=324,""cut"", id=325,""cuttlefish"", id=326,""d-and-d"", id=327,""d-and-d-beyond"", id=328,""dashcube"", id=329,""database"", id=330,""deaf"", id=331,""delicious"", id=332,""democrat"", id=333,""deploydog"", id=334,""deskpro"", id=335,""desktop"", id=336,""dev"", id=337,""deviantart"", id=338,""dharmachakra"", id=339,""dhl"", id=340,""diagnoses"", id=341,""diaspora"", id=342,""dice"", id=343,""dice-d20"", id=344,""dice-d6"", id=345,""dice-five"", id=346,""dice-four"", id=347,""dice-one"", id=348,""dice-six"", id=349,""dice-three"", id=350,""dice-two"", id=351,""digg"", id=352,""digital-ocean"", id=353,""digital-tachograph"", id=354,""directions"", id=355,""discord"", id=356,""discourse"", id=357,""divide"", id=358,""dizzy"", id=359,""dna"", id=360,""dochub"", id=361,""docker"", id=362,""dog"", id=363,""dollar-sign"", id=364,""dolly"", id=365,""dolly-flatbed"", id=366,""donate"", id=367,""door-closed"", id=368,""door-open"", id=369,""dot-circle"", id=370,""dove"", id=371,""download"", id=372,""draft2digital"", id=373,""drafting-compass"", id=374,""dragon"", id=375,""draw-polygon"", id=376,""dribbble"", id=377,""dribbble-square"", id=378,""dropbox"", id=379,""drum"", id=380,""drum-steelpan"", id=381,""drumstick-bite"", id=382,""drupal"", id=383,""dumbbell"", id=384,""dumpster"", id=385,""dumpster-fire"", id=386,""dungeon"", id=387,""dyalog"", id=388,""earlybirds"", id=389,""ebay"", id=390,""edge"", id=391,""edit"", id=392,""egg"", id=393,""eject"", id=394,""elementor"", id=395,""ellipsis-h"", id=396,""ellipsis-v"", id=397,""ello"", id=398,""ember"", id=399,""empire"", id=400,""envelope"", id=401,""envelope-open"", id=402,""envelope-open-text"", id=403,""envelope-square"", id=404,""envira"", id=405,""equals"", id=406,""eraser"", id=407,""erlang"", id=408,""ethereum"", id=409,""ethernet"", id=410,""etsy"", id=411,""euro-sign"", id=412,""evernote"", id=413,""exchange-alt"", id=414,""exclamation"", id=415,""exclamation-circle"", id=416,""exclamation-triangle"", id=417,""expand"", id=418,""expand-arrows-alt"", id=419,""expeditedssl"", id=420,""external-link-alt"", id=421,""external-link-square-alt"", id=422,""eye"", id=423,""eye-dropper"", id=424,""eye-slash"", id=425,""facebook"", id=426,""facebook-f"", id=427,""facebook-messenger"", id=428,""facebook-square"", id=429,""fantasy-flight-games"", id=430,""fast-backward"", id=431,""fast-forward"", id=432,""fax"", id=433,""feather"", id=434,""feather-alt"", id=435,""fedex"", id=436,""fedora"", id=437,""female"", id=438,""fighter-jet"", id=439,""figma"", id=440,""file"", id=441,""file-alt"", id=442,""file-archive"", id=443,""file-audio"", id=444,""file-code"", id=445,""file-contract"", id=446,""file-csv"", id=447,""file-download"", id=448,""file-excel"", id=449,""file-export"", id=450,""file-image"", id=451,""file-import"", id=452,""file-invoice"", id=453,""file-invoice-dollar"", id=454,""file-medical"", id=455,""file-medical-alt"", id=456,""file-pdf"", id=457,""file-powerpoint"", id=458,""file-prescription"", id=459,""file-signature"", id=460,""file-upload"", id=461,""file-video"", id=462,""file-word"", id=463,""fill"", id=464,""fill-drip"", id=465,""film"", id=466,""filter"", id=467,""fingerprint"", id=468,""fire"", id=469,""fire-alt"", id=470,""fire-extinguisher"", id=471,""firefox"", id=472,""first-aid"", id=473,""first-order"", id=474,""first-order-alt"", id=475,""firstdraft"", id=476,""fish"", id=477,""fist-raised"", id=478,""flag"", id=479,""flag-checkered"", id=480,""flag-usa"", id=481,""flask"", id=482,""flickr"", id=483,""flipboard"", id=484,""flushed"", id=485,""fly"", id=486,""folder"", id=487,""folder-minus"", id=488,""folder-open"", id=489,""folder-plus"", id=490,""font"", id=491,""font-awesome"", id=492,""font-awesome-alt"", id=493,""font-awesome-flag"", id=494,""fonticons"", id=495,""fonticons-fi"", id=496,""football-ball"", id=497,""fort-awesome"", id=498,""fort-awesome-alt"", id=499,""forumbee"", id=500,""forward"", id=501,""foursquare"", id=502,""free-code-camp"", id=503,""freebsd"", id=504,""frog"", id=505,""frown"", id=506,""frown-open"", id=507,""fulcrum"", id=508,""funnel-dollar"", id=509,""futbol"", id=510,""galactic-republic"", id=511,""galactic-senate"", id=512,""gamepad"", id=513,""gas-pump"", id=514,""gavel"", id=515,""gem"", id=516,""genderless"", id=517,""get-pocket"", id=518,""gg"", id=519,""gg-circle"", id=520,""ghost"", id=521,""gift"", id=522,""gifts"", id=523,""git"", id=524,""git-square"", id=525,""github"", id=526,""github-alt"", id=527,""github-square"", id=528,""gitkraken"", id=529,""gitlab"", id=530,""gitter"", id=531,""glass-cheers"", id=532,""glass-martini"", id=533,""glass-martini-alt"", id=534,""glass-whiskey"", id=535,""glasses"", id=536,""glide"", id=537,""glide-g"", id=538,""globe"", id=539,""globe-africa"", id=540,""globe-americas"", id=541,""globe-asia"", id=542,""globe-europe"", id=543,""gofore"", id=544,""golf-ball"", id=545,""goodreads"", id=546,""goodreads-g"", id=547,""google"", id=548,""google-drive"", id=549,""google-play"", id=550,""google-plus"", id=551,""google-plus-g"", id=552,""google-plus-square"", id=553,""google-wallet"", id=554,""gopuram"", id=555,""graduation-cap"", id=556,""gratipay"", id=557,""grav"", id=558,""greater-than"", id=559,""greater-than-equal"", id=560,""grimace"", id=561,""grin"", id=562,""grin-alt"", id=563,""grin-beam"", id=564,""grin-beam-sweat"", id=565,""grin-hearts"", id=566,""grin-squint"", id=567,""grin-squint-tears"", id=568,""grin-stars"", id=569,""grin-tears"", id=570,""grin-tongue"", id=571,""grin-tongue-squint"", id=572,""grin-tongue-wink"", id=573,""grin-wink"", id=574,""grip-horizontal"", id=575,""grip-lines"", id=576,""grip-lines-vertical"", id=577,""grip-vertical"", id=578,""gripfire"", id=579,""grunt"", id=580,""guitar"", id=581,""gulp"", id=582,""h-square"", id=583,""hacker-news"", id=584,""hacker-news-square"", id=585,""hackerrank"", id=586,""hamburger"", id=587,""hammer"", id=588,""hamsa"", id=589,""hand-holding"", id=590,""hand-holding-heart"", id=591,""hand-holding-usd"", id=592,""hand-lizard"", id=593,""hand-middle-finger"", id=594,""hand-paper"", id=595,""hand-peace"", id=596,""hand-point-down"", id=597,""hand-point-left"", id=598,""hand-point-right"", id=599,""hand-point-up"", id=600,""hand-pointer"", id=601,""hand-rock"", id=602,""hand-scissors"", id=603,""hand-spock"", id=604,""hands"", id=605,""hands-helping"", id=606,""handshake"", id=607,""hanukiah"", id=608,""hard-hat"", id=609,""hashtag"", id=610,""hat-wizard"", id=611,""haykal"", id=612,""hdd"", id=613,""heading"", id=614,""headphones"", id=615,""headphones-alt"", id=616,""headset"", id=617,""heart"", id=618,""heart-broken"", id=619,""heartbeat"", id=620,""helicopter"", id=621,""highlighter"", id=622,""hiking"", id=623,""hippo"", id=624,""hips"", id=625,""hire-a-helper"", id=626,""history"", id=627,""hockey-puck"", id=628,""holly-berry"", id=629,""home"", id=630,""hooli"", id=631,""hornbill"", id=632,""horse"", id=633,""horse-head"", id=634,""hospital"", id=635,""hospital-alt"", id=636,""hospital-symbol"", id=637,""hot-tub"", id=638,""hotdog"", id=639,""hotel"", id=640,""hotjar"", id=641,""hourglass"", id=642,""hourglass-end"", id=643,""hourglass-half"", id=644,""hourglass-start"", id=645,""house-damage"", id=646,""houzz"", id=647,""hryvnia"", id=648,""html5"", id=649,""hubspot"", id=650,""i-cursor"", id=651,""ice-cream"", id=652,""icicles"", id=653,""id-badge"", id=654,""id-card"", id=655,""id-card-alt"", id=656,""igloo"", id=657,""image"", id=658,""images"", id=659,""imdb"", id=660,""inbox"", id=661,""indent"", id=662,""industry"", id=663,""infinity"", id=664,""info"", id=665,""info-circle"", id=666,""instagram"", id=667,""intercom"", id=668,""internet-explorer"", id=669,""invision"", id=670,""ioxhost"", id=671,""italic"", id=672,""itch-io"", id=673,""itunes"", id=674,""itunes-note"", id=675,""java"", id=676,""jedi"", id=677,""jedi-order"", id=678,""jenkins"", id=679,""jira"", id=680,""joget"", id=681,""joint"", id=682,""joomla"", id=683,""journal-whills"", id=684,""js"", id=685,""js-square"", id=686,""jsfiddle"", id=687,""kaaba"", id=688,""kaggle"", id=689,""key"", id=690,""keybase"", id=691,""keyboard"", id=692,""keycdn"", id=693,""khanda"", id=694,""kickstarter"", id=695,""kickstarter-k"", id=696,""kiss"", id=697,""kiss-beam"", id=698,""kiss-wink-heart"", id=699,""kiwi-bird"", id=700,""korvue"", id=701,""landmark"", id=702,""language"", id=703,""laptop"", id=704,""laptop-code"", id=705,""laptop-medical"", id=706,""laravel"", id=707,""lastfm"", id=708,""lastfm-square"", id=709,""laugh"", id=710,""laugh-beam"", id=711,""laugh-squint"", id=712,""laugh-wink"", id=713,""layer-group"", id=714,""leaf"", id=715,""leanpub"", id=716,""lemon"", id=717,""less"", id=718,""less-than"", id=719,""less-than-equal"", id=720,""level-down-alt"", id=721,""level-up-alt"", id=722,""life-ring"", id=723,""lightbulb"", id=724,""line"", id=725,""link"", id=726,""linkedin"", id=727,""linkedin-in"", id=728,""linode"", id=729,""linux"", id=730,""lira-sign"", id=731,""list"", id=732,""list-alt"", id=733,""list-ol"", id=734,""list-ul"", id=735,""location-arrow"", id=736,""lock"", id=737,""lock-open"", id=738,""long-arrow-alt-down"", id=739,""long-arrow-alt-left"", id=740,""long-arrow-alt-right"", id=741,""long-arrow-alt-up"", id=742,""low-vision"", id=743,""luggage-cart"", id=744,""lyft"", id=745,""magento"", id=746,""magic"", id=747,""magnet"", id=748,""mail-bulk"", id=749,""mailchimp"", id=750,""male"", id=751,""mandalorian"", id=752,""map"", id=753,""map-marked"", id=754,""map-marked-alt"", id=755,""map-marker"", id=756,""map-marker-alt"", id=757,""map-pin"", id=758,""map-signs"", id=759,""markdown"", id=760,""marker"", id=761,""mars"", id=762,""mars-double"", id=763,""mars-stroke"", id=764,""mars-stroke-h"", id=765,""mars-stroke-v"", id=766,""mask"", id=767,""mastodon"", id=768,""maxcdn"", id=769,""medal"", id=770,""medapps"", id=771,""medium"", id=772,""medium-m"", id=773,""medkit"", id=774,""medrt"", id=775,""meetup"", id=776,""megaport"", id=777,""meh"", id=778,""meh-blank"", id=779,""meh-rolling-eyes"", id=780,""memory"", id=781,""mendeley"", id=782,""menorah"", id=783,""mercury"", id=784,""meteor"", id=785,""microchip"", id=786,""microphone"", id=787,""microphone-alt"", id=788,""microphone-alt-slash"", id=789,""microphone-slash"", id=790,""microscope"", id=791,""microsoft"", id=792,""minus"", id=793,""minus-circle"", id=794,""minus-square"", id=795,""mitten"", id=796,""mix"", id=797,""mixcloud"", id=798,""mizuni"", id=799,""mobile"", id=800,""mobile-alt"", id=801,""modx"", id=802,""monero"", id=803,""money-bill"", id=804,""money-bill-alt"", id=805,""money-bill-wave"", id=806,""money-bill-wave-alt"", id=807,""money-check"", id=808,""money-check-alt"", id=809,""monument"", id=810,""moon"", id=811,""mortar-pestle"", id=812,""mosque"", id=813,""motorcycle"", id=814,""mountain"", id=815,""mouse-pointer"", id=816,""mug-hot"", id=817,""music"", id=818,""napster"", id=819,""neos"", id=820,""network-wired"", id=821,""neuter"", id=822,""newspaper"", id=823,""nimblr"", id=824,""node"", id=825,""node-js"", id=826,""not-equal"", id=827,""notes-medical"", id=828,""npm"", id=829,""ns8"", id=830,""nutritionix"", id=831,""object-group"", id=832,""object-ungroup"", id=833,""odnoklassniki"", id=834,""odnoklassniki-square"", id=835,""oil-can"", id=836,""old-republic"", id=837,""om"", id=838,""opencart"", id=839,""openid"", id=840,""opera"", id=841,""optin-monster"", id=842,""osi"", id=843,""otter"", id=844,""outdent"", id=845,""page4"", id=846,""pagelines"", id=847,""pager"", id=848,""paint-brush"", id=849,""paint-roller"", id=850,""palette"", id=851,""palfed"", id=852,""pallet"", id=853,""paper-plane"", id=854,""paperclip"", id=855,""parachute-box"", id=856,""paragraph"", id=857,""parking"", id=858,""passport"", id=859,""pastafarianism"", id=860,""paste"", id=861,""patreon"", id=862,""pause"", id=863,""pause-circle"", id=864,""paw"", id=865,""paypal"", id=866,""peace"", id=867,""pen"", id=868,""pen-alt"", id=869,""pen-fancy"", id=870,""pen-nib"", id=871,""pen-square"", id=872,""pencil-alt"", id=873,""pencil-ruler"", id=874,""penny-arcade"", id=875,""people-carry"", id=876,""pepper-hot"", id=877,""percent"", id=878,""percentage"", id=879,""periscope"", id=880,""person-booth"", id=881,""phabricator"", id=882,""phoenix-framework"", id=883,""phoenix-squadron"", id=884,""phone"", id=885,""phone-slash"", id=886,""phone-square"", id=887,""phone-volume"", id=888,""php"", id=889,""pied-piper"", id=890,""pied-piper-alt"", id=891,""pied-piper-hat"", id=892,""pied-piper-pp"", id=893,""piggy-bank"", id=894,""pills"", id=895,""pinterest"", id=896,""pinterest-p"", id=897,""pinterest-square"", id=898,""pizza-slice"", id=899,""place-of-worship"", id=900,""plane"", id=901,""plane-arrival"", id=902,""plane-departure"", id=903,""play"", id=904,""play-circle"", id=905,""playstation"", id=906,""plug"", id=907,""plus"", id=908,""plus-circle"", id=909,""plus-square"", id=910,""podcast"", id=911,""poll"", id=912,""poll-h"", id=913,""poo"", id=914,""poo-storm"", id=915,""poop"", id=916,""portrait"", id=917,""pound-sign"", id=918,""power-off"", id=919,""pray"", id=920,""praying-hands"", id=921,""prescription"", id=922,""prescription-bottle"", id=923,""prescription-bottle-alt"", id=924,""print"", id=925,""procedures"", id=926,""product-hunt"", id=927,""project-diagram"", id=928,""pushed"", id=929,""puzzle-piece"", id=930,""python"", id=931,""qq"", id=932,""qrcode"", id=933,""question"", id=934,""question-circle"", id=935,""quidditch"", id=936,""quinscape"", id=937,""quora"", id=938,""quote-left"", id=939,""quote-right"", id=940,""quran"", id=941,""r-project"", id=942,""radiation"", id=943,""radiation-alt"", id=944,""rainbow"", id=945,""random"", id=946,""raspberry-pi"", id=947,""ravelry"", id=948,""react"", id=949,""reacteurope"", id=950,""readme"", id=951,""rebel"", id=952,""receipt"", id=953,""recycle"", id=954,""red-river"", id=955,""reddit"", id=956,""reddit-alien"", id=957,""reddit-square"", id=958,""redhat"", id=959,""redo"", id=960,""redo-alt"", id=961,""registered"", id=962,""renren"", id=963,""reply"", id=964,""reply-all"", id=965,""replyd"", id=966,""republican"", id=967,""researchgate"", id=968,""resolving"", id=969,""restroom"", id=970,""retweet"", id=971,""rev"", id=972,""ribbon"", id=973,""ring"", id=974,""road"", id=975,""robot"", id=976,""rocket"", id=977,""rocketchat"", id=978,""rockrms"", id=979,""route"", id=980,""rss"", id=981,""rss-square"", id=982,""ruble-sign"", id=983,""ruler"", id=984,""ruler-combined"", id=985,""ruler-horizontal"", id=986,""ruler-vertical"", id=987,""running"", id=988,""rupee-sign"", id=989,""sad-cry"", id=990,""sad-tear"", id=991,""safari"", id=992,""salesforce"", id=993,""sass"", id=994,""satellite"", id=995,""satellite-dish"", id=996,""save"", id=997,""schlix"", id=998,""school"", id=999,""screwdriver"", id=1000,""scribd"", id=1001,""scroll"", id=1002,""sd-card"", id=1003,""search"", id=1004,""search-dollar"", id=1005,""search-location"", id=1006,""search-minus"", id=1007,""search-plus"", id=1008,""searchengin"", id=1009,""seedling"", id=1010,""sellcast"", id=1011,""sellsy"", id=1012,""server"", id=1013,""servicestack"", id=1014,""shapes"", id=1015,""share"", id=1016,""share-alt"", id=1017,""share-alt-square"", id=1018,""share-square"", id=1019,""shekel-sign"", id=1020,""shield-alt"", id=1021,""ship"", id=1022,""shipping-fast"", id=1023,""shirtsinbulk"", id=1024,""shoe-prints"", id=1025,""shopping-bag"", id=1026,""shopping-basket"", id=1027,""shopping-cart"", id=1028,""shopware"", id=1029,""shower"", id=1030,""shuttle-van"", id=1031,""sign"", id=1032,""sign-in-alt"", id=1033,""sign-language"", id=1034,""sign-out-alt"", id=1035,""signal"", id=1036,""signature"", id=1037,""sim-card"", id=1038,""simplybuilt"", id=1039,""sistrix"", id=1040,""sitemap"", id=1041,""sith"", id=1042,""skating"", id=1043,""sketch"", id=1044,""skiing"", id=1045,""skiing-nordic"", id=1046,""skull"", id=1047,""skull-crossbones"", id=1048,""skyatlas"", id=1049,""skype"", id=1050,""slack"", id=1051,""slack-hash"", id=1052,""slash"", id=1053,""sleigh"", id=1054,""sliders-h"", id=1055,""slideshare"", id=1056,""smile"", id=1057,""smile-beam"", id=1058,""smile-wink"", id=1059,""smog"", id=1060,""smoking"", id=1061,""smoking-ban"", id=1062,""sms"", id=1063,""snapchat"", id=1064,""snapchat-ghost"", id=1065,""snapchat-square"", id=1066,""snowboarding"", id=1067,""snowflake"", id=1068,""snowman"", id=1069,""snowplow"", id=1070,""socks"", id=1071,""solar-panel"", id=1072,""sort"", id=1073,""sort-alpha-down"", id=1074,""sort-alpha-up"", id=1075,""sort-amount-down"", id=1076,""sort-amount-up"", id=1077,""sort-down"", id=1078,""sort-numeric-down"", id=1079,""sort-numeric-up"", id=1080,""sort-up"", id=1081,""soundcloud"", id=1082,""sourcetree"", id=1083,""spa"", id=1084,""space-shuttle"", id=1085,""speakap"", id=1086,""speaker-deck"", id=1087,""spider"", id=1088,""spinner"", id=1089,""splotch"", id=1090,""spotify"", id=1091,""spray-can"", id=1092,""square"", id=1093,""square-full"", id=1094,""square-root-alt"", id=1095,""squarespace"", id=1096,""stack-exchange"", id=1097,""stack-overflow"", id=1098,""stamp"", id=1099,""star"", id=1100,""star-and-crescent"", id=1101,""star-half"", id=1102,""star-half-alt"", id=1103,""star-of-david"", id=1104,""star-of-life"", id=1105,""staylinked"", id=1106,""steam"", id=1107,""steam-square"", id=1108,""steam-symbol"", id=1109,""step-backward"", id=1110,""step-forward"", id=1111,""stethoscope"", id=1112,""sticker-mule"", id=1113,""sticky-note"", id=1114,""stop"", id=1115,""stop-circle"", id=1116,""stopwatch"", id=1117,""store"", id=1118,""store-alt"", id=1119,""strava"", id=1120,""stream"", id=1121,""street-view"", id=1122,""strikethrough"", id=1123,""stripe"", id=1124,""stripe-s"", id=1125,""stroopwafel"", id=1126,""studiovinari"", id=1127,""stumbleupon"", id=1128,""stumbleupon-circle"", id=1129,""subscript"", id=1130,""subway"", id=1131,""suitcase"", id=1132,""suitcase-rolling"", id=1133,""sun"", id=1134,""superpowers"", id=1135,""superscript"", id=1136,""supple"", id=1137,""surprise"", id=1138,""suse"", id=1139,""swatchbook"", id=1140,""swimmer"", id=1141,""swimming-pool"", id=1142,""symfony"", id=1143,""synagogue"", id=1144,""sync"", id=1145,""sync-alt"", id=1146,""syringe"", id=1147,""table"", id=1148,""table-tennis"", id=1149,""tablet"", id=1150,""tablet-alt"", id=1151,""tablets"", id=1152,""tachometer-alt"", id=1153,""tag"", id=1154,""tags"", id=1155,""tape"", id=1156,""tasks"", id=1157,""taxi"", id=1158,""teamspeak"", id=1159,""teeth"", id=1160,""teeth-open"", id=1161,""telegram"", id=1162,""telegram-plane"", id=1163,""temperature-high"", id=1164,""temperature-low"", id=1165,""tencent-weibo"", id=1166,""tenge"", id=1167,""terminal"", id=1168,""text-height"", id=1169,""text-width"", id=1170,""th"", id=1171,""th-large"", id=1172,""th-list"", id=1173,""the-red-yeti"", id=1174,""theater-masks"", id=1175,""themeco"", id=1176,""themeisle"", id=1177,""thermometer"", id=1178,""thermometer-empty"", id=1179,""thermometer-full"", id=1180,""thermometer-half"", id=1181,""thermometer-quarter"", id=1182,""thermometer-three-quarters"", id=1183,""think-peaks"", id=1184,""thumbs-down"", id=1185,""thumbs-up"", id=1186,""thumbtack"", id=1187,""ticket-alt"", id=1188,""times"", id=1189,""times-circle"", id=1190,""tint"", id=1191,""tint-slash"", id=1192,""tired"", id=1193,""toggle-off"", id=1194,""toggle-on"", id=1195,""toilet"", id=1196,""toilet-paper"", id=1197,""toolbox"", id=1198,""tools"", id=1199,""tooth"", id=1200,""torah"", id=1201,""torii-gate"", id=1202,""tractor"", id=1203,""trade-federation"", id=1204,""trademark"", id=1205,""traffic-light"", id=1206,""train"", id=1207,""tram"", id=1208,""transgender"", id=1209,""transgender-alt"", id=1210,""trash"", id=1211,""trash-alt"", id=1212,""trash-restore"", id=1213,""trash-restore-alt"", id=1214,""tree"", id=1215,""trello"", id=1216,""tripadvisor"", id=1217,""trophy"", id=1218,""truck"", id=1219,""truck-loading"", id=1220,""truck-monster"", id=1221,""truck-moving"", id=1222,""truck-pickup"", id=1223,""tshirt"", id=1224,""tty"", id=1225,""tumblr"", id=1226,""tumblr-square"", id=1227,""tv"", id=1228,""twitch"", id=1229,""twitter"", id=1230,""twitter-square"", id=1231,""typo3"", id=1232,""uber"", id=1233,""ubuntu"", id=1234,""uikit"", id=1235,""umbrella"", id=1236,""umbrella-beach"", id=1237,""underline"", id=1238,""undo"", id=1239,""undo-alt"", id=1240,""uniregistry"", id=1241,""universal-access"", id=1242,""university"", id=1243,""unlink"", id=1244,""unlock"", id=1245,""unlock-alt"", id=1246,""untappd"", id=1247,""upload"", id=1248,""ups"", id=1249,""usb"", id=1250,""user"", id=1251,""user-alt"", id=1252,""user-alt-slash"", id=1253,""user-astronaut"", id=1254,""user-check"", id=1255,""user-circle"", id=1256,""user-clock"", id=1257,""user-cog"", id=1258,""user-edit"", id=1259,""user-friends"", id=1260,""user-graduate"", id=1261,""user-injured"", id=1262,""user-lock"", id=1263,""user-md"", id=1264,""user-minus"", id=1265,""user-ninja"", id=1266,""user-nurse"", id=1267,""user-plus"", id=1268,""user-secret"", id=1269,""user-shield"", id=1270,""user-slash"", id=1271,""user-tag"", id=1272,""user-tie"", id=1273,""user-times"", id=1274,""users"", id=1275,""users-cog"", id=1276,""usps"", id=1277,""ussunnah"", id=1278,""utensil-spoon"", id=1279,""utensils"", id=1280,""vaadin"", id=1281,""vector-square"", id=1282,""venus"", id=1283,""venus-double"", id=1284,""venus-mars"", id=1285,""viacoin"", id=1286,""viadeo"", id=1287,""viadeo-square"", id=1288,""vial"", id=1289,""vials"", id=1290,""viber"", id=1291,""video"", id=1292,""video-slash"", id=1293,""vihara"", id=1294,""vimeo"", id=1295,""vimeo-square"", id=1296,""vimeo-v"", id=1297,""vine"", id=1298,""vk"", id=1299,""vnv"", id=1300,""volleyball-ball"", id=1301,""volume-down"", id=1302,""volume-mute"", id=1303,""volume-off"", id=1304,""volume-up"", id=1305,""vote-yea"", id=1306,""vr-cardboard"", id=1307,""vuejs"", id=1308,""walking"", id=1309,""wallet"", id=1310,""warehouse"", id=1311,""water"", id=1312,""waze"", id=1313,""weebly"", id=1314,""weibo"", id=1315,""weight"", id=1316,""weight-hanging"", id=1317,""weixin"", id=1318,""whatsapp"", id=1319,""whatsapp-square"", id=1320,""wheelchair"", id=1321,""whmcs"", id=1322,""wifi"", id=1323,""wikipedia-w"", id=1324,""wind"", id=1325,""window-close"", id=1326,""window-maximize"", id=1327,""window-minimize"", id=1328,""window-restore"", id=1329,""windows"", id=1330,""wine-bottle"", id=1331,""wine-glass"", id=1332,""wine-glass-alt"", id=1333,""wix"", id=1334,""wizards-of-the-coast"", id=1335,""wolf-pack-battalion"", id=1336,""won-sign"", id=1337,""wordpress"", id=1338,""wordpress-simple"", id=1339,""wpbeginner"", id=1340,""wpexplorer"", id=1341,""wpforms"", id=1342,""wpressr"", id=1343,""wrench"", id=1344,""x-ray"", id=1345,""xbox"", id=1346,""xing"", id=1347,""xing-square"", id=1348,""y-combinator"", id=1349,""yahoo"", id=1350,""yammer"", id=1351,""yandex"", id=1352,""yandex-international"", id=1353,""yarn"", id=1354,""yelp"", id=1355,""yen-sign"", id=1356,""yin-yang"", id=1357,""yoast"", id=1358,""youtube"", id=1359,""youtube-square"", id=1360,""zhihu"")
| eval to=random()%$numNodes$
|  eval color = random()%4 | eval color=case(color=0,""blue"", color=1,""yellow"",color=2,""green"", 1=1, ""red"")
| eval value = type
| table from, to, value, type, color",,,performance
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=""corelight_conn"" src_ip=$ip$
| head 100 
| table src_ip, dest_ip, dest_port",,"sourcetype=corelight_conn","protocol_anomaly_detection"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_cpu_deployment"
tbd,,,,,"lookup dmc_assets","splunk_monitoring_console",,"
      | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/hostwide
      | lookup dmc_assets serverName AS splunk_server OUTPUT search_group
      | where $role_post_filter$
      | join type=outer splunk_server [
          | `dmc_get_instance_roles` 
          | fields role serverName
          | rename serverName as splunk_server]
      | eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
      | eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
      | eval cpu_usage = 'cpu_system_pct' + 'cpu_user_pct'
      | fields splunk_server, role, normalized_load_avg_1min, core_info, cpu_usage, cpu_count, virtual_cpu_count, cpu_system_pct, cpu_user_pct
      | eval role = replace(role, "" $"", """")
      | eval role = split(role, "" "")
    ",,,"resource_usage_cpu_deployment"
tbd,,,,,"lookup dmc_assets","splunk_monitoring_console",,"
      <![CDATA[
      | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
      | lookup dmc_assets serverName AS splunk_server OUTPUT search_group
      | where $role_post_filter$
      | fields normalized_pct_cpu pct_cpu process process_type search_props.mode search_props.type search_props.provenance splunk_server
      | join type=outer splunk_server [
          | `dmc_get_instance_roles`
          | fields role serverName
          | `dmc_get_primary_role`
          | rename serverName AS splunk_server]
      | fillnull normalized_pct_cpu pct_cpu
      | eval process_type_l2 = if(match(process_type, ""^search$""), 'process_type'."":"".'search_props.type', 'process_type')
      | `dmc_pretty_print_role(primary_role)`
      | stats sum(normalized_pct_cpu) AS normalized_pct_cpu by process_type_l2, splunk_server, primary_role
      | chart avg(normalized_pct_cpu) AS normalized_pct_cpu over primary_role by process_type_l2
      | foreach * [eval <<FIELD>> = if(isnum('<<FIELD>>'), round('<<FIELD>>', 2), '<<FIELD>>')]
      ]]>
    ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" $role_post_filter$ sourcetype=splunk_resource_usage component=Hostwide
      | eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" $role_post_filter$ sourcetype=splunk_resource_usage component=PerProcess
      | `dmc_rename_introspection_fields`
      | `dmc_classify_processes`
      | `dmc_resource_usage_by_processes_timechart(normalized_pct_cpu, $funcPerpCPU$)`
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats sum(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_get_primary_role`
            | stats sum(virtual_cpu_count) by primary_role
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""search_head""
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""search_head""
            | stats median(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats dc(splunk_server) AS instance_count by role
            | where role = ""search_head""
            | fields instance_count
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""indexer""
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""indexer""
            | stats median(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats dc(splunk_server) AS instance_count by role
            | where role = ""indexer""
            | fields instance_count
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | fields splunk_server, role, core_info, cpu_usage, normalized_load_avg_1min
            | `dmc_pretty_print_role(role)`
            | sort - cpu_usage
            | rename splunk_server AS Instance, role as Role, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_set_bin`
            | stats latest(total_cpu_usage) as dedup_total_cpu_usage by host _time
            | `dmc_timechart` $avgCPUFunc$(dedup_total_cpu_usage) as cpu_usage
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | timechart minspan=10s bins=200 partial=f $countCPUFunc$(total_cpu_usage) as cpu_usage by host
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | eval server = host
            | `dmc_set_bin_for_timechart`
            | stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
            | `dmc_cpu_usage_rangemap_and_timechart`
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_drilldown_resource_usage_cpu_deployment_usage(""$role$"", ""$group$"", $role_post_filter$, $countCPUFunc$, $drilldown_cpu_usage_metric$)`
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/hostwide
      | join type=outer splunk_server [
          | `dmc_get_instance_roles` 
          | fields role serverName
          | rename serverName as splunk_server]
      | eventstats min(eval(if(isnull(normalized_load_avg_1min), ""0"", ""1""))) as _load_avg_full_availability
      | eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
      | eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
      | eval cpu_usage = 'cpu_system_pct' + 'cpu_user_pct'
      | fields splunk_server, role, normalized_load_avg_1min, core_info, cpu_usage, cpu_count, virtual_cpu_count, cpu_system_pct, cpu_user_pct
      | eval role = replace(role, "" $"", """")
      | eval role = split(role, "" "")
    ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
      | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
      | fields normalized_pct_cpu pct_cpu process process_type search_props.mode search_props.type search_props.provenance splunk_server
      | fillnull normalized_pct_cpu pct_cpu
      | eval process_type_l2 = if(match(process_type, ""^search$""), 'process_type'."":"".'search_props.type', 'process_type')
      | stats sum(normalized_pct_cpu) AS normalized_pct_cpu by process_type_l2, splunk_server
      | chart avg(normalized_pct_cpu) AS normalized_pct_cpu over splunk_server by process_type_l2
      | foreach * [eval <<FIELD>> = if(isnum('<<FIELD>>'), round('<<FIELD>>', 2), '<<FIELD>>')]
      ]]>
    ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess
      | `dmc_rename_introspection_fields`
      | `dmc_classify_processes`
      | `dmc_resource_usage_by_processes_timechart(normalized_pct_cpu, $funcPerpCPU$)`
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | fields splunk_server, role, core_info, cpu_usage, normalized_load_avg_1min
            | `dmc_pretty_print_role(role)`
            | sort - cpu_usage
            | rename splunk_server AS Instance, role as Role, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)""
          ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role$)` | search search_group!=""dmc_group_*""",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/hostwide
| join type=outer splunk_server [
  | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/iostats
  | eval iops = round(reads_ps + writes_ps)
  | eval iops_mountpoint = iops."" ("".mount_point."")""
  | eval cpupct_mountpoint = cpu_pct.""% ("".mount_point."")""
  | stats values(iops_mountpoint) as iops_mountpoint, values(cpupct_mountpoint) as cpupct_mountpoint by splunk_server]
| eventstats min(eval(if(isnull(normalized_load_avg_1min), ""0"", ""1""))) as _load_avg_full_availability
| eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
| eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
| eval cpu_usage = cpu_system_pct + cpu_user_pct
| eval mem_used_pct = round(mem_used / mem * 100 , 2)
| eval mem_used = round(mem_used, 0)
| eval mem = round(mem, 0)
| fields splunk_server, normalized_load_avg_1min, core_info, cpu_usage, mem, mem_used, mem_used_pct, iops_mountpoint, cpupct_mountpoint
| sort - cpu_usage, -mem_used
| rename splunk_server AS Instance, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)"", mem AS ""Physical Memory Capacity (MB)"", mem_used AS ""Physical Memory Usage (MB)"", mem_used_pct AS ""Physical Memory Usage (%)"", iops_mountpoint as ""I/O Operations per second (Mount Point)"", cpupct_mountpoint as ""Storage I/O Saturation (Mount Point)""
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countLoadAvgFunc$(data.normalized_load_avg_1min) as load_average by server _time
                    | `dmc_load_average_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countLoadAvgFunc$(data.normalized_load_avg_1min) as load_average by server _time
| `dmc_load_average_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_load_average(""$role$"", ""$group$"", $countLoadAvgFunc$, $drilldown_load_average_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
|`dmc_timechart` partial=f limit=25 $countLoadAvgFunc$(data.normalized_load_avg_1min) AS load_average by host
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval server = host
| `dmc_set_bin`
| stats latest(data.normalized_load_avg_1min) as dedup_load_average by server _time
| `dmc_timechart` $loadAvgFunc$(dedup_load_average) as load_average
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
                    | `dmc_cpu_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct' 
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
| `dmc_cpu_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_cpu_usage(""$role$"", ""$group$"", $countCPUFunc$, $drilldown_cpu_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
|`dmc_timechart` partial=f limit=25 $countCPUFunc$(total_cpu_usage) AS cpu_usage by host
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = ('data.cpu_system_pct' + 'data.cpu_user_pct')
| eval server = host
| `dmc_set_bin`
| stats latest(total_cpu_usage) as dedup_total_cpu_usage by server _time
| `dmc_timechart` $avgCPUFunc$(dedup_total_cpu_usage) as cpu_usage
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval pct_mem_used = 'data.mem_used' / 'data.mem'
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countMemFunc$(pct_mem_used) as pct_mem_used by server _time
                    | `dmc_memory_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_used = 'data.mem_used' / 'data.mem' 
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countMemFunc$(pct_mem_used) as pct_mem_used by server _time
| `dmc_memory_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_memory_usage(""$role$"", ""$group$"", $countMemFunc$, $drilldown_memory_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_used = 'data.mem_used' / 'data.mem' * 100
| eval server = host
| `dmc_set_bin_for_timechart`
| `dmc_timechart` partial=f limit=25 $countMemFunc$(pct_mem_used) AS pct_mem_used by server
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_usage = 'data.mem_used' / 'data.mem' * 100 
| eval server = host 
| `dmc_set_bin`
| stats latest(pct_mem_usage) as dedup_pct_mem_usage by server _time 
| `dmc_timechart` $avgMemFunc$(dedup_pct_mem_usage) as pct_mem_usage
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=IOStats
      | eval mount_point = 'data.mount_point'
      | eval cpu_pct = 'data.cpu_pct'
      | eval host_mountpoint = host."":"".mount_point
      | `dmc_set_bin_for_iostats`
    ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint, _time
                    | `dmc_iostats_rangemap(cpu_pct)`
                    | `dmc_timechart_for_iostats` dc(host_mountpoint) as count by range
                    | fields _time, ""80-100%"", ""60-80%"", ""0-60%"", ""abnormal""
                ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint, _time
            | `dmc_iostats_rangemap(cpu_pct)`
            | `dmc_timechart_for_iostats` dc(host_mountpoint) as count by range
            | fields _time, ""80-100%"", ""60-80%"", ""0-60%"", ""abnormal""
          ",,,"resource_usage_deployment"
tbd,,"host = replace",,,,"splunk_monitoring_console",,"
            stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint
            | `dmc_iostats_rangemap(cpu_pct)`
            | where range==$drilldown_iostats_metric|s$
            | eval host = replace(host_mountpoint, "":.*"", """")
            | eval mount_point = replace(host_mountpoint, "".*:"", """")
            | `dmc_drilldown_join_peers_by_peerURI`
            | eval Action = Action."" $role$""
            | fields serverName, machine, mount_point, cpu_pct, range, numberOfCores, ram, version, Action
            | rename serverName as Instance, machine as Machine, mount_point as ""Mount Point"", cpu_pct as ""I/O Bandwidth Utilizatio (%)"", range as ""Storage I/O Saturation Range"", version as Version, numberOfCores as Cores, ram as RAM
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` limit=25 $countIOstatsFunc$(cpu_pct) by host_mountpoint
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` $aggIOstatsFunc$(cpu_pct) as cpu_pct
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
                    | eval mount_point = 'data.mount_point'
                    | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
                    | eval pct_disk_usage = 1 - free / 'data.capacity'
                    | `dmc_set_bin_for_timechart_for_disk_usage`
                    | eval server_mount_point = host."":"".mount_point
                    | stats $countDiskFunc$(pct_disk_usage) as pct_disk_usage by server_mount_point _time
                    | `dmc_disk_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = 1 - free / 'data.capacity'
            | `dmc_set_bin_for_timechart_for_disk_usage`
            | eval server_mount_point = host."":"".mount_point
            | stats $countDiskFunc$(pct_disk_usage) as pct_disk_usage by server_mount_point _time
            | `dmc_disk_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_disk_usage(""$role$"", ""$group$"", $countDiskFunc$, $drilldown_disk_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = (1 - free / 'data.capacity') * 100
            | `dmc_set_bin_for_timechart_for_disk_usage`
            | eval server_mount_point = host."":"".mount_point
            | `dmc_timechart` partial=f limit=25 $countDiskFunc$(pct_disk_usage) AS pct_disk_usage by server_mount_point
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval server = host
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = (1 - free / 'data.capacity') * 100
            | `dmc_set_bin_for_disk_usage`
            | stats latest(pct_disk_usage) as dedup_pct_disk_usage by server mount_point _time
            | `dmc_timechart_for_disk_usage` $avgDiskFunc$(dedup_pct_disk_usage) as pct_disk_usage
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups`
          | search search_group=""dmc_group_*""
        ",,,"resource_usage_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_instance"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
| inputlookup dmc_assets
| search machine=$machine$
| mvcombine search_group
| join type=outer peerURI
  [| rest splunk_server=local /services/search/distributed/peers
   | rename title as peerURI]
| join type=outer peerURI [|rest splunk_server=local /services/server/info
  | eval peerURI = ""localhost""
  | eval status = ""Up""
  | fields peerURI, status, version]
| join type=outer peerURI [|rest splunk_server=local /services/server/settings
  | eval peerURI = ""localhost""
  | fields peerURI, mgmtHostPort]
| join peerURI
  [| `dmc_get_instance_roles` ]
| eval status = if(status == ""Up"", status, ""Unreachable"")
| makemv role
| fields serverName, peerURI, role, version, status, mgmtHostPort
| eval peerURI = if(peerURI == ""localhost"", ""localhost:"".mgmtHostPort, peerURI)
| fields - mgmtHostPort
| rename serverName as Instance, peerURI as URI, status as ""Status"", role as Role, version as ""Version""
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/server/status/resource-usage/hostwide 
| stats first(normalized_load_avg_1min) as load_average first(cpu_system_pct) as system, first(cpu_user_pct) as user first(mem) AS mem first(mem_used) AS mem_used by splunk_server
        ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"fields load_average",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"fields splunk_server system user",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
fields mem mem_used
| eval perc_80=mem*0.8 
| eval perc_90=mem*0.9 
| eval mem_used=round(mem_used, 0) 
| gauge mem_used 0 perc_80 perc_90, mem
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$instanceDiskUsageSnapshot$ /services/server/status/partitions-space
| join type=outer splunk_server, mount_point [
  | rest splunk_server=$instanceDiskUsageSnapshot$ /services/server/status/resource-usage/iostats
  | eval iops = round(reads_ps + writes_ps)
  | fields splunk_server, mount_point, iops, cpu_pct]
| eval free = if(isnotnull(available), available, free)
| eval usage = round((capacity - free) / 1024, 2)
| eval capacity = round(capacity / 1024, 2)
| eval compare_usage = usage."" / "".capacity
| eval pct_usage = round(usage / capacity * 100, 2) 
| stats first(fs_type) as fs_type first(compare_usage) as compare_usage first(pct_usage) as pct_usage, first(iops) as iops, first(cpu_pct) as cpu_pct by mount_point
| rename mount_point as ""Mount Point"", fs_type as ""File System Type"", compare_usage as ""Disk Usage (GB)"", capacity as ""Capacity (GB)"", pct_usage as ""Disk Usage (%)"", iops as ""I/O operations per second"", cpu_pct as ""Storage I/O Saturation(%)""
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` sourcetype=splunk_resource_usage component=IOStats host=$host$
          | eval mount_point = 'data.mount_point'
          | eval reads_ps = 'data.reads_ps'
          | eval writes_ps = 'data.writes_ps'
          | eval interval = 'data.interval'
          | eval op_count = (reads_ps + writes_ps) * interval
          | eval avg_service_ms = 'data.avg_service_ms'
          | eval avg_wait_ms = 'data.avg_total_ms'
          | eval cpu_pct = 'data.cpu_pct'
          | eval network_pct = 'data.network_pct'
        ",,"sourcetype=splunk_resource_usage","resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            | search data.mount_point=$io_mount_point|s$
            | `dmc_timechart_for_iostats` per_second(op_count) as iops, avg(data.cpu_pct) as avg_cpu_pct, avg(data.avg_service_ms) as avg_service_ms, avg(data.avg_total_ms) as avg_wait_ms, avg(data.network_pct) as avg_network_pct
            | eval iops = round(iops)
            | eval avg_cpu_pct = round(avg_cpu_pct)
            | eval avg_service_ms = round(avg_service_ms)
            | eval avg_wait_ms = round(avg_wait_ms)
            | eval avg_network_pct = round(avg_network_pct)
            | fields _time, iops $io_overlay$
            | rename $io_overlay$ as $io_overlay_label|s$
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` per_second(op_count) as op_count by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` avg($io_perf_metric$) by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group=""dmc_customgroup_*"" OR search_group=""dmc_indexerclustergroup_*"" OR search_group=""dmc_searchheadclustergroup_*""
        ",,,"scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /servicesNS/-/-/saved/searches preserve_args_case=t noProxy=t search=""is_scheduled=1"" search=""disabled=0""
            | stats count by splunk_server
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/status/limits/search-concurrency
              | fields splunk_server max_hist_scheduled_searches, max_rt_scheduled_searches]
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
              | search search_props.role=""head"" `dmc_match_all_scheduled_search_types`
              | dedup search_props.sid
              | stats count(eval('search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical"")) as count_hist_search, count(eval('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")) as count_rt_search by splunk_server ]
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/info
              | fields splunk_server, numberOfCores, numberOfVirtualCores]
            | eval count_hist_search = if(isnull(count_hist_search), 0, count_hist_search)
            | eval count_rt_search = if(isnull(count_rt_search), 0, count_rt_search)
            | eval hist_concur_vs_limit = count_hist_search."" / "".max_hist_scheduled_searches
            | eval rt_concur_vs_limit = count_rt_search."" / "".max_rt_scheduled_searches
            | `dmc_get_core_info`
            | fields splunk_server, core_info, hist_concur_vs_limit, rt_concur_vs_limit, count
            | rename splunk_server as Instance, core_info AS ""CPU Cores (Physical / Virtual)"", hist_concur_vs_limit as ""Concurrency of Historical Scheduled Report (Running/Limit)"", rt_concur_vs_limit as ""Concurrency of Real-time Scheduled Report (Running/Limit)"", count as ""Unique Scheduled Reports""
          ",,,"scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | timechart partial=f limit=20 count by $scheduler_execution_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | bin _time minspan=1min
            | stats count by host, _time
            | timechart partial=f $scheduler_execution_metric$
            | rename total as Total, min_count as Minimum, avg_count as Average, median_count as Median, max_count as Maximum
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats dc(sid) AS distinct_search_count by _time, host
            | `dmc_timechart` partial=f limit=20 $agg_report_concurrency_by_instance$(distinct_search_count) AS agg_distinct_search_count by host
            | eval agg_distinct_search_count = round(agg_distinct_search_count, 0)
            | rename agg_distinct_search_count as ""Concurrent Report Count""
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats dc(sid) AS distinct_search_count by _time, host
            | stats sum(distinct_search_count) as count by _time
            | `dmc_timechart` partial=f $agg_report_concurrency_all_instance$
            | rename min_count as Minimum, avg_count as Average, median_count as Median, max_count as Maximum
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | stats count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec by _time, host, app, savedsearch_name, user, savedsearch_id
            | `dmc_timechart` partial=f limit=20 eval(round(sum(skipped_exec) / sum(total_exec) * 100, 2)) as skip_ratio by host
            | rename skip_ratio as ""Skip Ratio""
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | stats count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec by _time, savedsearch_id
            | `dmc_timechart` partial=f eval(round(sum(skipped_exec) / sum(total_exec) * 100, 2)) as skip_ratio
            | rename skip_ratio as ""Skip Ratio""
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler status=""completed""
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart partial=f limit=20 eval(round(avg(execution_latency), 0)) as latency by $exec_lat_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler status=""completed""
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart partial=f $exec_lat_agg$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group!=""dmc_group_*""
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/info
            | eval core_info = if(isnull(numberOfCores), ""N/A"", numberOfCores).""/"".if(isnull(numberOfVirtualCores), ""N/A"", numberOfVirtualCores)
            | fields core_info
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
          | dedup search_props.sid
          | search `dmc_match_all_scheduled_search_types`
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats count(eval('search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical"")) as count_hist_scheduled_search, count(eval('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")) as count_rt_scheduled_search by splunk_server
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/limits/search-concurrency
          | fields max_hist_scheduled_searches, max_rt_scheduled_searches
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_search_limit = ""$count_hist_scheduled_search$"".""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_search_limit
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_search_limit = ""$count_rt_scheduled_search$"".""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_search_limit
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where 'search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical""
            | fields search_props.name, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed, search_props.sid
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.name as ""Scheduled Report Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Spent (sec)"", search_props.sid as ""SID""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where 'search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""
            | fields search_props.name, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed, search_props.sid
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.name as ""Scheduled Report Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Spent (sec)"", search_props.sid as ""SID""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | table splunk_server eai:acl.app eai:acl.owner cron_schedule title scheduled_times
            | mvexpand scheduled_times
            | rename scheduled_times as _time eai:acl.app as app eai:acl.owner as user title as search
            | stats count
            | eval count = `dmc_convert_count_unit(count)`
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
          | eval window_time = if(isnotnull(window_time), window_time, 0)
          | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
          | timechart span=1h partial=f avg(execution_latency) AS avg_exec_latency, count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec
          | eval skip_ratio = round(skipped_exec / total_exec * 100, 2)
          | eval avg_exec_latency = round(avg_exec_latency, 2)
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, skip_ratio",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, avg_exec_latency",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" search=""is_scheduled=1"" search=""disabled=0""
          | fields title, eai:acl.app, eai:acl.owner, cron_schedule, dispatch.earliest_time, dispatch.latest_time,
          schedule_window, actions
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by eai:acl.app",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by eai:acl.owner",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            $inventory_app_filter$
            | $inventory_user_filter$
            | eval dispatch.earliest_time = if(isnull('dispatch.earliest_time') OR 'dispatch.earliest_time' == ""0"" OR
            'dispatch.earliest_time'=="""", ""not set"", 'dispatch.earliest_time')
            | eval dispatch.latest_time = if(isnull('dispatch.latest_time') OR 'dispatch.latest_time' == """", ""not set"",
            'dispatch.latest_time')
            | eval actions = if(isnull(actions) OR actions == """", ""none"", actions)
            | rename title as ""Report Name"", eai:acl.app as App, eai:acl.owner as User, cron_schedule as ""Cron
            Schedule"", dispatch.earliest_time as ""Earliest Time"", dispatch.latest_time as ""Latest Time"", schedule_window as ""Schedule Window (minutes)"", actions as Actions
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
          status=""deferred"")
          | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
          | stats count by $scheduler_execution_split_by$
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename status as Status, app as App, user as User, savedsearch_name as ""Report Name"", alert_actions as ""Alert Actions"", count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"")
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | timechart partial=f count by $scheduler_execution_timechart_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=""searchscheduler"" host=$host$
            | `dmc_timechart_for_metrics_log` sum(completed) as completed_count,
            $scheduler_completion_runtime_aggregation$(total_runtime) as total_runtime
            | eval total_runtime=round(total_runtime, 0)
          ",,"sourcetype=splunkd","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.mode=""$searchMode$"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_rename_introspection_fields`
            | `dmc_set_bin`
            | stats dc(sid) AS distinct_search_count by _time, $concurrencySplitBy$
            | `dmc_timechart` $concurrencyFunction$(distinct_search_count) AS ""$concurrencyFunction$ of search
            concurrency"" by $concurrencySplitBy$
            | eval search_mode = ""$searchMode$""
            | eval scheduled_search_limit = case(match(search_mode, ""historical""), $max_hist_scheduled_searches$,
            match(search_mode, ""RT""), $max_rt_scheduled_searches$, true(), NULL)
            | fields - search_mode
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess data.search_props.sid::* data.search_props.role=""head"" `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats max(elapsed) as ELAPSED by sid $runtimeSplitBy$ _time
            | streamstats current=t global=f window=2 earliest(ELAPSED) as prev_ELAPSED latest(ELAPSED) as curr_ELAPSED by sid $runtimeSplitBy$
            | eval `dmc_collection_interval`
            | eval delta_ELAPSED = curr_ELAPSED - prev_ELAPSED
            | eval runtime = if(delta_ELAPSED = 0, min(curr_ELAPSED, collection_interval), delta_ELAPSED)
            | `dmc_timechart` sum(runtime) by $runtimeSplitBy$
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"")
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | stats avg(run_time) as runtime, avg(execution_latency) AS avg_exec_latency, count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec count(eval(status==""deferred"")) AS deferred_exec by app, savedsearch_name, user, savedsearch_id
            | join savedsearch_id type=outer [
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | search NOT (dispatch.earliest_time=rt* OR dispatch.latest_time=rt*)
            | stats dc(scheduled_times) as count max(scheduled_times) as max_t min(scheduled_times) as min_t by title, eai:acl.app, eai:acl.owner cron_schedule
            | eval schedule_interval=round((max_t-min_t)/(count-1), 0)
            | eval savedsearch_id = 'eai:acl.owner'."";"".'eai:acl.app'."";"".title
            | fields savedsearch_id, cron_schedule, schedule_interval ]
            | eval runtime = round(runtime, 0)
            | eval avg_exec_latency = round(avg_exec_latency, 0)
            | eval search_workload = round(runtime / schedule_interval * 100, 2)."" %""
            | eval skip_ratio = round(skipped_exec / total_exec * 100, 2)."" %""
            | fields savedsearch_name, app, user, cron_schedule, schedule_interval, runtime, search_workload, total_exec, skipped_exec, skip_ratio, deferred_exec, avg_exec_latency
            | sort - search_workload
            | rename savedsearch_name as ""Report Name"", app as App, user as User, cron_schedule as ""Cron Schedule"", runtime as ""Average Runtime (sec)"", total_exec as ""Total Executions"", skip_ratio as ""Skip Ratio"", skipped_exec as ""Skipped Executions"", deferred_exec AS ""Deferred Executions"", schedule_interval as ""Schedule Interval (sec)"", search_workload as ""Interval Load Factor"", avg_exec_latency AS ""Average Execution Latency (sec)""
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"") savedsearch_name=$runtime_statistics_drilldown|s$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""completed""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart eval(round($exe_lag_agg$(execution_latency), 0)) as latency by $exe_lag_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
          | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
          | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
          | stats count by $count_skipped_split_by$
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename reason as Reason, savedsearch_name as ""Report Name"", alert_actions as ""Alert Actions"", app as App, user as User, count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
            | timechart count by $count_skipped_timechart_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
            | stats count AS count values(alert_actions) AS alert_actions by savedsearch_name, reason
            | eval reason_and_count = reason."" ("".count."")""
            | stats values(reason_and_count) AS reasons first(alert_actions) AS alert_actions by savedsearch_name
            | rename reasons AS ""Skip Reason (Skip Count)"" alert_actions AS ""Alert Actions"" savedsearch_name AS ""Report
            Name""
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (log_level=""ERROR"" OR log_level=""WARN*"")
          | eval event_message=coalesce(event_message, message)
          | cluster t=0.7 field=event_message showcount=t countfield=count
          | table event_message, count, punct
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename event_message as Message, count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (log_level=""ERROR"" OR log_level=""WARN*"") punct=$warningErrorPunct|s$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role$)` | search search_group!=""dmc_group_*""",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/limits/search-concurrency
| join splunk_server type=outer [
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
| eval search_pct_cpu  = if(isnotnull('search_props.sid'), pct_cpu, 0)
| eval search_mem_used = if(isnotnull('search_props.sid'), mem_used, 0)
| eventstats sum(search_pct_cpu) as search_pct_cpu, sum(search_mem_used) as search_mem_used by search_props.sid
| dedup search_props.sid
| stats dc(search_props.sid) as search_count, sum(search_pct_cpu) as sum_pct_cpu, sum(search_mem_used) as sum_mem_used, sum(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as sum_auto_summary_search, sum(eval('search_props.type'==""ad-hoc"" AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as sum_rt_search, sum(eval('search_props.type'==""ad-hoc"" AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as sum_hist_search, sum(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as sum_rt_scheduled_search, sum(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as sum_hist_scheduled_search by splunk_server]
| eval sum_pct_cpu  = round(sum_pct_cpu, 2)
| eval sum_mem_used = round(sum_mem_used, 2)
| sort -search_count, -sum_pct_cpu
| eval search_count = if(isnull(search_count) OR search_count=="""", 0, search_count)
| eval sum_auto_summary_search = if(isnull(sum_auto_summary_search) OR sum_auto_summary_search=="""", 0, sum_auto_summary_search)
| eval sum_hist_scheduled_search = if(isnull(sum_hist_scheduled_search) OR sum_hist_scheduled_search=="""", 0, sum_hist_scheduled_search)
| eval sum_hist_search = if(isnull(sum_hist_search) OR sum_hist_search=="""", 0, sum_hist_search)
| eval sum_rt_scheduled_search = if(isnull(sum_rt_scheduled_search) OR sum_rt_scheduled_search=="""", 0, sum_rt_scheduled_search)
| eval sum_rt_search = if(isnull(sum_rt_search) OR sum_rt_search=="""", 0, sum_rt_search)
| eval sum_pct_cpu = if(isnull(sum_pct_cpu) OR sum_pct_cpu=="""", 0, sum_pct_cpu)
| eval count_cpu = round(sum_pct_cpu / 100.0, 2)
| eval sum_mem_used = if(isnull(sum_mem_used) OR search_count=="""", 0, sum_mem_used)
| eval scheduled_vs_total_auto_summary_search_limit = sum_auto_summary_search.""/"".max_auto_summary_searches
| eval scheduled_vs_total_hist_scheduled_search_limit = sum_hist_scheduled_search.""/"".max_hist_scheduled_searches
| eval scheduled_vs_total_hist_search_limit = sum_hist_search.""/"".max_hist_searches
| eval scheduled_vs_total_rt_scheduled_search_limit = sum_rt_scheduled_search.""/"".max_rt_scheduled_searches
| eval scheduled_vs_total_rt_search_limit = sum_rt_search.""/"".max_rt_searches
| fields splunk_server, search_count, scheduled_vs_total_auto_summary_search_limit, scheduled_vs_total_hist_scheduled_search_limit, scheduled_vs_total_hist_search_limit, scheduled_vs_total_rt_scheduled_search_limit, scheduled_vs_total_rt_search_limit, count_cpu, sum_mem_used
| rename splunk_server as Instance, search_count as ""Count of Searches"", scheduled_vs_total_auto_summary_search_limit as ""Summarization"", scheduled_vs_total_hist_scheduled_search_limit as ""Historical Schedule Report"", scheduled_vs_total_hist_search_limit  as ""Historical Search"", scheduled_vs_total_rt_scheduled_search_limit as ""Real-time Schedule Report"",  scheduled_vs_total_rt_search_limit  as ""Real-time Search"", count_cpu as ""CPU Usage (# cores)"", sum_mem_used as ""Memory Usage (MB)""
          ",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
                    | `dmc_set_bin`
                    | stats dc(data.search_props.sid) AS distinct_search_count by host, _time
                    | bin _time minspan=10s
                    | stats $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host, _time
                    | `dmc_search_count_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats dc(sid) AS distinct_search_count by host, _time
          | bin _time minspan=10s
          | stats $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host, _time
          | `dmc_search_count_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_search_activity_deployment_search_concurrency(""$role$"", ""$group$"", $searchConcurrencyAggrFunc$, $drilldown_search_concurrency_metric$)`",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats dc(sid) AS distinct_search_count by host, _time
          | `dmc_timechart` partial=f limit=25 $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,"host = round",,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=""$role$"" search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_set_bin`
          | stats dc(data.search_props.sid) AS distinct_search_count by host, _time
          | timechart minspan=10s bins=200 partial=f $aggrSearchConcurFunc$(distinct_search_count) AS distinct_search_count_per_host
          | eval distinct_search_count_per_host = round(distinct_search_count_per_host, 2)
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
                    | `dmc_set_bin`
                    | eval data.search_props.sid = if(component==""Hostwide"", ""n/a"", 'data.search_props.sid')
                    | eval data.pid = if(component==""Hostwide"", ""n/a"", 'data.pid')
                    | eval data.$resourceType$ = if(component==""Hostwide"", 0, 'data.$resourceType$')
                    | stats latest(data.$resourceType$) AS resource_usage_dedup by _time, data.search_props.sid, data.pid, host
                    | stats sum(resource_usage_dedup) AS sum_resource_usage by _time, host
                    | eval sum_resource_usage = if(""data.$resourceType$"" == ""data.pct_cpu"", round(sum_resource_usage / 100.0, 2), sum_resource_usage)
                    | bin _time minspan=10s
                    | stats $resourceAggrFunc$(sum_resource_usage) as resource_usage by _time, host
                    | `dmc_$resourceType$_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | eval sid = if(component==""Hostwide"", ""n/a"", sid)
          | eval pid = if(component==""Hostwide"", ""n/a"", pid)
          | eval $resourceType$ = if(component==""Hostwide"", 0, $resourceType$)
          | stats latest($resourceType$) AS resource_usage_dedup by _time, sid, pid, host
          | stats sum(resource_usage_dedup) AS sum_resource_usage by _time, host
          | eval sum_resource_usage = if(""$resourceType$"" == ""pct_cpu"", round(sum_resource_usage / 100.0, 2), sum_resource_usage)
          | `dmc_set_bin_for_timechart`
          | stats $resourceAggrFunc$(sum_resource_usage) as resource_usage by _time, host
          | `dmc_$resourceType$_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_search_activity_deployment_resource_usage(""$role$"", ""$group$"", $resourceType$, $resourceAggrFunc$, $drilldown_resource_usage_metric$)`",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats latest($resourceType$) AS resource_usage by _time, sid, pid, host
          | eval resource_usage = if(""$resourceType$"" == ""pct_cpu"", round(resource_usage / 100.0, 2), resource_usage)
          | `dmc_timechart_for_metrics_log` partial=f limit=25 $resourceAggrFunc$(resource_usage) AS resource_usage by host
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage data.search_props.sid::* data.search_props.mode!=RT
            | `dmc_rename_introspection_fields`
            | stats max(elapsed) as runtime max(mem_used) as mem_used earliest(_time) as _time by sid, label, provenance, type, mode, app, role, user, host
            | eval mem_used = round(mem_used, 2)
            | sort 20 - mem_used, runtime
            | fields sid, label, provenance, mem_used, host, runtime, _time, type, mode, app, user, role
            | eval _time=strftime(_time,""%+"")
            | rename sid as SID, label as Name, provenance as Provenance, mem_used as ""Memory Usage (MB)"", host as Instance, runtime as Runtime, _time as Started, type as Type, mode as Mode, app as App, user as User, role as Role
            | fieldformat Runtime = `dmc_convert_runtime(Runtime)`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
          | where process_type = ""search""
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | stats count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_total_hist,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_total_rt,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_hist_scheduled_search,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_rt_scheduled_search,
          count(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as count_auto_summary_search by splunk_server,      
          | eval count_total_hist = if(isnull(count_total_hist) OR count_total_hist=="""", 0, count_total_hist)
          | eval count_total_rt = if(isnull(count_total_rt) OR count_total_rt=="""", 0, count_total_rt)
          | eval count_hist_scheduled_search = if(isnull(count_hist_scheduled_search) OR count_hist_scheduled_search=="""", 0, count_hist_scheduled_search)
          | eval count_rt_scheduled_search = if(isnull(count_rt_scheduled_search) OR count_rt_scheduled_search=="""", 0, count_rt_scheduled_search)
          | eval count_auto_summary_search = if(isnull(count_auto_summary_search) OR count_auto_summary_search=="""", 0, count_auto_summary_search)
          | eval count_total_adhoc_scheduled_search = count_total_hist + count_total_rt
          | eval count_total_scheduled_search = count_hist_scheduled_search + count_rt_scheduled_search
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/limits/search-concurrency
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_historical_vs_limit = ""$count_total_hist$"".""/"".max_hist_searches
            | fields total_historical_vs_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_rt_vs_limit = ""$count_total_rt$"".""/"".max_rt_searches
            | fields total_rt_vs_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_scheduled_search_limit = ""$count_hist_scheduled_search$"".""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_scheduled_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_scheduled_search_limit = ""$count_rt_scheduled_search$"".""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_scheduled_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_auto_summary_search_limit = ""$count_auto_summary_search$"".""/"".max_auto_summary_searches
            | fields scheduled_vs_total_auto_summary_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| search urlField=""*$c-input$*""  OR datasources=""*$c-input$*"" OR eai.data=""*$c-input$*""
| fillnull value=""N/A"" datasources
| table urlField, datasources, myapp  author, eai:acl.sharing, count, dc_user","source=dashboard_views",,"search_for_dashboard"
tbd,,,,"inputlookup splunk_commands.csv",,"lame_training",,"| inputlookup splunk_commands.csv | fields command",,,"search_lame_youtube"
tbd,,,,"inputlookup lame_youtube_channel.csv",,"lame_training",,"| inputlookup lame_youtube_channel.csv 
|  search (title=""$command_name$"" 
AND about=""*$key_phrase$*"") 
OR (title=$command_name$ 
AND title=""*$key_phrase$*"")
|  table title, about, url",,,"search_lame_youtube"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group=""dmc_customgroup_*"" OR search_group=""dmc_indexerclustergroup_*"" OR search_group=""dmc_searchheadclustergroup_*""
        ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_audit_get_searches_for_groups(""$group$"")`
      | stats min(_time) as _time, values(user) as user, max(total_run_time) as total_run_time, first(search) as search, first(search_type) as search_type, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime by search_id, host
      | where isnotnull(search) $filter_out_non_adhoc$
    ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(user) as user_count",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(host) as search_head_count",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats dc(host) as count_host, median(total_run_time) as median_runtime, sum(total_run_time) as cum_runtime, count(search) as count, max(_time) as last_use by user
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval cum_runtime = if(isnotnull(cum_runtime), cum_runtime, ""-"")
            | `dmc_time_format(last_use)`
            | fields user, count, count_host, median_runtime, cum_runtime, last_use
            | sort - count
            | rename user as User, count_host as ""Search Head Count"", count as ""Search Count"", median_runtime as ""Median Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=$user_drilldown_hosts$
            | stats count(search) as count by host
            | sort - count
            | fields host, count
            | rename host as ""Search Head"", count as ""Search Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=$user_drilldown_searches$
            | stats count by search
            | sort - count
            | fields count, search
            | rename search as ""Report Name/Search String"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats median(total_run_time) as median_runtime sum(total_run_time) as cum_runtime count(search) as count_search max(_time) as last_use dc(user) as count_user by host
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval cum_runtime = if(isnotnull(cum_runtime), cum_runtime, ""-"")
            | `dmc_time_format(last_use)`
            | fields host, count_search, count_user, median_runtime, cum_runtime, last_use
            | sort - count_search
            | rename host as ""Search Head"", count_search as ""Search Count"", count_user as ""User Count"", median_runtime as ""Median Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search host=$host_drilldown_users|s$
            | stats count(search) as count by user
            | sort - count
            | fields user, count
            | rename user as ""User"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search host=$host_drilldown_searches|s$
            | stats count by search
            | sort - count        
            | fields count, search
            | rename search as ""Report Name/Search String"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=""*""
            | stats count(search) as count, median(total_run_time) as median_runtime, max(total_run_time) as max_runtime, values(user) as user, values(host) as host, values(search_type) as search_type by search
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval max_runtime = if(isnotnull(max_runtime), max_runtime, ""-"")
            | sort - count
            | rename search as ""Report Name/Search String"", count as ""Count"", median_runtime as ""Median Runtime"", max_runtime as ""Max Runtime"", user as Users, host as Hosts, search_type as Type
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=""*"" search=""*""
            | eval earliest = case(like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"", like(apiStartTime, ""%ZERO_TIME%""), ""-"", 1 == 1, apiStartTime )
            | eval latest = case(like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"", like(apiEndTime, ""%ZERO_TIME%""), ""-"", 1 == 1, apiEndTime ) 
            | `dmc_time_format(_time)`
            | stats max(total_run_time) as total_run_time by search, _time, earliest, latest, search_type, user, host, search_id
            | sort - total_run_time 
            | eval total_run_time = if(isnotnull(total_run_time), total_run_time, ""-"")
            | fields search, total_run_time, _time, earliest, latest, search_type, user, host, search_id
            | rename search as ""Report Name/Search String"", total_run_time as ""Search Runtime"", _time as ""Search Start"", earliest as ""Earliest Time"", latest as ""Latest Time"", search_type as Type, user as ""User"", host as ""Host"", search_id as SID
            | fieldformat ""Search Runtime"" = `dmc_convert_runtime('Search Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"where search_id=$long_running_sid|s$",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group!=""dmc_group_*""
        ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_audit_get_searches($host$)`
      | stats min(_time) as _time, values(user) as user, max(total_run_time) as total_run_time, first(search) as search, first(search_type) as search_type, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime by search_id
      | where isnotnull(search) $filter_out_non_adhoc$
    ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(user) as user_count",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats median(total_run_time) as median_runtime Perc90(total_run_time) as Perc90_runtime sum(total_run_time) as cum_runtime count(search) as count max(_time) as last_use by user
            | eval last_use = strftime(last_use, ""%F %T"")
            | fields user, count, median_runtime, Perc90_runtime, cum_runtime, last_use
            | rename user as User, count as ""Search Count"", median_runtime as ""Median Runtime"", Perc90_runtime as ""90th Percentile Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""90th Percentile Runtime"" = `dmc_convert_runtime('90th Percentile Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_audit_get_searches($host$)`
            | stats count by user
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            $common_search_user_filter$
            | stats count median(total_run_time) as median_runtime max(total_run_time) as max_runtime values(user) as user by search
            | eval median_runtime=if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval max_runtime=if(isnotnull(max_runtime), max_runtime, ""-"")
            | sort - count
            | rename search as ""Search"", count as ""Count"", median_runtime as ""Median Runtime"", max_runtime as ""Max Runtime"", user as User
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_audit_get_searches($host$)`
            | stats count by user
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
$long_running_search_user_filter$
| fields search, total_run_time, _time, apiStartTime, apiEndTime, search_type, user
| eval earliest = case(
    like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"",
    like(apiStartTime, ""%ZERO_TIME%""), ""-"",
    1 == 1, apiStartTime
)
| eval latest = case(
    like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"",
    like(apiEndTime, ""%ZERO_TIME%""), ""-"",
    1 == 1, apiEndTime
)
| eval search = if(isnotnull(search), search, ""N/A"")
| `dmc_time_format(_time)`
| sort - total_run_time
| fields search, total_run_time, _time, earliest, latest, search_type, user
| rename search as Search, total_run_time as ""Search Runtime"", _time as ""Search Start"", earliest as ""Earliest Time"", latest as ""Latest Time"", search_type as Type, user as ""User""
| fieldformat ""Search Runtime"" = `dmc_convert_runtime('Search Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
eval commands = commands(search)
| streamstats window=1 values(commands) as commands
| stats count avg(total_run_time) as avg_runtime max(total_run_time) as max_runtime by commands
| eval avg_runtime = round(avg_runtime, 2)
| eval max_runtime = round(max_runtime, 2)
| sort - count, - max_runtime, - avg_runtime
| rename commands as Command, avg_runtime as ""Average Runtime"", max_runtime as ""Max Runtime"", count as ""Count""
| eval ""Average Runtime"" = `dmc_convert_runtime('Average Runtime')`
| eval ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/local splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
| search install_source_checksum=*
| fields title, label, splunk_server, install_source_checksum
| rename title as app_name, install_source_checksum as checksum
  | append [ | rest /services/apps/deploy splunk_server_group=""$group$"" splunk_server_group=dmc_group_shc_deployer
  | fields title, splunk_server, checksum
| rename title as app_name]
| stats values(checksum) as checksum, values(label) as label by app_name
| eval status = if(mvcount(checksum) > 1, ""Out of Synchronization!"", ""Synchronized"")
| fields label status app_name
| rename label as App, status as ""Status""
| sort status
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/local splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
| search install_source_checksum=* AND title=""$appLocalDrilldown$""
| fields splunk_server, install_source_checksum
| rename splunk_server as Instance, install_source_checksum as Checksum
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/deploy splunk_server_group=""$group$"" splunk_server_group=dmc_group_shc_deployer
| search title=""$appLocalDrilldown$""
| fields splunk_server, checksum
| rename splunk_server as Instance, checksum as Checksum
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=dmc_group_shc_deployer sourcetype=splunkd_access uri_path=""/services/apps/deploy"" method=POST
| stats values(spent) as spent by _time, user, status
        ",,"sourcetype=splunkd_access","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by user",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
$deploymentRequestStatusScope$
| $deploymentRequestStatusScope$
| `dmc_timechart` $deploymentRequestMetric$ $deploymentRequestSplitBy$
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=dmc_group_shc_deployer sourcetype=splunkd_conf component=ConfDeployment data.task=sendDeployableApps
| stats count by _time, data.target_label, data.status
        ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by data.status",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by data.target_label",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
$sendDeployableAppsStatusFilter$
| $sendDeployableAppsTargetFilter$
| `dmc_timechart` count by $sendDeployableAppsSplitBy$
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | table data.apps{}.action, data.apps{}.name
            | eval apps = mvzip('data.apps{}.action', 'data.apps{}.name', "","")
            | fields - data.apps{}.action, data.apps{}.name
            | mvexpand apps
            | rex field=apps ""(?&lt;action&gt;\w+),(?&lt;app_name&gt;.+)""
            | fields action, app_name
            | where action!=""preserved""
            | stats count by app_name
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | stats count by host
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | table _time, data.apps{}.action, data.apps{}.name, host
            | eval apps = mvzip('data.apps{}.action', 'data.apps{}.name', "","")
            | mvexpand apps
            | rex field=apps ""(?&lt;action&gt;\w+),(?&lt;app_name&gt;.+)""
            | fields _time, host, action, app_name
            | where action!=""preserved""
            | $actionFilter$
            | $appNameFilter$
            | $hostFilter$
            | timechart count(action) as count_action by $appDeploymentSplitBy$
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", *Artifact*)`
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", *Artifact*)` punct=""$warningErrorPunct$""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/artifacts
          | fields title, status, splunk_server, label, user, eai:acl.app
          | rename splunk_server as member, label as search_name, eai:acl.app as app
        ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by member",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by app",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by search_name",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by user",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            $count_artifact_status_filter$
            | $count_artifact_member_filter$
            | $count_artifact_app_filter$
            | $count_artifact_search_name_filter$
            | $count_artifact_user_filter$
            | stats count by $count_artifact_split_by$
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            $count_artifact_status_filter$
            | $count_artifact_member_filter$
            | search $count_artifact_drilldown_name$ = ""$count_artifact_drilldown_value$""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=shccaptain_artifact search_group=""$group$""
| `dmc_timechart_for_metrics_log` median(to_fix_rep_factor) as artifacts
          ",,"sourcetype=splunkd","shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=executor name=poolmember_executor search_group=""$group$""
| eval backlog_change = jobs_added - jobs_finished
| `dmc_timechart_for_metrics_log` sum(jobs_added) AS ""jobs added"" sum(jobs_finished) AS ""jobs finished"" sum(backlog_change) AS ""backlog change""
          ",,"sourcetype=splunkd","shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd_conf search_group=""$group$"" component=ConfOp data.task=*addCommit
| stats count by _time, host, optype_desc, object_name, object_type, app, owner
    ",,"sourcetype=splunkd_conf","shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/members
          | join label type=outer [
            | rest /services/replication/configuration/health check_share_baseline=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
            | stats values(server_name) as baselines, count(server_name) as num_baselines by splunk_server, check_share_baseline
            | eval shared_common_baseline = if(check_share_baseline == ""Yes"", baselines, """")
            | eval no_shared_common_baseline = if(check_share_baseline == ""No"", baselines, """")
            | eval unable_to_connect = if(check_share_baseline == ""Connection error"", baselines, """")
            | eval num_shared_common_baseline = if(check_share_baseline == ""Yes"", num_baselines, 0)
            | eval num_no_shared_common_baseline = if(check_share_baseline == ""No"", num_baselines, 0)
            | eval num_unable_to_connect = if(check_share_baseline == ""Connection error"", num_baselines, 0)
            | stats sum(num_shared_common_baseline) as total_shared_common_baseline, sum(num_no_shared_common_baseline) as total_no_shared_common_baseline, sum(num_unable_to_connect) as total_unable_to_connect, values(shared_common_baseline) as shared_common_baseline, values(no_shared_common_baseline) as no_shared_common_baseline, values(unable_to_connect) as unable_to_connect by splunk_server
            | eval ratio = total_shared_common_baseline . ""/"" . (total_shared_common_baseline+total_no_shared_common_baseline+total_unable_to_connect)
            | rename splunk_server as label
          ]
          | where total_no_shared_common_baseline+total_unable_to_connect > 0
        ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", ConfReplication*)`
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", ConfReplication*)` punct=""$warningErrorPunct$""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by host
            | eval host_scope = ""where host == \"""".host.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by app
            | eval app_scope = ""where app == \"""".app.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by optype_desc
            | eval optype_desc_scope = ""where optype_desc == \"""".optype_desc.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by object_type
            | eval object_type_scope = ""where object_type == \"""".object_type.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by object_name
            | eval object_name_scope = ""where object_name == \"""".object_name.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by owner
            | eval owner_scope = ""where owner == \"""".owner.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            $confOpAppScope$
            | $confOpOperationTypeScope$
            | $confOpObjTypeScope$
            | $confOpObjNameScope$
            | $confOpOwnerScope$
            | $confOpHostScope$
            | timechart count by $confOpBy$ usenull=f
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            fields - count
| $confOpOperationTypeScope$
| $confOpObjTypeScope$
| $confOpObjNameScope$
| $confOpOwnerScope$
| $confOpHostScope$
| sort - _time
| rename _time as Time, host as Instance, optype_desc as ""Operation Type"", object_name as ""Object Name"", object_type as ""Object Type"", app as App, owner as Owner
| `dmc_time_format(Time)`
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd search_group=""$group$"" group=conf
| `dmc_set_bin_for_metrics_log`
| stats sum(count) as count, sum(wallclock_ms_total) as sum_total_time, max(wallclock_ms_max) as max_max_time by _time, action, host
        ",,"sourcetype=splunkd","shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_timechart_for_metrics_log` sum(count) as count latest(eval(sum_total_time / count)) as avg_time_per_action latest(max_max_time) as max_time_per_action
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by action",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
$confRepPerfCountHostScope$
| $confRepPerfCountActionScope$
| `dmc_timechart_for_metrics_log` sum(count) by $confRepPerfCountSplitBy$
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by action",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
$confRepPerfTimeHostScope$
| $confRepPerfTimeActionScope$
| `dmc_timechart_for_metrics_log` $confRepPerfMetric$ by $confRepPerfTimeSplitBy$
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=searchscheduler search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(dispatched) as dispatched, sum(skipped) as skipped, sum(delegated) as delegated $funcStatus$(delegated_waiting) as delegated_waiting, sum(delegated_scheduled) as delegated_scheduled, $funcStatus$(max_pending) as max_pending, $funcStatus$(max_running) as max_running
    ",,"sourcetype=splunkd","shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
          | dedup label
        ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$captain_server_name$ /services/shcluster/captain/jobs
| eval peer = if(isnotnull(peer_servername), peer_servername, peer)
| fields dispatch_time, job_state, peer, saved_search, savedsearchtype, search_app, search_owner, sid, splunk_server, success, title
| eval sid = if(isnotnull(sid), sid, ""N/A"")
        ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by $jobsSplitBy$",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by job_state",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by saved_search",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
$jobStateFilter$
| $jobSidFilter$
| $jobSavedSearchNameFilter$
| `dmc_time_format(dispatch_time)`
| sort - dispatch_time
| fields dispatch_time, job_state, success, splunk_server, peer, saved_search, savedsearchtype, search_app, search_owner, sid
| eval success = if (success == 1 or success == ""1"", ""Succeeded"", ""Failed"")
| rename dispatch_time as ""Dispatch Time"", job_state as ""Job State"", success as Success, splunk_server as ""Delegating Instance (Captain)"" peer as ""Delegated Instance"", saved_search as ""Scheduled Search"", savedsearchtype as ""Saved Search Type"", search_app as App, search_owner as Owner, sid as SID
          ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"fields - delegated_waiting delegated_scheduled max_pending max_running",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"fields - dispatched skipped delegated",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
      | dedup peer_scheme_host_port
      | fields label
    ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=local /services/search/distributed/peers
| where search_groups=""$group$"" AND server_roles=""search_head""
| eval label = host
| join guid type=outer [
  | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/members count=0
  | dedup label
  | eval guid = title
]
| join label type=outer [
  | rest splunk_server=$captain_name|s$ /services/shcluster/captain/members count=0
  | where splunk_server == label
  | fields label, last_heartbeat
  | rename last_heartbeat as last_heartbeat_captain
]
| eventstats values(last_heartbeat_captain) as last_heartbeat_captain
| join label type=outer [
  | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
  | where splunk_server == label
  | eval age = now() - elected_captain
  | eval captain_age = case(age &lt; 60, ""&lt; 1m"", age &gt;= 60 AND age &lt; 3600, round(age / 60, 0).""m"", age &gt;= 3600 AND age &lt; 86400, round(age / 3600, 0).""h"", age &gt;= 86400, round(age / 86400, 0).""d"")
  | `dmc_time_format(elected_captain)`
  | eval role = ""Captain ("" . captain_age . "")""
  | fields label captain_age elected_captain role
]
| join label type=outer [
  | rest /services/replication/configuration/health check_share_baseline=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
  | stats values(server_name) as baselines, count(server_name) as num_baselines by splunk_server, check_share_baseline
  | eval shared_common_baseline = if(check_share_baseline == ""Yes"", baselines, """")
  | eval no_shared_common_baseline = if(check_share_baseline == ""No"", baselines, """")
  | eval unable_to_connect = if(check_share_baseline == ""Connection error"", baselines, """")
  | eval num_shared_common_baseline = if(check_share_baseline == ""Yes"", num_baselines, 0)
  | eval num_no_shared_common_baseline = if(check_share_baseline == ""No"", num_baselines, 0)
  | eval num_unable_to_connect = if(check_share_baseline == ""Connection error"", num_baselines, 0)
  | stats sum(num_shared_common_baseline) as total_shared_common_baseline, sum(num_no_shared_common_baseline) as total_no_shared_common_baseline, sum(num_unable_to_connect) as total_unable_to_connect, values(shared_common_baseline) as shared_common_baseline, values(no_shared_common_baseline) as no_shared_common_baseline, values(unable_to_connect) as unable_to_connect by splunk_server
  | eval ratio = total_shared_common_baseline . ""/"" . (total_shared_common_baseline+total_no_shared_common_baseline+total_unable_to_connect)
  | rename splunk_server as label
]
| join label type=outer [
  | rest /services/replication/configuration/health unpublished=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
  | rename ""Number of unpublished changes"" as unpublished_changes
  | eval unpublished_changes=if(unpublished_changes==""0 (this instance is the captain)"", 0, unpublished_changes)
  | rename splunk_server as label
]
| eval role = if(isnotnull(role), role, ""Member"")
| sort role
    ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          fields label, status, last_heartbeat, last_heartbeat_captain, unable_to_connect
          | join label type=outer [
            | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/configs/conf-server/shclustering
            | fields splunk_server, heartbeat_timeout
            | rename splunk_server as label
          ]
          | where (status != ""Up"" OR (last_heartbeat_captain - last_heartbeat) > heartbeat_timeout) OR unable_to_connect != """"
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/search/distributed/peers
          | fields splunk_server, peerName
          | stats values(peerName) as peers by splunk_server
          | nomv peers
          | stats values(splunk_server) AS search_heads by peers
          | makemv peers
          | fields search_heads peers
          | rename search_heads as ""Search Head Cluster Member"", peers as ""Search Peer List""
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          where total_no_shared_common_baseline+total_unable_to_connect > 0
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            fields label, last_heartbeat, last_heartbeat_captain, heartbeat_timeout, status, unable_to_connect
            | eval heartbeat_timeout = if(isnotnull(heartbeat_timeout), heartbeat_timeout, ""N/A"")
            | `dmc_time_format(last_heartbeat)`
            | `dmc_time_format(last_heartbeat_captain)`
            | eval last_heartbeat = if(isnotnull(last_heartbeat), last_heartbeat, ""N/A"")
            | rename label as Member, last_heartbeat as ""Last Heartbeat Sent to Captain"", last_heartbeat_captain as ""Last Heartbeat Received by Captain"", heartbeat_timeout as ""Heartbeat Timeout (sec)"", status as ""Status"", unable_to_connect as ""Member Unreachable""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/server/status/resource-usage/splunk-processes
          | search search_props.role=""head""
          | dedup search_props.sid
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_total_hist,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_total_rt,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_hist_scheduled_search,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_rt_scheduled_search,
            count(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as count_auto_summary_search
            | eval count_total_adhoc_scheduled_search = count_total_hist + count_total_rt
            | eval count_total_scheduled_search = count_hist_scheduled_search + count_rt_scheduled_search
            | eval dummy_key = ""dummy_key""
            | fields count_total_hist, count_hist_scheduled_search,
                     count_total_rt, count_rt_scheduled_search,
                     count_auto_summary_search,
                     count_total_adhoc_scheduled_search, count_total_scheduled_search,
                     dummy_key
            | join dummy_key type=outer [
              | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head ""/services/server/status/limits/search-concurrency?cluster_wide_quota=1""
              | stats max(max_hist_searches) as max_hist_searches, max(max_hist_scheduled_searches) as max_hist_scheduled_searches, max(max_rt_searches), as max_rt_searches, max(max_rt_scheduled_searches) as max_rt_scheduled_searches, max(max_auto_summary_searches) as max_auto_summary_searches
              | eval dummy_key = ""dummy_key""
              | fields max_hist_searches, max_hist_scheduled_searches,
                  max_rt_searches, max_rt_scheduled_searches,
                  max_auto_summary_searches,
                  dummy_key
            ]
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_historical_vs_limit = count_total_hist.""/"".max_hist_searches
            | fields total_historical_vs_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_rt_vs_limit = count_total_rt.""/"".max_rt_searches
            | fields total_rt_vs_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_scheduled_search_limit = count_hist_scheduled_search.""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_scheduled_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_scheduled_search_limit = count_rt_scheduled_search.""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_scheduled_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_auto_summary_search_limit = count_auto_summary_search.""/"".max_auto_summary_searches
            | fields scheduled_vs_total_auto_summary_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
fields label role status last_heartbeat ratio unpublished_changes artifact_count
| eval ratio = if(isnotnull(ratio), ratio, ""N/A"")
| eval unpublished_changes = if(isnotnull(unpublished_changes), unpublished_changes, ""N/A"")
| eval artifact_count = if(isnotnull(artifact_count), artifact_count, ""N/A"")
| `dmc_time_format(last_heartbeat)`
| `dmc_time_format(last_heartbeat_captain)`
| eval last_heartbeat = if(isnotnull(last_heartbeat), last_heartbeat, ""N/A"")
| rename label as Instance, role as Role, status as Status, last_heartbeat as ""Last Heartbeat Sent to Captain"", ratio as ""Configuration Baseline Consistency"", unpublished_changes as ""Number of Unpublished Changes"", artifact_count as ""Artifact Count""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
where label == ""$SHCMemberDrilldown$""
| eval elected_captain = if(isnotnull(elected_captain), elected_captain, ""N/A"")
| fields - _timediff
| fields elected_captain guid advertise_restart_required advertise_restart_required_reason delayed_artifacts_to_discard fixup_set pending_job_count replication_count status_counter.Complete status_counter.PendingDiscard peer_scheme_host_port adhoc_searchhead kv_store_host_port replication_port replication_use_ssl site
| transpose
| rename column as ""Configuration and Status"", ""row 1"" as Value
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where label == ""$SHCMemberBaselineDrilldown$""
            | fields shared_common_baseline no_shared_common_baseline unable_to_connect
            | rename shared_common_baseline as ""Shares Common Baseline With"", no_shared_common_baseline as ""Does Not Share Common Baseline With"", unable_to_connect as ""No Response From""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
        | `dmc_rename_introspection_fields`
        | `dmc_set_bin`
        | stats dc(sid) AS distinct_search_count by provenance, mode, app, type, user, host, _time
        ",,"sourcetype=splunk_resource_usage","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          $shcSearchConHostScope$
          | stats sum(distinct_search_count) as total_distinct_search_count by provenance, mode, app, type, user, host, _time
          | `dmc_timechart` partial=false $shcSearchConFunc$(total_distinct_search_count) as search_count by $shcSearchConSplitBy$
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability upgrades_to_captain=1
| stats count by _time, upgrades_to_captain, host
        ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
$captainElectionHostScope$
| `dmc_timechart_for_metrics_log` count(upgrades_to_captain) as captain_election_event
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
$captainElectionHostScope$
| eval event = host."" was elected as captain.""
| fields _time, event
| sort - _time
| rename _time as Time, event as Event
| `dmc_time_format(Time)`
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability (upgrades_to_captain=1 OR downgrades_from_captain=1 OR num_pollled_candidates&gt;0)
| `dmc_timechart_for_metrics_log` count(num_polled_captain) as candidate sum(upgrades_to_captain) as upgrades_to_captain sum(downgrades_from_captain) as downgrades_from_captain
          ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability
| `dmc_set_bin_for_metrics_log`
| eval status=case(
(upgrades_to_captain=0) AND (downgrades_from_captain=0) AND (num_polled_candidate=0), ""stable"",
1=1, ""perturbing"")
| eval reason=case(
upgrades_to_captain=1 AND num_polled_candidate&gt;0, host."" became candidate and was elected as captain"",
upgrades_to_captain=0 AND num_polled_candidate&gt;0, host."" became candidate"",
upgrades_to_captain=1 AND num_polled_candidate=0, host."" was elected as captain"",
downgrades_from_captain=1, host."" changed from captain to a member"",
1=1, """")
| stats count by _time, status, reason
| sort - _time
| fields - count
| $captainHistoryFilter$
          ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top sourcetype",,,"show_time_ranges"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | chart count by sourcetype",,,"simple_auto_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count by sourcetype",,,"simple_auto_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"simple_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"simple_chart"
tbd,,,"index = _internal",,,"simple_xml_examples",,"
                        index = _internal sourcetype=splunkd_access
                        | stats count sum(bytes) as ""Total Bytes"" by status, date_hour
                        | table status date_hour count ""Total Bytes""
                    ",,"sourcetype=splunkd_access","simple_chart_bubble"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top sourcetype",,,"simple_chart_color_options"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top sourcetype",,,"simple_chart_color_options"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"simple_chart_color_options"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype useother=f",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by status usenull=f | fields - skipped, success",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count dc(source)",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by status usenull=f | fields - skipped, success",,,"simple_chart_enhancements"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunk_web_access | stats count",,"sourcetype=splunk_web_access","simple_chart_gauges"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunk_web_access | stats count",,"sourcetype=splunk_web_access","simple_chart_gauges"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunk_web_access | stats count",,"sourcetype=splunk_web_access","simple_chart_gauges"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                        index=_internal
                        | timechart count
                        | eventstats avg(count) as average
                        | eval average=round(average,0)
                    ",,,"simple_chart_overlay"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count as total count(eval(sourcetype=""splunkd"")) as ""splunkd""
                    ",,"sourcetype=splunkd","simple_chart_overlay"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_range_selection"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_range_selection"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_range_selection"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_chart_range_selection"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top sourcetype",,,"simple_chart_range_selection"
tbd,,,,"inputlookup geo_attr_countries",,"simple_xml_examples",,"
                        | inputlookup geo_attr_countries
                        | fields country, region_un
                        | geom geo_countries featureIdField=country
                    ",,,"simple_choropleth"
tbd,,,,"inputlookup states_pop_density.csv",,"simple_xml_examples",,"
                        | inputlookup states_pop_density.csv
                        | fields state, region
                        | geom geo_us_states featureIdField=state
                    ",,,"simple_choropleth"
tbd,,,,"inputlookup geo_us_states",,"simple_xml_examples",,"| inputlookup geo_us_states",,,"simple_choropleth"
tbd,,,,"inputlookup craigslist.csv","lookup geo_sf_neighborhoods","simple_xml_examples",,"
                        | inputlookup craigslist.csv
                        | lookup geo_sf_neighborhoods latitude AS location.lat, longitude AS location.long OUTPUT featureId AS neighborhood
                        | stats median(price) by neighborhood
                        | geom geo_sf_neighborhoods featureIdField=neighborhood
                    ",,,"simple_choropleth"
tbd,,,,"inputlookup craigslist.csv","lookup geo_sf_neighborhoods","simple_xml_examples",,"
                        | inputlookup craigslist.csv
                        | lookup geo_sf_neighborhoods latitude AS location.lat, longitude AS location.long OUTPUT featureId AS neighborhood
                        | stats median(price) by neighborhood
                        | geom geo_sf_neighborhoods featureIdField=neighborhood
                    ",,,"simple_choropleth_color_mode"
tbd,,,,"inputlookup craigslist.csv","lookup geo_sf_neighborhoods","simple_xml_examples",,"
                        | inputlookup craigslist.csv
                        | lookup geo_sf_neighborhoods latitude AS location.lat, longitude AS location.long OUTPUT featureId AS neighborhood
                        | stats median(price) by neighborhood
                        | geom geo_sf_neighborhoods featureIdField=neighborhood
                    ",,,"simple_choropleth_color_mode"
tbd,,,,"inputlookup craigslist.csv","lookup geo_sf_neighborhoods","simple_xml_examples",,"
                        | inputlookup craigslist.csv
                        | lookup geo_sf_neighborhoods latitude AS location.lat, longitude AS location.long OUTPUT featureId AS neighborhood
                        | stats median(price) as price by neighborhood
                        | rangemap field=price ""1. low""=0-2000 ""2. medium""=2001-3500 ""3. high""=3501-5000 ""4. very high""=5001-20000
                        | fields neighborhood, range
                        | sort range
                        | geom geo_sf_neighborhoods featureIdField=neighborhood
                    ",,,"simple_choropleth_color_mode"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top sourcetype",,,"simple_display_controls_example"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 100 | stats count by sourcetype",,,"simple_drilldown_no_action"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 100 | stats count by sourcetype",,,"simple_drilldown_to_custom_url"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 100 | stats count by sourcetype",,,"simple_drilldown_to_custom_url"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 100 | chart count by sourcetype",,,"simple_drilldown_to_dashboard"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 100 | timechart count by sourcetype",,,"simple_drilldown_to_report"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_drilldown_to_search"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-h | stats count",,,"simple_drilldown_to_search"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | stats count by sourcetype",,,"simple_drilldown_to_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=$sourcetype$ | head 1000 | timechart count",,,"simple_drilldown_to_tokens"
tbd,,,"index=_audit",,,"simple_xml_examples",,"index=_audit action=""login attempt"" user=$env:user|s$ | table _time user action info",,,"simple_environment_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal |  top sourcetype",,,"simple_eval_tokens"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd group=per_sourcetype_thruput",,"sourcetype=splunkd","simple_event"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                    index=_internal sourcetype=splunkd group=per_sourcetype_thruput
                    | fields + series, ev, eps, kb, kbps
                ",,"sourcetype=splunkd","simple_event"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                  index=_internal (log_level=""WARN"" OR log_level=""ERROR"" OR log_level=""INFO"") | eval annotation_label = message | eval annotation_category = log_level | table _time annotation_label annotation_category
              ",,,"simple_event_annotations"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_event_annotations"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                    index=_internal status=404 | eval annotation_color = ""#0099cc""
                ",,,"simple_event_annotations"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_event_annotations"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by user",,,"simple_form_advanced"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$ | head 1000",,,"simple_form_advanced"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by user",,,"simple_form_cascading"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$| stats count by sourcetype",,,"simple_form_cascading"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                    index=_internal user=$username$ sourcetype=$source$
                    | head 1000
                    | table _time, user, sourcetype, _raw
                ",,,"simple_form_cascading"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"simple_form_checkbox"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal $sourcetype_token$ | stats count by sourcetype",,,"simple_form_checkbox"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"simple_form_dropdown"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$ $source$ | timechart count",,,"simple_form_dropdown"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$ $source$ | table _time, user, sourcetype, _raw",,,"simple_form_dropdown"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=""$sourcetype1$"" | timechart count",,,"simple_form_inputs_in_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=""$sourcetype2$"" | head 3",,,"simple_form_inputs_in_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"simple_form_multiselect"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal $sourcetype_token$ | stats count by sourcetype",,,"simple_form_multiselect"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count by sourcetype",,,"simple_form_radio"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$ $source$ | timechart count",,,"simple_form_radio"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal user=$username$ $source$ | timechart count",,,"simple_form_radio"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=$limit$ sourcetype | eval percent = round(percent,2)",,,"simple_form_text"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_form_time"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_form_time"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"simple_form_timerange_in_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 3",,,"simple_form_timerange_in_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal $s_level$ | table _time log_level _raw",,,"simple_input_change"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events by sourcetype",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events by sourcetype",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events by sourcetype",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events by sourcetype",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events by sourcetype",,,"simple_layout_grouping_chart"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-24h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-12h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-24h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-24h | stats count",,,"simple_layout_grouping_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-30m | stats count",,,"simple_layout_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-h | stats count",,,"simple_layout_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal earliest=-24h | stats count",,,"simple_layout_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events",,,"simple_layout_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count as events",,,"simple_layout_panels"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_layout_panels"
tbd,,,,"inputlookup geomaps_data.csv",,"simple_xml_examples",,"
                    | inputlookup geomaps_data.csv
                    | iplocation device_ip
                    | geostats latfield=lat longfield=lon count by method
                ",,,"simple_maps"
tbd,,,,"inputlookup geomaps_data.csv","lookup geo_countries","simple_xml_examples",,"
                    | inputlookup geomaps_data.csv
                    | iplocation device_ip
                    | lookup geo_countries latitude AS lat longitude AS lon OUTPUT featureId AS country
                    | stats count by country
                    | geom geo_countries featureIdField=country
                ",,,"simple_maps"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=3 sourcetype",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=3 sourcetype",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=3 sourcetype",,,"simple_panel_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 10000 | timechart count as events| predict events",,,"simple_predict_charts"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 10000 | timechart count as events| predict events algorithm=LLP",,,"simple_predict_charts"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 10000 | timechart count as events| predict events future_timespan=20
                ",,,"simple_predict_charts"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 10000 | timechart count as events| predict events upper85=high
                    lower99=low
                ",,,"simple_predict_charts"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | timechart count",,"sourcetype=splunkd","simple_search_data_sampling"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | timechart count",,"sourcetype=splunkd","simple_search_data_sampling"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | timechart count",,"sourcetype=splunkd","simple_search_data_sampling"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | timechart count",,"sourcetype=splunkd","simple_search_data_sampling"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_search_inline"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count",,,"simple_search_inline"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000",,,"simple_search_inline"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd component!=""Metrics"" | chart count over component by log_level | addtotals",,"sourcetype=splunkd","simple_search_multi"
tbd,,,,,,"simple_xml_examples",,"stats sum(INFO)",,,"simple_search_multi"
tbd,,,,,,"simple_xml_examples",,"stats sum(WARN)",,,"simple_search_multi"
tbd,,,,,,"simple_xml_examples",,"stats sum(ERROR)",,,"simple_search_multi"
tbd,,,,,,"simple_xml_examples",,"| sort -Total limit=10 | fields - Total",,,"simple_search_multi"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000",,,"simple_search_postprocess"
tbd,,,,,,"simple_xml_examples",,"timechart count",,,"simple_search_postprocess"
tbd,,,,,,"simple_xml_examples",,"top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_search_postprocess"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_search_realtime"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | head 1000 | timechart count",,,"simple_search_realtime"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal component=* | fields _time sourcetype component log_level",,,"simple_search_recursive_post_process"
tbd,,,,,,"simple_xml_examples",,"stats count by sourcetype | sort -count | eval label=sourcetype . "" ("" . count . "")""",,,"simple_search_recursive_post_process"
tbd,,,,,,"simple_xml_examples",,"search sourcetype=""$sourcetypeSelection$"" | timechart count by log_level",,,"simple_search_recursive_post_process"
tbd,,,,,,"simple_xml_examples",,"fillnull value=0 ERROR | stats sum(ERROR) as errorCount",,,"simple_search_recursive_post_process"
tbd,,,,,,"simple_xml_examples",,"fillnull value=0 WARN | stats sum(WARN) as warnCount",,,"simple_search_recursive_post_process"
tbd,,,,,,"simple_xml_examples",,"fillnull value=0 INFO | stats sum(INFO) as infoCount",,,"simple_search_recursive_post_process"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
                        index=_internal component=* sourcetype=""$sourcetypeSelection$""
                        | stats count(eval(log_level=""ERROR"")) AS ERROR, count(eval(log_level=""WARN"")) AS WARN, count(eval(log_level=""INFO"")) AS INFO by sourcetype
                        | addcoltotals labelField=sourcetype label=""TOTAL""
                    ",,,"simple_search_recursive_post_process"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_search_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_search_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | stats count",,,"simple_search_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"simple_search_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count by sourcetype",,,"simple_search_refresh"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal |  top sourcetype",,,"simple_search_result_setter"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_single"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | timechart count",,,"simple_single"
tbd,,,,"inputlookup sf-temperatures.csv",,"simple_xml_examples",,"| inputlookup sf-temperatures.csv | eval _time=strptime(DATE, ""%Y%m%d"") | timechart avg(TMAX) AS tmax span=7d | eval tmax = tmax/10/5*9+32",,,"simple_single_color"
tbd,,,,"inputlookup splunk-stock.csv",,"simple_xml_examples",,"| inputlookup splunk-stock.csv | eval _time=strptime(date, ""%Y-%m-%d"") | timechart last(close) span=7d | fillnull value=NULL",,,"simple_single_color"
tbd,,,,"inputlookup splunk-stock.csv",,"simple_xml_examples",,"| inputlookup splunk-stock.csv | eval _time=strptime(date, ""%Y-%m-%d"") | timechart last(close) span=7d | fillnull value=NULL",,,"simple_single_color"
tbd,,,,"inputlookup sf-temperatures.csv",,"simple_xml_examples",,"| inputlookup sf-temperatures.csv | eval _time=strptime(DATE, ""%Y%m%d"") | timechart avg(TMAX) AS tmax span=7d | eval tmax = tmax/10/5*9+32",,,"simple_single_color"
tbd,,,,"inputlookup splunk-stock.csv",,"simple_xml_examples",,"| inputlookup splunk-stock.csv | eval _time=strptime(date, ""%Y-%m-%d"") | timechart last(close) span=7d | fillnull value=NULL",,,"simple_single_color"
tbd,,,,"inputlookup splunk-stock.csv",,"simple_xml_examples",,"| inputlookup splunk-stock.csv | eval _time=strptime(date, ""%Y-%m-%d"") | timechart last(close) span=7d | fillnull value=NULL",,,"simple_single_color"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_table"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_table_data_overlay"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | top limit=100 sourcetype | eval percent = round(percent,2)",,,"simple_table_data_overlay"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
            index=_internal | stats count by sourcetype
          ",,,"simple_table_formats"
tbd,,,"index=_internal",,,"simple_xml_examples",,"
            index=_internal | stats count by sourcetype
          ",,,"simple_table_formats"
tbd,,,,"inputlookup splunk-stock.csv",,"simple_xml_examples",,"
            | inputlookup splunk-stock.csv
            | fields date, volume
          ",,,"simple_table_formats"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal sourcetype=splunkd | head 10 | table _time sourcetype message date_mday date_month date_year date_wday",,"sourcetype=splunkd","simple_table_hidden_fields"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count sparkline by sourcetype | sort -count",,,"simple_table_sparklines"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count sparkline by sourcetype | sort -count",,,"simple_table_sparklines"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count sparkline by sourcetype | sort -count",,,"simple_table_sparklines"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count sparkline(count, 1h) as trend by sourcetype | sort -count",,,"simple_table_sparklines"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count sparkline(count, 1h) as trend by sourcetype | sort -count",,,"simple_table_sparklines"
tbd,,,,"inputlookup creditcard_transactions.csv",,"simple_xml_examples",,"| inputlookup creditcard_transactions.csv 
| chart avg(amount) count by response_code, card_provider",,,"simple_trellis"
tbd,,,,"inputlookup creditcard_transactions.csv",,"simple_xml_examples",,"| inputlookup creditcard_transactions.csv 
| stats avg(amount) by card_provider state
| geom geo_us_states featureIdField=state",,,"simple_trellis"
tbd,,,"index=_internal",,,"simple_xml_examples",,"index=_internal | chart count over useragent by method",,,"simple_viz_chart_bar"
tbd,,,"index = _internal",,,"simple_xml_examples",,"index = _internal | stats count, mode(status) by method, status, date_hour",,,"simple_viz_chart_scatter"
tbd,,,,,,"simple_xml_examples",,"where (status) &gt;= 200 and (status) &lt; 300",,,"simple_viz_chart_scatter"
tbd,,,,,,"simple_xml_examples",,"where (status) &gt;= 300 and (status) &lt; 400",,,"simple_viz_chart_scatter"
tbd,,,,,,"simple_xml_examples",,"where (status) &gt;= 400 and (status) &lt; 500",,,"simple_viz_chart_scatter"
tbd,,,,,,"simple_xml_examples",,"where (status) &gt;= 500 and (status) &lt; 600",,,"simple_viz_chart_scatter"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log CacheManager action=* earliest=-1h 
                | stats count(eval(status=""failed"")) as txn_failed, count(eval(status=""succeeded"")) as txn_succeeded 
                | eval available=case(txn_failed=0 AND txn_succeeded=0, ""IDLE"", txn_succeeded>0, ""ONLINE"", true(), ""OFFLINE"") 
                | fields - txn_failed - txn_succeeded","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=$search_group$ /services/admin/cacheman
            | eval stable_size = if('cm:bucket.stable'=1, 'cm:bucket.estimated_size', 0)
            | stats sum(stable_size) as stable_bytes, sum(cm:bucket.estimated_size) as all_bytes
            | eval migration_progress=round((stable_bytes / all_bytes) * 100)
            | fields - stable_bytes - all_bytes
          ",,,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$search_group$ group=cmmaster source=*metrics.log subgroup=buckets_re_creation
            | stats sum(success_count) as successes, sum(in_progress_count) as in_progress
            | eval total = successes + in_progress
            | eval percent = if(total=0, 100, (1 - ((in_progress - successes) / total)) * 100)
            | fields percent
          ","source=*metrics.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log ((action=download AND download_set=*journal*) OR action=upload) status=succeeded component=CacheManager
            | eval mb = kb / 1024
            | timechart $funcAggregate$(mb) by action
          ","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log (action=download OR action=upload) status=failed component=CacheManager
              | rex field=reason ""HTTP Error (?<statuscode>\d*): (?<description>.*)""
              | eval reason_str = statuscode."": "".description
              | fillnull reason_str value=""Non-HTTP""
              | timechart count by $aggregation$
            ]]>
          ","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log CacheManager AND action=* AND earliest=-1h 
                | stats count(eval(status=""failed"")) as txn_failed, count(eval(status=""succeeded"")) as txn_succeeded 
                | eval available=case(txn_failed=0 AND txn_succeeded=0, ""IDLE"", txn_succeeded>0, ""ONLINE"", true(), ""OFFLINE"") 
                | fields - txn_failed - txn_succeeded","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/admin/cacheman
            | eval stable_size = if('cm:bucket.stable'=1, 'cm:bucket.estimated_size', 0)
            | stats sum(stable_size) as stable_bytes, sum(cm:bucket.estimated_size) as all_bytes
            | eval migration_progress=round((stable_bytes / all_bytes) * 100)
            | fields - stable_bytes - all_bytes
          ",,,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$splunk_server$ group=cmmaster source=*metrics.log subgroup=buckets_re_creation
            | stats sum(success_count) as successes, sum(in_progress_count) as in_progress
            | eval total = successes + in_progress
            | eval percent = if(total=0, 100, (1 - ((in_progress - successes) / total)) * 100)
            | fields percent
          ","source=*metrics.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log ((action=download AND download_set=*journal*) OR action=upload) status=succeeded component=CacheManager
            | eval mb = kb / 1024
            | timechart $funcAggregate$(mb) by action
          ","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log (action=download OR action=upload) status=failed component=CacheManager
              | rex field=reason ""HTTP Error (?<statuscode>\d*): (?<description>.*)""
              | eval reason_str = statuscode."": "".description
              | fillnull reason_str value=""Non-HTTP""
              | timechart count by $aggregation$
            ]]>
          ","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/diskUsage/minFreeSpace | eval label = IF(match(value,""\%""), value, tostring(value, ""commas"").""MB"") | fields label",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/eviction_padding | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/max_cache_size | eval size=if(value=0, ""No Max"", tostring(value, ""commas"").""MB"") | fields size",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/hotlist_recency_secs | appendpipe [stats count | eval value=86500 | where count=0] | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/hotlist_bloom_filter_recency_hours | appendpipe [stats count | eval value=360 | where count=0] | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ source=*metrics.log | timechart sum(evicted) as Evicted","source=*metrics.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
             `dmc_set_index_internal` search_group=$search_group$  source=*/splunkd_access.log
              | rex field=uri ""/services/admin/cacheman/bid|(?<bid>[^|]*)|/close""
              | search uri=*/close*
              | eval mytime=strftime(_time, ""%Y-%m-%d %H:%M:%S"")
              | stats last(_time) as _time count as buckets, sum(miss_ms) as miss_ms sum(search_ms) as search_ms, min(mytime) as issuetime by sid
              | fillnull value=0 search_ms
              | eval overheadRatio=(miss_ms/search_ms)
              | fillnull value=0 overheadRatio
              | eval searchSpeed=case(overheadRatio > 2,""More than 50%"", overheadRatio < .2, ""Less than 10%"", true(), ""10%-50%"")
              | timechart count by searchSpeed
            ]]>
          ","source=*",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ group=cachemgr_bucket source=*metrics.log | timechart count(cache_hit) as Hits count(cache_miss) as Misses","source=*metrics.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log CacheManager AND TERM(action=download) AND TERM(status=succeeded) AND download_set=""*journal*""
              | rex field=cache_id "".*\|(?<customer_index>.*)~.*~.*\|""
              | eval identifier=(cache_id + host)
              | stats count by identifier, customer_index
              | stats count(eval(count>1)) as duplicate_downloads, sum(count) as all_downloads count(eval(count>=10)) as excessive_duplicate_downloads by customer_index
              | eval duplicate_percent=if(all_downloads=0, 0, round((duplicate_downloads/all_downloads)*100, 2))
              | sort  - duplicate_percent
              | fields customer_index, duplicate_percent  all_downloads duplicate_downloads excessive_duplicate_downloads 
              | rename customer_index as Index, duplicate_percent as ""Repeat Download Percent"",  all_downloads as ""All Downloads"", duplicate_downloads as ""Repeated Downloads"", excessive_duplicate_downloads as ""Excessively Repeated Downloads""
            ]]>
          ","source=*splunkd.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/diskUsage/minFreeSpace | eval label = IF(match(value,""\%""), value, tostring(value, ""commas"").""MB"") | fields label",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/eviction_padding | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/max_cache_size | eval size=if(value=0, ""No Max"", tostring(value, ""commas"").""MB"") | fields size",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/hotlist_recency_secs | appendpipe [stats count | eval value=86500 | where count=0] | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/hotlist_bloom_filter_recency_hours | appendpipe [stats count | eval value=360 | where count=0] | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*metrics.log | timechart sum(evicted) as Evicted","source=*metrics.log",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$  source=*/splunkd_access.log
              | rex field=uri ""/services/admin/cacheman/bid|(?<bid>[^|]*)|/close""
              | search uri=*/close*
              | eval mytime=strftime(_time, ""%Y-%m-%d %H:%M:%S"")
              | stats last(_time) as _time count as buckets, sum(miss_ms) as miss_ms sum(search_ms) as search_ms, min(mytime) as issuetime by sid
              | fillnull value=0 search_ms
              | eval overheadRatio=(miss_ms/search_ms)
              | fillnull value=0 overheadRatio
              | eval searchSpeed=case(overheadRatio > 2,""More than 50%"", overheadRatio < .2, ""Less than 10%"", true(), ""10%-50%"")
              | timechart count by searchSpeed
            ]]>
          ","source=*",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*metrics.log group=cachemgr_bucket | timechart count(cache_hit) as Hits count(cache_miss) as Misses","source=*metrics.log",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$  source=*splunkd.log CacheManager AND TERM(action=download) AND TERM(status=succeeded) AND download_set=""*journal*""
              | rex field=cache_id "".*\|(?<customer_index>.*)~.*~.*\|""
              | eval identifier=(cache_id + host)
              | stats count by identifier, customer_index
              | stats count(eval(count>1)) as duplicate_downloads, sum(count) as all_downloads count(eval(count>=10)) as excessive_duplicate_downloads by customer_index
              | eval duplicate_percent=if(all_downloads=0, 0, round((duplicate_downloads/all_downloads)*100, 2))
              | sort  - duplicate_percent
              | fields customer_index, duplicate_percent  all_downloads duplicate_downloads excessive_duplicate_downloads 
              | rename customer_index as Index, duplicate_percent as ""Repeated Download Percent"",  all_downloads as ""All Downloads"", duplicate_downloads as ""Repeated Downloads"", excessive_duplicate_downloads as ""Excessively Repeated Downloads""
            ]]>
          ","source=*splunkd.log",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_app_soar",,"
      | rest /services/authentication/current-context splunk_server=local | table title roles | mvexpand roles | search roles=""*admin"" OR roles=""*splunk_app_soar"" OR roles=""*splunk_app_soar_dashboards""
    ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"| eval index_prefix_label=index_prefix.""*""  | stats count by index_prefix_label, index_prefix | sort -count",,,"soar_case_insights"
tbd,,,"index=*phantom_container
index=*phantom_note",,,"splunk_app_soar",,"(index=*phantom_container OR index=*phantom_note) sourcetype=phantom_search | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)(container|note)$"" | fields * ",,"sourcetype=phantom_search","soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
      | search index=$index_prefix$container | dedup id | search id=""$case$"" | table status, id, close_time, start_time, owner_name
    ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
      | search index=$index_prefix$note container=""$case$"" | table note_type, content, create_time, container, author, container_label, title
    ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"| table status | head 1",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
            | dedup id
            | eval tnow=now()
            | strcat start_time ""+0000"" start_utc
            | strcat close_time ""+0000"" end_utc
            | eval end = if (close_time=""null"", tnow, strptime(end_utc,""%Y-%m-%dT%H:%M:%S""))
            | eval start = strptime(start_utc,""%Y-%m-%dT%H:%M:%S.%6QZ%z"")
            | eval duration= end-start
            | eval field_in_hhmmss=tostring(duration, ""duration"")
            | rex field=field_in_hhmmss ""(?&lt;d&gt;[^\.]+)\.[0-9]+""
            | table d
          ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"| eval username=if(owner_name==""null"", ""unassigned"", owner_name) | table username | head 1",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
            index=$index_prefix$action_run | dedup id | search container=""$case$"" | spath path=""targets{}.parameters{}.""
            | rename ""targets{}.parameters{}.""* as ""parameters""* | table create_time action name message type status parameters
            | rename create_time as ""Create Time"" action as ""Action"" name as ""Action Name"" message as ""Message"" type as ""Action Type"" status as ""Status"" parameters as ""Parameters""         ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
            search note_type=general | rex field=content ""&lt;p&gt;(?&lt;content&gt;.*)&lt;/p&gt;"" | table create_time container container_label title content
            | rename create_time as ""Create Time"" container as ""Container ID"" container_label as ""Container Label"" title as ""Title"" content as ""Content""
          ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
            search note_type=task | table create_time title content
            | rename create_time as ""Create Time"" title as ""Title"" content as ""Content""
          ",,,"soar_case_insights"
tbd,,,,,,"splunk_app_soar",,"
      | rest /services/authentication/current-context splunk_server=local | table title roles | mvexpand roles | search roles=""*admin"" OR roles=""*splunk_app_soar"" OR roles=""*splunk_app_soar_dashboards""
    ",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval index_prefix_label=index_prefix.""*"" | stats count by index_prefix_label, index_prefix | sort -count",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| stats  count by owner_name | sort -count",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| search owner_name=$owner_name$ $severity$ $sensitivity$ | stats count by container_label | sort - count",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| search owner_name=$owner_name$ $severity$ $sensitivity$ | eval status_label = upper(substr(status,1,1)).substr(status,2) | stats count by status_label, status | sort - count",,,"soar_cases_performance"
tbd,,,"index=*phantom_container",,,"splunk_app_soar",,"index=*phantom_container sourcetype=phantom_search | rex field=index ""(?&lt;index_prefix&gt;[\S]*_*phantom_)(container)$"" | fields * ",,"sourcetype=phantom_search","soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| search index=$index_prefix$container | table index, id, close_time, start_time, container_update_time, avg_duration, status, end_utc, start_utc, container_type, owner_name, owner, role, container_label, severity, sensitivity | eval owner_name=if((owner == ""null"" and role == ""null""), ""Unassigned"", owner_name) | eval owner_name=if((owner == ""null"" and role != ""null""), owner_name, owner_name) | eval most_recent_update=if((container_update_time == ""null""), start_time, container_update_time) | sort -most_recent_update | dedup index id | fields - owner",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| search $container_type$ container_label=* owner_name=$owner_name$ $status$ $severity$ $sensitivity$ | table index, id, close_time, start_time, avg_duration, status, end_utc, start_utc, container_type, owner_name, container_label",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| search $container_type$ $label$ owner_name=""$owner_name$"" $status$ $severity$ $sensitivity$ | table index, id, close_time, start_time, avg_duration, status, end_utc, start_utc, container_type, owner_name, container_label",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval tnow = now() | strcat start_time ""+0000"" start_utc | strcat close_time ""+0000"" end_utc | eval end = if (close_time=""null"", tnow,strptime(end_utc,""%Y-%m-%dT%H:%M:%S.%6QZ%z"") ) |  eval start = strptime(start_utc,""%Y-%m-%dT%H:%M:%S.%6QZ%z"") | eval duration= end -start ",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval tnow = now() | strcat start_time ""+0000"" start_utc | strcat close_time ""+0000"" end_utc | eval end = if (close_time=""null"", tnow,strptime(end_utc,""%Y-%m-%dT%H:%M:%S.%6QZ%z"") ) |  eval start = strptime(start_utc,""%Y-%m-%dT%H:%M:%S.%6QZ%z"") | eval duration= end -start ",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval new=mvfilter(match(status,""new"")) | stats count(new)",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval open=mvfilter(match(status,""open"")) | stats count(open)",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval closed=mvfilter(match(status,""closed"")) | stats count(closed)",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| stats avg(duration) as avg_duration | eval field_in_hhmmss=tostring(avg_duration, ""duration"") | rex field=field_in_hhmmss ""(?&lt;duration&gt;[^\.]+)(\.[0-9])*"" | fields - field_in_hhmmss | fields - avg_duration",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| eval end = strptime(close_time,""%Y-%m-%dT%H:%M:%S"") | eval start = strptime(start_time,""%Y-%m-%dT%H:%M:%S"") | eval duration= end -start | stats avg(duration) as avg_duration | eval field_in_hhmmss=tostring(avg_duration, ""duration"") | rex field=field_in_hhmmss ""(?&lt;duration&gt;[^\.]+)(\.[0-9])*"" | fields - field_in_hhmmss | fields - avg_duration",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| stats count by status",,,"soar_cases_performance"
tbd,,,"index = owner_name.",,,"splunk_app_soar",,"| eval duration = round(duration/3600, 2) | eval username=if((owner_name == ""Unassigned""), null, owner_name) | sort - duration, + username | head 10 | eval username_index = owner_name."" ("".index."")""  | chart list(duration) by id, username_index  | rename owner_name as ""Analyst"" index as ""Index"" duration as ""Duration in Hours"" id as ""Container ID"" | fields - avg_duration_sec",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"
            | stats count as ""Total Container Assigned"" avg(duration) as avg_duration_sec by index owner_name   |   sort - avg_duration | head  10 |  eval duration = round(avg_duration_sec/3600, 2)
            | rename owner_name as ""Analyst"" | fields - avg_duration_sec | rename duration as ""Duration in Hours"" index as ""Index""
          ",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"
            | stats avg(duration) as avg_duration by index id owner_name status container_type  | eval duration = round(avg_duration/3600, 2) | sort - duration  |  head 10 | eval container_type=if(container_type=""default"",  ""event"", container_type)
            | rename id as ""Container ID"" owner_name as ""Container Owner"" duration as ""Duration in Hours"" container_type as ""Container Type"" status as ""Status"" index as ""Index""|  fields - avg_duration",,,"soar_cases_performance"
tbd,,,,,,"splunk_app_soar",,"| stats avg(duration) as avg_duration_sec by index id | eval duration = round(avg_duration_sec/3600, 2) | sort - duration | head 10  | chart list(duration) by id, index | rename duration as ""Duration in Hours"" id as ""Container ID"" | fields - avg_duration_sec",,,"soar_cases_performance"
tbd,,,"index=*",,,"lame_documentation",,"| eventcount summarize=false index=* | stats count by index | fields index",,,"sourcetype_analytics"
tbd,,,,,,"lame_documentation",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"sourcetype_analytics"
tbd,,,,"inputlookup Sourcetype_Field_Info.csv",,"lame_documentation",,"| inputlookup Sourcetype_Field_Info.csv 
| search sourcetype=$src_type$
| eval fieldvalue = case(fieldvalue=1, ""Useless"", fieldvalue=5, ""Mostly Useless"", fieldvalue=10, ""Useful"")
| table fieldname, fieldvalue, rationale, fieldvalue",,,"sourcetype_analytics"
tbd,,,,"inputlookup Sourcetype_Analytics.csv",,"lame_documentation",,"| inputlookup Sourcetype_Analytics.csv | search index=$idx$ sourcetype=$src_type$ | table metric_description, metric_query",,,"sourcetype_analytics"
tbd,,,,,,"lame_documentation",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""

| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

| rename eai:acl.app as myapp

| mvexpand queries

| search queries=""*$src_type$*""
| table urlField, myapp, author, queries",,,"sourcetype_analytics"
tbd,,,"index=*",,,"CyberSentry_Training",,"| eventcount summarize=false index=* | stats count by index | fields index",,,"sourcetype_documentation"
tbd,,,,,,"CyberSentry_Training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"sourcetype_documentation"
tbd,,,,"inputlookup Sourcetype_Field_Info.csv",,"CyberSentry_Training",,"| inputlookup Sourcetype_Field_Info.csv 
| search sourcetype=$src_type$
| eval fieldvalue = case(fieldvalue=1, ""Useless"", fieldvalue=5, ""Mostly Useless"", fieldvalue=10, ""Useful"")
| table fieldname, fieldvalue, rationale, fieldvalue",,,"sourcetype_documentation"
tbd,,,,"inputlookup Sourcetype_Analytics.csv",,"CyberSentry_Training",,"| inputlookup Sourcetype_Analytics.csv | search index=$idx$ sourcetype=$src_type$ | table metric_description, metric_query",,,"sourcetype_documentation"
tbd,,,,,,"CyberSentry_Training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""

| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

| rename eai:acl.app as myapp

| mvexpand queries

| search queries=""*$src_type$*""
| table urlField, myapp, author, queries",,,"sourcetype_documentation"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"sourcetype_documentation"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"sourcetype_documentation"
tbd,,,,"inputlookup SourcetypeInfo",,"lame_training",,"| inputlookup SourcetypeInfo 
| search sourcetype=$src_type$
| eval fieldvalue = case(fieldvalue=1, ""Useless"", fieldvalue=5, ""Mostly Useless"", fieldvalue=10, ""Useful"")
| table sourcetype, fieldname, fieldvalue, rationale, fieldvalue",,,"sourcetype_documentation"
tbd,,,,"inputlookup Sourcetype_Analytics",,"lame_training",,"| inputlookup Sourcetype_Analytics 
| search sourcetype=$src_type$
| table sourcetype, metric_description, metric_query",,,"sourcetype_documentation"
tbd,,,,"inputlookup sourcetype_info",,"lame_analytic_documentation",,"| inputlookup sourcetype_info | search description = ""tbd""
| rename _key as the_key
| table the_key, index, sourcetype, description, usegroup",,,"sourcetype_information"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* finished | eval last_bucket_time=strftime(latest_bucket_time_secs, ""%F %T %z"")| eval transfered_mb=remote_bucket_bytes/1000000 | rename splunk_index AS ""Splunk Index"", virtual_index AS ""Archive Index"" | stats max(last_bucket_time) as ""Latest Archive Bucket Time"" sum(transfered_mb) as ""Total Transfered MB"" sum(buckets_copied) as ""Total Buckets Copied"" by ""Splunk Index"", ""Archive Index""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* earliest=-1d | rex max_match=1000 ""\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+ -\d{4} (?&lt;severity&gt;\w+) "" | where severity=""ERROR""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed $splunk_idx1$ | timechart count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed ""$splunk_idx2$"" |  eval mb = remote_bucket_bytes/1000000 | timechart sum(mb) by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | rename bucket_name AS ""Archived Bucket"", splunk_index AS ""Splunk Index"" | eval mb=round(remote_bucket_bytes/1000000,2) | stats sum(mb) as ""Archived Bucket MB"" by ""Splunk Index"", ""Archived Bucket""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* | rex max_match=1000 ""\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+ -\d{4} (?&lt;severity&gt;\w+) "" | where severity=""ERROR"" | timechart count AS errors","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* report: buckets_to_freeze_remaining_count buckets_to_freeze_deleted | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* buckets_to_freeze_remaining_count buckets_to_freeze_deleted report: $splunk_idx3$ | timechart sum(buckets_to_freeze_remaining_count) as ""Buckets to freeze"", sum(buckets_to_freeze_deleted) as ""Buckets frozen"" by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* buckets_to_freeze_size_bytes buckets_to_freeze_deleted_size_bytes report: $splunk_idx3$ | timechart sum(buckets_to_freeze_size_bytes) as ""to_freeze"", sum(buckets_to_freeze_deleted_size_bytes) as ""frozen"", by splunk_index | eval ""to_freeze_mb""=to_freeze/1000000 | eval frozen_mb=frozen/1000000 | rename to_freeze_mb AS ""Remaning diskspace to free (MB)"", frozen_mb AS ""Frozen transfered (MB)"", splunk_index AS ""Splunk index"" | fields -  to_freeze, frozen","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* Report: | eval secs = total_elapsed_ms/1000 | timechart sum(secs) as ""Seconds spent archiving"" by host","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* Report: | eval mb = remote_bucket_bytes/1000000 | timechart sum(mb) as ""Data transferred"" by host","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,,"inputlookup splunk-instance.csv",,"network-diagram-viz",,"| inputlookup splunk-instance.csv
| eval linktext=if(""$showPorts$""==""true"",linktext, """")
| eval color=random()%4 
| eval color=case(color==0,""blue"", color==1,""yellow"",color==2,""green"", 1==1, ""red"")
| eval color=if($showColors|s$==""1"",color,null)
",,,"splunk_examples"
tbd,,,,,,"network-diagram-viz",,"$healthCheckSPL$",,,"splunk_examples"
tbd,,,,,,"network-diagram-viz",,"$userActivitySPL$",,,"splunk_examples"
tbd,,,,"inputlookup nd-index-buckets.csv",,"network-diagram-viz",,"|inputlookup nd-index-buckets.csv
| eventstats max(linktext) as maximum
| eval linkWidth = $showLinkWidth$",,,"splunk_examples"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" services/server/introspection/queues
        | search title=""tcpin_queue*""
        | rex field=title ""tcpin_queue\.(?<pipeline_number>\d+)""
        | join splunk_server
        [ | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/properties/inputs
        | where match(title, ""splunktcp(-ssl)?:"")
        `dmc_get_port_from_splunktcp_stanza(title)`
        | stats delim="", "" values(port) as ports by splunk_server
        | mvcombine ports
        | fields splunk_server, ports]
        | join splunk_server [|rest splunk_server_group=dmc_group_indexer services/data/inputs/tcp/ssl]
        | eval val_last_1min=if(isnotnull(value_cntr1_size_bytes_lookback), round(value_cntr1_size_bytes_lookback, 2), ""N/A"")
        | eval val_last_10min=if(isnotnull(value_cntr2_size_bytes_lookback), round(value_cntr2_size_bytes_lookback, 2), ""N/A"")
        | eval queue_fill_last_1min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_1min, val_last_1min)
        | eval queue_fill_last_10min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_10min, val_last_10min)
      ]]>
    ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`| search search_group!=""dmc_group_*""
        ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              rex field=queue_fill_last_10min ""pset\d+:\s(?<queue_fill_last_10min>\d+\.\d+)""
              | stats max(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
              | where queue_fill_last_10min > 60
              | stats count as degraded_instance_count_queue_fill_ratio
            ]]>
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_indexer search_group=""$group$"" sourcetype=splunkd source=*splunkd.log ""WARN"" TcpInputConfig ""reverse dns lookups appear to be excessively slow, this may impact receiving from network inputs.""
            | stats count as degraded_instance_count_dnc_lookup, values(host) as hosts
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_indexer search_group=""$group$"" sourcetype=splunkd source=*splunkd.log log_level=""WARN"" component=TcpInputProc ""Stopping all listening ports.""
            | stats count as degraded_instance_count_port_closure, values(host) as hosts
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count as pset_count, values(ports), as ports values(queue_fill_last_1min) as queue_fill_last_1min, values(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
            | fields splunk_server, pset_count, ports, queue_fill_last_1min, queue_fill_last_10min
            | rename splunk_server as ""Instance"", pset_count as ""Pipeline Set Count"", ports as ""Ports"", queue_fill_last_1min as ""Queue Fill Ratio (Last 1 Minute)"", queue_fill_last_10min as ""Queue Fill Ratio (Last 10 Minutes)""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_throughput_split_by(host, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_timechart_for_metrics_log` per_second(kb) as avg_tcp_KBps by fwdType
            | rename avg_tcp_KBps as ""KB/s""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_throughput_split_by(destPort, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count, per_second(kb) as tcp_KBps
            | rename forwarder_count as ""Forwarder Count"", tcp_KBps as ""Throughput (KB/s)""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_forwarder_count_split_by(host, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count by fwdType
            | rename forwarder_count as ""Forwarder Count""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_forwarder_count_split_by(destPort, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | where host=""$drilldown_indexer_name$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | where fwdType=""$drilldown_type_forwarder_type$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | where destPort=""$drilldown_port_forwarder_port$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | bin _time minspan=30s
            | stats $fillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
            | `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | bin _time minspan=30s
            | stats $fillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
            | `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_queue_fill_ratio(""*"", splunktcpin, $fillRatioAggrFunc$, $drilldown_queue_fill_ratio_metric$)`",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | `dmc_timechart_for_metrics_log` partial=f limit=25 $fillRatioAggrFunc$(fill_perc) AS fill_percentage by host
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | `dmc_timechart_for_metrics_log` partial=f $fillRatioAggrFunc$(fill_perc) AS fill_percentage
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
            | rest splunk_server=$splunk_server$ services/server/introspection/queues
            | search title=""tcpin_queue*""
            | rex field=title ""tcpin_queue\.(?<pipeline_number>\d+)""
            | join splunk_server
            [ | rest splunk_server=$splunk_server$ /services/properties/inputs
              | where match(title, ""splunktcp(-ssl)?:"")
              `dmc_get_port_from_splunktcp_stanza(title)`
              | stats delim="", "" values(port) as ports by splunk_server
              | mvcombine ports
              | fields splunk_server, ports]
            | join splunk_server [|rest splunk_server_group=dmc_group_indexer services/data/inputs/tcp/ssl]
            | eval val_last_1min=if(isnotnull(value_cntr1_size_bytes_lookback), round(value_cntr1_size_bytes_lookback, 2), ""N/A"")
            | eval val_last_10min=if(isnotnull(value_cntr2_size_bytes_lookback), round(value_cntr2_size_bytes_lookback, 2), ""N/A"")
            | eval queue_fill_last_1min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_1min, val_last_1min)
            | eval queue_fill_last_10min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_10min, val_last_10min)
            ]]>
    ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              rex field=queue_fill_last_10min ""pset\d+:\s(?<queue_fill_last_10min>\d+\.\d+)""
              | stats max(queue_fill_last_10min) as queue_fill_last_10min
              | where queue_fill_last_10min > 60
              | stats count as degraded_instance_count_queue_fill_ratio
            ]]>
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` splunk_server=$splunk_server$ sourcetype=splunkd source=*splunkd.log ""WARN"" TcpInputConfig ""reverse dns lookups appear to be excessively slow, this may impact receiving from network inputs.""
            | stats count as dns_lookup_warning_count
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` splunk_server=$splunk_server$ sourcetype=splunkd source=*splunkd.log log_level=""WARN"" component=TcpInputProc ""Stopping all listening ports.""
            | stats count as port_closure_count
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count as pset_count, values(ports), as ports values(queue_fill_last_1min) as queue_fill_last_1min, values(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
            | fields splunk_server, pset_count, ports, queue_fill_last_1min, queue_fill_last_10min
            | rename splunk_server as ""Instance"", pset_count as ""Pipeline Set Count"", ports as ""Ports"", queue_fill_last_1min as ""Queue Fill Ratio (Last 1 Minute)"", queue_fill_last_10min as ""Queue Fill Ratio (Last 10 Minutes)""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` host=$host$
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count, avg(tcp_KBps) as avg_tcp_KBps
            | rename forwarder_count as ""Forwarder Count"", avg_tcp_KBps as ""Throughput (KB/s)""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` host=$host$
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline 
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb) 
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline) 
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) 
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps) 
            | `dmc_rename_forwarder_type(fwdType)` 
            | `dmc_time_format(last_connected)` 
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count 
            | search hostname=""***"" 
            | search status=""*"" 
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv | search id = $qID$
| table question optionA, optionB, optionC,optionD, reference, video, answer, id",,,"study_questions"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv 
| stats count by testType",,,"study_questions"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv
| search testType=""$s_cert$""
| table question optionA, optionB, optionC,optionD, reference, video, answer, id",,,"study_questions"
tbd,,,,,,"lame_training",,"| table id, answer ",,,"study_questions"
tbd,,,,"inputlookup source_info",,"lame_analytic_documentation",,"| inputlookup source_info
| rename _key as the_key
| table the_key, index, source, description, usegroup",,,"summary_index_information"
tbd,,,"index=corelight",,,"log_analysis_made_easy",,"index=corelight sourcetype=corelight_conn 
| stats count by src_ip
| rex field=src_ip ""(?&lt;src_subnet&gt;\d+\.\d+\.\d+)\.\d+""
| stats count by src_subnet",,"sourcetype=corelight_conn","test_collapsible"
tbd,,,"index=corelight",,,search,,"index=corelight sourcetype=corelight_conn | head 10
| table src_ip, dest_ip, dest_port, src_port",,"sourcetype=corelight_conn",testurl
tbd,,,,,,"log_analysis_made_easy",,"
      | makeresults | eval foo=""bar""
      | table foo
    ",,,timebasedqueries
tbd,,,"index=corelight",,"lookup ip_inventory.csv","log_analysis_made_easy",,"
      index=corelight sourcetype=corelight_conn src_ip=$src_ip$ | lookup ip_inventory.csv ip as src_ip output hostname, purpose, other, model, os | table src_ip, hostname, purpose, other, model, os
    ",,"sourcetype=corelight_conn",tokentrial
tbd,,,,,,"lame_training",,"| makeresults | eval foo = ""bar""",,,urlgenerator
tbd,,,"index=wekan",,"lookup wekan_lists.csv
lookup wekan_users.csv","lame_wekan_pm",,"index=wekan (sourcetype=""wekan_card_comments"" OR sourcetype=""wekan_checklistitems""  OR (sourcetype=""wekan_cards""  AND boardId=""$board$"" (userId=""$user$"" OR assignees_0=""$user$"")  ) )
| eval joinID = coalesce(cardId, id)
| dedup id
| eval checklistTitleWithStatus = case(sourcetype=""wekan_checklistitems"" AND isFinished=""true"", title + "" - Complete"", sourcetype=""wekan_checklistitems"" AND isFinished=""false"", title + "" - Pending"" )
| eval dateLastActivityTime = case(sourcetype=""wekan_cards"", endAt)
| eval cardTitle = case(sourcetype=""wekan_cards"", title)
| eval dueAt = strftime(dueAt, ""%Y-%m-%d"")
| eval endAt = strftime(endAt, ""%Y-%m-%d"")
| eval startAt = strftime(startAt, ""%Y-%m-%d"")
| eval modifiedAt = strftime(modifiedAt, ""%Y-%m-%d"")
| stats values(*) as * by joinID
| lookup wekan_lists.csv id as listId output title as listTitle
| lookup wekan_users.csv id as assignees_0 output username as assignees_0
| eval _time = dateLastActivityTime
| table archived cardTitle, description, text, checklistTitleWithStatus, status assignedBy, listTitle, assignees_0, createdAt, _time,  dueAt, endAt, isFinished, isOvertime, listTitle, modifiedAt, requestedBy, startAt, title, type

",,"sourcetype=wekan_card_comments
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_checklistitems
sourcetype=wekan_checklistitems
sourcetype=wekan_cards
sourcetype=wekan_cards","user_activity_overview"
tbd,,,,"inputlookup wekan_report_categories.csv",,"lame_wekan_pm",,"| inputlookup wekan_report_categories.csv | search report_template=$reportTemplate$
| sort -order
| table id
| transpose
| rename ""row 1"" as row1, ""row 2"" as row2, ""row 3"" as row3, ""row 4"" as row4, ""row 5"" as row5
",,,"user_activity_overview"
tbd,,,"index=wekan",,,"lame_wekan_pm",,"index=wekan sourcetype=wekan_users | stats values(id) as id count by username",,"sourcetype=wekan_users","user_activity_overview"
tbd,,,"index=wekan",,,"lame_wekan_pm",,"index=wekan sourcetype=""wekan_boards"" archived=false
| dedup title
| table id, title",,"sourcetype=wekan_boards","user_activity_overview"
tbd,,,,"inputlookup wekan_report_categories.csv",,"lame_wekan_pm",,"| inputlookup wekan_report_categories.csv | stats values(template_name) as template_name count by report_template",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search archived=false | stats dc(cardTitle) by listTitle",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search listTitle = ""$tok_1$"" AND archived=false
| fields - createdAt, modifiedAt title listTitle, isFinished,  isOverTime,  type, assignedBy, assignees_0
| mvexpand checklistTitleWithStatus
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, endAt  dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""
| sort startAt, Task",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search listTitle = ""$tok_2$"" AND archived=false
| fields - createdAt, modifiedAt title listTitle, isFinished,  isOverTime,  type, assignedBy, assignees_0
| mvexpand checklistTitleWithStatus
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search listTitle = ""$tok_3$"" AND archived=false
| fields - createdAt, modifiedAt title listTitle, isFinished,  isOverTime,  type, assignedBy, assignees_0
| mvexpand checklistTitleWithStatus
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search listTitle = ""$tok_4$"" AND archived=false
| fields - createdAt, modifiedAt title listTitle, isFinished,  isOverTime,  type, assignedBy, assignees_0
| mvexpand checklistTitleWithStatus
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""",,,"user_activity_overview"
tbd,,,,,,"lame_wekan_pm",,"| search listTitle = ""$tok_5$"" AND archived=false
| fields - createdAt, modifiedAt title listTitle, isFinished,  isOverTime,  type, assignedBy, assignees_0
| mvexpand checklistTitleWithStatus
| eval status = case(checklistTitleWithStatus like ""%Pending"", ""pending"", checklistTitleWithStatus like ""%Complete"", ""complete"")
| table cardTitle, description, text checklistTitleWithStatus, status requestedBy, startAt, dueAt,  isOvertime
| rename cardTitle as ""Task"", text as ""Comments on Task"", checklistTitleWithStatus as ""Checklist Items"", status as ""Checklist Item Status""",,,"user_activity_overview"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/index-volumes/$volume$
      | eval volumeSizeGB = if(total_size > 1, round(total_size / 1024, 2), 0)
      | eval maxSizeGB = if(isNum(max_size), round(max_size / 1024, 2), ""unlimited"")
      | eval diskUsageGB = if(volumeSizeGB == 0, ""-"", volumeSizeGB)."" / "".maxSizeGB
    ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/index-volumes
          | `dmc_exclude_volumes`
          | fields title
          | dedup title
        ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats dc(splunk_server) as Indexers
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(volumeSizeGB) as totalVolumeSizeGB
            | eval totalVolumeSizeGB = round(totalVolumeSizeGB, 2)."" GB""
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats avg(volumeSizeGB) as avgVolumeSizeGB
            | eval avgVolumeSizeGB = round(avgVolumeSizeGB, 2)."" GB""
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            fields splunk_server, diskUsageGB, maxSizeGB, volume_path
            | rename splunk_server as ""Instance"", diskUsageGB as ""Volume Usage (GB)"", maxSizeGB as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_set_bin_for_disk_usage`
            | stats latest(data.total_size) as totalSize by host _time
            | `dmc_timechart_for_disk_usage` $funcDiskSizeUsage$(eval(totalSize /1024)) as ""Volume Size""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | eval pctDiskUsage = 'data.total_size' / 'data.max_size'
            | bin _time minspan=10min
            | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
            | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
            | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
            | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | eval pctDiskUsage = 'data.total_size' / 'data.max_size'
            | bin _time minspan=10min
            | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
            | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
            | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
            | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/index-volumes
      | fields title, total_size, max_size, volume_path
      | `dmc_exclude_volumes`
    ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
      | join type=outer title [
        | rest splunk_server=$splunk_server$ /services/data/indexes $datatype$]
      | `dmc_exclude_indexes`
      | rex field=homePath ""volume:(?&lt;home_vol_name&gt;[^/\\\]*)(?:/|\\\)""
      | rex field=coldPath ""volume:(?&lt;cold_vol_name&gt;[^/\\\]*)(?:/|\\\)""
    ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by title",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | stats dc(title) as count
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.hot_bucket_count, bucket_dirs.home.warm_bucket_count, bucket_dirs.cold.bucket_count, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval bucket_dirs.home.hot_bucket_count = if(isnotnull('bucket_dirs.home.hot_bucket_count'), 'bucket_dirs.home.hot_bucket_count', 0)
            | eval bucket_dirs.home.warm_bucket_count = if(isnotnull('bucket_dirs.home.warm_bucket_count'), 'bucket_dirs.home.warm_bucket_count', 0)
            | eval bucket_dirs.cold.bucket_count = if(isnotnull('bucket_dirs.cold.bucket_count'), 'bucket_dirs.cold.bucket_count', 0)
            | eval bucket_count = case(
                home_vol_name == cold_vol_name, 'bucket_dirs.home.hot_bucket_count' + 'bucket_dirs.home.warm_bucket_count' + 'bucket_dirs.cold.bucket_count',
                home_vol_name == ""$volume_name$"", 'bucket_dirs.home.hot_bucket_count' + 'bucket_dirs.home.warm_bucket_count',
                cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.bucket_count')
            | stats sum(bucket_count) as bucket_count
            | eval bucket_count = toString(bucket_count, ""commas"")
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_count, bucket_dirs.cold.event_count, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval bucket_dirs.home.event_count = if(isnotnull('bucket_dirs.home.event_count'), 'bucket_dirs.home.event_count', 0)
            | eval bucket_dirs.cold.event_count = if(isnotnull('bucket_dirs.cold.event_count'), 'bucket_dirs.cold.event_count', 0)
            | eval event_count = case(
            home_vol_name == cold_vol_name, 'bucket_dirs.home.event_count' + 'bucket_dirs.cold.event_count',
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_count',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_count')
            | stats sum(event_count) as count
            | eval count = toString(count, ""commas"")
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval min_time = case(
            home_vol_name == cold_vol_name, min('bucket_dirs.home.event_min_time', 'bucket_dirs.cold.event_min_time'),
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_min_time',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_min_time')
            | stats min(min_time) as min_time
            | `dmc_time_format(min_time)`
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_max_time, bucket_dirs.cold.event_max_time, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval max_time = case(
            home_vol_name == cold_vol_name, max('bucket_dirs.home.event_max_time', 'bucket_dirs.cold.event_max_time'),
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_max_time',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_max_time')
            | stats max(max_time) as max_time
            | `dmc_time_format(max_time)`
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where title == ""$volume_name$""
            | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
            | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
            | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
            | fields title, disk_usage_gb, max_size_gb, volume_path
            | rename title as Volume, disk_usage_gb as ""Volume Usage (GB)"", max_size_gb as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
            | fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, cold_bucket_size, total_size, home_vol_name, cold_vol_name
            | rename cold_bucket_size AS cold_size
            | eval cold_size = if(isnull(cold_size), 0, cold_size)
            | eval home_size = total_size - cold_size
            | eval cold_size = round(cold_size / 1024, 2)
            | eval home_size = round(home_size / 1024, 2)
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval dir_size = case(dir == ""home"", home_size, dir == ""cold"", cold_size)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | eval data_age_home_day = if(dir == ""home"", round((now() - 'bucket_dirs.home.event_min_time') / 86400, 0), NULL)
            | eval data_age_cold_day = if(dir == ""cold"", round((now() - 'bucket_dirs.cold.event_min_time') / 86400, 0), NULL)
            | eval data_age_day = case(dir == ""home"", data_age_home_day, dir == ""cold"", data_age_cold_day)
            | eval data_age_day = if(isnull(data_age_day), ""n/a"", data_age_day)
            | where vol_name == ""$volume_name$""
            | fields title dir dir_size data_age_day vol_name
            | where isnotnull(vol_name)
            | eval index_dir_name = title."":"".dir
            | fields index_dir_name dir_size data_age_day
            | sort - dir_size
            | rename index_dir_name as ""Index:Directory"" dir_size AS ""Disk Usage (GB)"" data_age_day as ""Data Age (Days)""
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
            | fields title, cold_bucket_size, total_size, home_vol_name, cold_vol_name
            | rename cold_bucket_size AS cold_size
            | eval cold_size = if(isnull(cold_size), 0, cold_size)
            | eval home_size = total_size - cold_size
            | eval cold_size = round(cold_size / 1024, 2)
            | eval home_size = round(home_size / 1024, 2)
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval dir_size = case(dir == ""home"", home_size, dir == ""cold"", cold_size)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | where vol_name == ""$volume_name$""
            | fields title dir dir_size vol_name
            | where isnotnull(vol_name) AND isnotnull(dir_size) AND dir_size > 0
            | eval index_dir_name = title."":"".dir
            | where vol_name == ""$volume_name$""
            | stats sum(dir_size) AS dir_size by index_dir_name
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, home_vol_name, cold_vol_name, datatype
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval indexwithtype=title."":"".datatype, title = case(dir == ""home"", title."":home"", dir == ""cold"", title."":cold"")
            | eval data_age_home_day = if(dir == ""home"", round((now() - 'bucket_dirs.home.event_min_time') / 86400, 0), NULL)
            | eval data_age_cold_day = if(dir == ""cold"", round((now() - 'bucket_dirs.cold.event_min_time') / 86400, 0), NULL)
            | eval data_age_day = case(dir == ""home"", data_age_home_day, dir == ""cold"", data_age_cold_day)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | where vol_name == ""$volume_name$""
            | sort - data_age_day
            | fields title, data_age_home_day, data_age_cold_day, indexwithtype
            | where NOT (isnull(data_age_home_day) AND isnull(data_age_cold_day))
            | rename title as Index, data_age_home_day as ""home data age (days)"", data_age_cold_day as ""cold data age (days)""
          ",,,"volume_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Volumes data.name=""$volume_name$""
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | eval data.total_capacity = round('data.max_size' / 1024, 2)                    
            | `dmc_timechart_for_disk_usage` partial=false $funcVolume$(data.total_size) as volume_usage, latest(data.total_capacity) as volume_capacity
            | fields _time, volume_usage $capacityOverlay$
          ",,"sourcetype=splunk_disk_objects","volume_detail_instance"
tbd,,,"index=zeek",,,"log_analysis_made_easy",,"index=zeek sourcetype=bro_conn | head 100 | table src_ip, dest_ip",,"sourcetype=bro_conn",workflowactions
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_introspection` search_group=""$role$"" sourcetype=splunk_resource_usage component=PerProcess
             | `dmc_rename_introspection_fields`
             | `dmc_set_bin`
             | stats sum(pct_cpu) as pct_cpu by host, workload_pool, _time
             | `dmc_timechart` $CPUFunction$(pct_cpu) as pct_cpu by workload_pool
          ",,"sourcetype=splunk_resource_usage","workload_management"
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_introspection` search_group=""$role$"" sourcetype=splunk_resource_usage component=PerProcess
             | `dmc_rename_introspection_fields`
             | `dmc_set_bin`
             | stats sum(mem_used) as mem_used by host, workload_pool _time
             | `dmc_timechart` $MemFunc$(mem_used) as mem_used by workload_pool
          ",,"sourcetype=splunk_resource_usage","workload_management"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
         `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ host=$host$ sourcetype=wlm_*
         | stats dc(search_id) as ""search_count"" by wlm_action
      ",,"sourcetype=wlm_*","workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""abort"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""move"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""alert"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
	  `dmc_set_index_internal` host=$host$ sourcetype=wlm_*
	  | stats dc(search_id) as ""search_count"" by wlm_action
      ",,"sourcetype=wlm_*","workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""abort"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""move"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""alert"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_per_pool_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest /services/workloads/pools splunk_server_group=* splunk_server_group=*
            | rename title AS pool_name category AS pool_type
            | eval pool_and_type = pool_name."" ("".pool_type."")""
            | stats values(splunk_server)  values(pool_type) as pool_type values(pool_name) as pool_name by pool_and_type
            | fields pool_and_type pool_type pool_name
          ",,,"workload_management_per_pool_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_per_pool_instance"
tbd,,,,,,"splunk_monitoring_console",,"
              | rest splunk_server_group=$role$ splunk_server_group=$dmc_group$ splunk_server=$host$ /services/workloads/pools
              | rename title AS pool_name category AS pool_type
              | eval pool_and_type = pool_name."" ("".pool_type."")""
              | stats values(pool_type) as pool_type values(pool_name) as pool_name by pool_and_type
              | fields pool_and_type pool_type pool_name
          ",,,"workload_management_per_pool_instance"
tbd,,,"index=lame_training",,,"lame_training",,"index=lame_training sourcetype=lame_conn src_ip=""$mySrcIP$"" direction=$myDirection$ dest_ip IN ($dest_ip$)
          | table _time, src_ip, dest_ip, direction, bytes_in, bytes_out",,"sourcetype=lame_conn","youtube_dashboard"
tbd,,,"index=lame_training",,,"lame_training",,"index=lame_training sourcetype=lame_conn | table direction, dest_ip",,"sourcetype=lame_conn","youtube_dashboard"
tbd,,,,,,"lame_training",,"
| stats count by direction",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by dest_ip",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| table _time, src_ip, dest_ip, direction, bytes_in, bytes_out",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by direction",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by bytes_in",,,"youtube_dashboard"
