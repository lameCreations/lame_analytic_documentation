description,eventtypes,hosts,indexes,inputlookups,lookups,myapp,outputlookups,queries,sources,sourcetypes,urlField
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| stats dc(dest_ip) by src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| eventstats dc(dest_ip) by src_ip
| sort src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| table src_ip, dest_ip 
| sort src_ip
| streamstats dc(dest_ip) by src_ip",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| streamstats count as EventCount
| table _time, EventCount",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_conn 
| streamstats avg(bytes_out)  window=10
| table avg(bytes_out), _time",,"sourcetype=corelight_conn","12_help_stats_eventstats_streamstats"
tbd,,,"index=corelight",,,"lame_training",,"index=corelight sourcetype=corelight_dhcp 
| streamstats current=false last(src_ip) as new_dhcp_ip last(_time) as time_of_change by mac
| where src_ip!=new_dhcp_ip 
| convert ctime(time_of_change) as time_of_change 
| rename src_ip as old_dhcp_ip 
| table time_of_change, mac, old_dhcp_ip, new_dhcp_ip",,"sourcetype=corelight_dhcp","12_help_stats_eventstats_streamstats"
tbd,,,"index=os",,,"lame_training",,"index=os process=sshd (action=failure OR action=success) AND (Accepted OR Failed) user=troy
| sort 0 src, dest, user, _time asc
| streamstats count as contiguous_action by src, dest, user action current=true window=50 reset_on_change=true
| where (action=""success"" AND contiguous_action&gt;3) OR (action=""failure"" AND contiguous_action=1)
| sort - _time
| transaction src, dest, user  maxevents=2 startswith=action=""success"" endswith=action=failure
| rename contiguous_action as success_attempts

| stats count values(dest) as distinct_dests sum(success_attempts) as ""Total Successes"" by user",,,"12_help_stats_eventstats_streamstats"
tbd,,,,"inputlookup Demo_SSH_Logs.csv",,"lame_training",,"| inputlookup Demo_SSH_Logs.csv
| sort 0 src, dest, user, _time asc
 | streamstats count as contiguous_action by src, dest, user action current=true window=50 reset_on_change=true
| where (action=""success"" AND contiguous_action&gt;3) OR (action=""failure"" AND contiguous_action=1)
| sort - _time
| transaction src, dest, user  maxevents=2 startswith=action=""success"" endswith=action=failure
| rename contiguous_action as success_attempts

| stats count values(dest) as distinct_dests sum(success_attempts) as ""Total Successes"" by user",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) AND action!=dropped
|  stats sum(bytes_out) as ""sumOfBytesOut"" by src_ip, dest_ip
| eventstats sum(sumOfBytesOut) AS total_bytes_out by src_ip
| table src_ip dest_ip sumOfBytesOut, total_bytes_out
| sort src_ip - sumOfBytesOut",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) AND action!=dropped
|  stats sum(bytes_out) as ""sumOfBytesOut"" by src_ip, dest_ip
| eventstats sum(sumOfBytesOut) AS total_bytes_out by src_ip
| eval percent_bytes_out = sumOfBytesOut/total_bytes_out * 100
| table src_ip dest_ip total_bytes_out percent_bytes_out
| where percent_bytes_out &gt; 51
| sort - percent_bytes_out dest",,,"12_help_stats_eventstats_streamstats"
tbd,"eventtype=bro_conn",,,,,"lame_training",,"eventtype=bro_conn src_ip=10.89.11.* NOT (dest_ip=192.168.* OR dest_ip=10.* ) action!=dropped
| sort _time
| streamstats sum(bytes_out) as total_bytes_out by src_ip
| table _time bytes_out total_bytes_out",,,"12_help_stats_eventstats_streamstats"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv 
| search type=""app""
| eval tag=""csv"" 
| rename name as label
| append 
    [| rest /services/apps/local 
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""app"" | table name | rename name as label ] 
    | eval tag=""rest"" ] 
| stats count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by label
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename label as app
| table app status",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""lookup""
| eval tag=""csv"" 
| rename name as title
| append 
    [| rest /servicesNS/-/-/data/lookup-table-files
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""lookup"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as lookup_name
| table lookup_name, status",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""eventtype""
| eval tag=""csv"" 
| rename name as title
| append 
    [|rest servicesNS/-/-/saved/eventtypes
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""eventtype"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats values(search) as search count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as eventtype
| table eventtype, status, search",,,"about_this_app"
tbd,,,,"inputlookup lame_training_requirements.csv
inputlookup lame_training_requirements.csv",,"lame_training",,"| inputlookup lame_training_requirements.csv
| search type=""datamodel""
| eval tag=""csv"" 
| rename name as title
| append 
    [ |rest /services/data/models
    | search disabled=0 [| inputlookup lame_training_requirements.csv where required=""yes"" AND type=""datamodel"" | table name | rename name as title ] 
    | eval tag=""rest"" ] 
| stats values(search) as search count(eval(tag==""csv"")) as csv count(eval(tag==""rest"")) as rest list(required) as required by title
| eval status=if((rest&gt;0), ""Installed"", ""Missing"") | rename title as datamodel
| table datamodel, status",,,"about_this_app"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* prefilter_action=filter
        | stats count by search_type
      ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",," stats sum(count) as total_prefiltered",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_event_local_search_dispatch`
            | stats dc(search_id) as total_dispatched
            | eval output_with_percentage = ""$total_prefiltered$""."" ("".round($total_prefiltered$/($total_prefiltered$ + total_dispatched) * 100, 2)."" %)""
            | fields output_with_percentage
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""adhoc""
            | fields count
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""scheduled""
            | fields count
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* prefilter_action=filter
          | stats count by $SplitByVariable$
        ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eval percent = round(count / $total_prefiltered_searches$ * 100, 2)."" %""
            | rename prefilter_rule as Rule, user as User, app as App, search_type as ""Search Type"", search_name as ""Search Name"", host as Instance, count as Count, percent as ""Percent of Total""
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ sourcetype=wlm_* search_type=""scheduled*"" prefilter_action=filter
            | stats count as prefiltered_count by prefilter_rule, user, app, host, search_name
            | fields search_name, app, user, host, prefilter_rule, prefiltered_count
            | sort - prefiltered_count
            | rename search_name as ""Search Name"", app as App, user as User, host as Instance, prefilter_rule as ""Prefiltering Rule Triggered"", prefiltered_count as ""Prefiltered Count""
          ",,"sourcetype=wlm_*","admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest [inputlookup dmc_assets | where host = $scheduled_search_host_drilldown|s$ | rename serverName AS splunk_server | return splunk_server] ""/servicesNS/-/-/saved/searches/$scheduled_search_drilldown$"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | fields cron_schedule, next_scheduled_time, search 
            | rename cron_schedule as ""Cron Schedule"", next_scheduled_time as ""Next Scheduled Time"", search as ""Search""
          ",,,"admission_control_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_internal` host=$host$ sourcetype=wlm_* prefilter_action=filter
        | stats count by search_type
      ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",," stats sum(count) as total_prefiltered",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_event_local_search_dispatch`
            | stats dc(search_id) as total_dispatched
            | eval output_with_percentage = ""$total_prefiltered$""."" ("".round($total_prefiltered$/($total_prefiltered$ + total_dispatched) * 100, 2)."" %)""
            | fields output_with_percentage
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""adhoc""
            | fields count
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | where search_type=""scheduled""
            | fields count
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=wlm_* host=$host$ prefilter_action=filter
          | stats count by $SplitByVariable$
        ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eval percent = round(count / $total_prefiltered_searches$ * 100, 2)."" %""
            | rename prefilter_rule as Rule, user as User, app as App, search_type as ""Search Type"", search_name as ""Search Name"", count as Count, percent as ""Percent of Total""
          ",,,"admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=wlm_* search_type=""scheduled*"" prefilter_action=filter
            | stats count as prefiltered_count by prefilter_rule, user, app, search_name
            | fields search_name, app, user, prefilter_rule, prefiltered_count
            | sort - prefiltered_count
            | rename search_name as ""Search Name"", app as App, user as User, prefilter_rule as ""Prefiltering Rule Triggered"", prefiltered_count as ""Prefiltered Count""
          ",,"sourcetype=wlm_*","admission_control_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/$scheduled_search_drilldown$"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | fields cron_schedule, next_scheduled_time, search 
            | rename cron_schedule as ""Cron Schedule"", next_scheduled_time as ""Next Scheduled Time"", search as ""Search""
          ",,,"admission_control_monitoring_instance"
tbd,,,,"inputlookup analytics_info",,"lame_analytic_documentation",,"| inputlookup analytics_info
| rename _key as the_key
| table the_key, queries, description, myapp, urlField",,,"analytic_information"
tbd,,,,,"lookup dashboard_details","lame_training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| table eai:data, app.owner, id, eai:acl.app author, eai:acl.sharing | lookup dashboard_details id as id output details, mitre, usecase


",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"| rest /servicesNS/-/-/saved/searches splunk_server=local
| search eai:acl.app=* AND author!=""nobody"" 
| table eai:acl.app, author, cron_schedule is_scheduled, disabled",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",," 
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,,,,"lame_training",,"
| eval cron_schedule = case(cron_schedule="""", ""n/a"", 1=1, cron_schedule)
| search eai:acl.app=* AND author!=""nobody"" cron_schedule!=""n/a""  is_scheduled=1 disabled=0
| stats count by author",,,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats count by myapp","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| fillnull value=""N/A"" datasources
| mvexpand queries
| search queries!=""|*"" AND queries!=""search*""
| table queries urlField, , myapp  author,
| stats dc(queries) as queries values(myapp) as myapp  values(author) as author by urlField
| sort - queries","source=dashboard_views",,"analytics_and_knowledge_object_overview"
tbd,,,,,,"splunk_monitoring_console",,"
              | `dmc_get_groups_containing_role(dmc_group_search_head)` 
              | search search_group!=""dmc_group_*""
            ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | rest splunk_server=$splunk_server$ /services/search/distributed/bundle/replication/config
            ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | fields replicationPolicy
                | eval replicationPolicy = upper(substr(replicationPolicy,1,1)).substr(replicationPolicy,2)
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | eval replicationThreads = case(isnull(replicationThreads), ""N/A"", isnotnull(replicationThreads), replicationThreads)
                | fields replicationThreads
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                  bundle_type=full_bundle
                  | eval repl_sec = replication_time_msec/1000
                  | stats avg(repl_sec)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                  bundle_type=delta_bundle
                  | eval repl_sec = replication_time_msec/1000
                  | stats avg(repl_sec)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                  bundle_type=full_bundle
                  | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                  | stats avg(size_mb)
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                  bundle_type=delta_bundle
                  | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                  | stats avg(size_mb)
                ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                | fields bundle_name, checksum, id, splunk_server, upload_time
                | mvcombine splunk_server
                | sort upload_time desc
                | eval latest_common=case(mvcount(splunk_server)==[
                  | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                  | stats dc(splunk_server) AS total_server_count
                  | return $total_server_count], 
                ""found"", mvcount(splunk_server)!=[
                  | rest splunk_server_group=dmc_group_indexer /services/admin/bundles/$splunk_server$
                  | stats dc(splunk_server) AS total_server_count
                  | return $total_server_count],
                ""none"")
                | where latest_common==""found""
                | head 1
              ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | eval upload_time=""Created at: "".(strftime(upload_time, ""%m/%d/%Y %H:%M:%S %p""))
                  | fields upload_time
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | rex field=id ""(?&lt;id&gt;([^\/]+$))""
                  | eval id = ""Bundle Id: "".tostring(id)
                  | fields id
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | eval checksum = ""Checksum: "".tostring(checksum)
                  | fields checksum
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                  | appendpipe [
                    stats count
                    | eval checksum=""No common knowledge bundle found across all peers""
                    | where count=0]
                  | fields checksum
                ",,,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=cycle_dispatch
                | stats count
              ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=cycle_dispatch
                | eval ratio = round((peer_success_count/peer_count)*100, 2)
                | stats count by ratio
                | rename ratio as ""Peer Success Ratio""
              ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
              | eval repl_sec = replication_time_msec/1000
              | `dmc_timechart_for_metrics_log` avg(repl_sec) by bundle_type
            ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundle_replication name=bundle_metadata
                | eval size_mb = round(bundle_bytes /1024 / 1024, 2)
                | `dmc_timechart_for_metrics_log` avg(size_mb) by bundle_type
            ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                     `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                     | eval repl_sec = replication_time_msec/1000
                     | `dmc_timechart_for_metrics_log` avg(repl_sec) AS avg_repl_time p90(repl_sec) AS 90th_perc_repl_time p10(repl_sec) AS 10th_perc_repl_time
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
                     `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=bundles_uploads name=peer_dispatch
                     | eval repl_sec = replication_time_msec/1000
                     | `dmc_timechart_for_metrics_log` limit=50 avg(repl_sec) AS avg_repl_time by peer_name
                  ","source=*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` (search_group=dmc_group_indexers OR search_group=dmc_group_search_head) source=*splunkd.log*
              (component=BundlesAdminHandler OR  component=BundleDataProcessor OR component=BundleDeltaHandler OR
              component=BundleReplicationProvider OR component=BundleStatusManager OR component=BundleTransaction OR
              component=CascadePlan OR component=CascadeReplicationReaper OR component=CascadingBundleReplicationProvider OR
              component=CascadingReplicationManager OR component=CascadingReplicationTransaction OR 
              component=CascadingReplicationStatusActor OR component=CascadingUploadHandler OR component=ClassicBundleReplicationProvider OR
              component=DistBundleRestHandler OR component=DistributedBundleReplicationManager OR
              component=GetCascadingReplicationStatusTransaction OR component=RFSManager OR component=RFSBundleReplicationProvider)
              (log_level=WARN OR log_level=ERROR)
              | timechart bins=200 partial=f count by component
            ","source=*splunkd.log*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` (search_group=dmc_group_indexers OR search_group=dmc_group_search_head) source=*splunkd.log*
              (component=BundlesAdminHandler OR  component=BundleDataProcessor OR component=BundleDeltaHandler  OR
              component=BundleReplicationProvider OR component=BundleStatusManager OR component=BundleTransaction OR
              component=CascadePlan OR component=CascadeReplicationReaper OR component=CascadingBundleReplicationProvider OR component=CascadingReplicationManager OR component=CascadingReplicationTransaction OR component=CascadingReplicationStatusActor OR component=CascadingUploadHandler OR component=ClassicBundleReplicationProvider OR component=DistBundleRestHandler OR
              component=DistributedBundleReplicationManager OR component=GetCascadingReplicationStatusTransaction OR
              component=RFSManager OR component=RFSBundleReplicationProvider)
              (log_level=WARN OR log_level=ERROR)
              | chart count by component, log_level
              | sort - ERROR, WARN
            ","source=*splunkd.log*",,"bundle_replication"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | where search_group=""$role$""
        ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/search/distributed/bundle/replication/config
          ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | fields replicationPolicy
              | eval replicationPolicy = upper(substr(replicationPolicy,1,1)).substr(replicationPolicy,2)
            ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              | fields replicationThreads
            ",,,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by num_levels
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by num_peers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_metadata
               | dedup planid
               | stats count by endpoint
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_replication
               | dedup planid
               | stats count by num_receivers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | stats count by num_receivers
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
               | multisearch
                 [search `dmc_set_index_internal` source=*metrics.log group=cascading name=plan_replication]
                 [search `dmc_set_index_internal` source=*metrics.log group=cascading name=payload_replication]
               | chart count over name by status
               | replace payload_replication WITH payload
               | replace plan_replication WITH plan
               | rename name as ""Replication Type"" 
               | sort -""Replication Type""
            ","source=*metrics.log
source=*metrics.log",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=cascading name=plan_replication
               | dedup planid
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_ms) AS avg_plan_msec
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` search_group=dmc_group_indexer source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | eval dispatch_time_sec=dispatch_time_ms/1000
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_sec) AS avg_payload_sec
            ","source=*",,"cascading_replication"
tbd,,,,,,"splunk_monitoring_console",,"
              `dmc_set_index_internal` search_group=dmc_group_indexer source=*/metrics.log* group=cascading name=payload_replication
               | dedup planid
               | eval dispatch_time_sec=dispatch_time_ms/1000
               | `dmc_timechart_for_metrics_log` avg(dispatch_time_sec) by peer_guid
            ","source=*",,"cascading_replication"
tbd,,,,,,"lame_training",,"| makeresults 
| eval a=""$ip1$""
| eval b=""$ip2$""
| eval a_port=$ip1_port$
| eval b_port=$ip2_port$
| eval proto=""$proto$""
| cid proto b, a, b_port, a_port
| table a, b, a_port, b_port proto cid",,,"community_id_generator"
tbd,,,,,,"lame_inventory_creator",,"| rest servicesNS/-/-/saved/eventtypes
| search id = ""*nt_index""
| table title, search
| rename title as ""Eventtype Title""
| rename search as Value",,,"create_inventory_tutorial"
tbd,,,,,,"lame_inventory_creator",,"| rest servicesNS/-/-/saved/eventtypes
| search id = ""*nt_sourcetypes""
| table title, search
| rename title as ""Eventtype Title""
| rename search as Value",,,"create_inventory_tutorial"
tbd,,,,,,"lame_inventory_creator",,"| tstats values(sourcetype) as sourcetype from datamodel=Network_Traffic groupby All_Traffic.src_ip, All_Traffic.dest_ip | head 5",,,"create_inventory_tutorial"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | search search_group=dmc_group_indexer
        ",,,"data_quality"
tbd,,,"index=*
index=_*",,,"splunk_monitoring_console",,"
      (index=* OR index=_*) host=$host$ source=$source$ sourcetype=$sourcetype$ _index_earliest=$time.earliest$ _index_latest=$latest_time$
      | search splunk_server=*
      | eval event_size = len(_raw)
      | eval time_disparity = (_indextime - _time) / 36000
      | stats count by linecount, event_size, time_disparity
    ",,,"data_quality"
tbd,,"host = if",,,,"splunk_monitoring_console",,"
            <![CDATA[
            `dmc_set_index_internal` search_group=dmc_group_indexer splunk_server=$splunk_server$ source=*splunkd.log*
            (component=AggregatorMiningProcessor OR component=LineBreakingProcessor OR component=DateParserVerbose OR component=MetricSchemaProcessor OR component=MetricsProcessor)
            (log_level=WARN OR log_level=ERROR)
            | rex field=event_message ""Context: source(::|=)(?<context_source>[^\|]*?)\|host(::|=)(?<context_host>[^\|]*?)\|(?<context_sourcetype>[^\|]*?)\|""
            | eval data_source = if(isnull(data_source) AND isnotnull(context_source), context_source, data_source)
            | eval data_host = if(isnull(data_host) AND isnotnull(context_host), context_host, data_host)
            | eval data_sourcetype = if(isnull(data_sourcetype) AND isnotnull(context_sourcetype), context_sourcetype, data_sourcetype)
            | stats
              count(eval(component==""LineBreakingProcessor"" OR component==""DateParserVerbose"" OR component==""AggregatorMiningProcessor"" OR component==""MetricSchemaProcessor"" OR component==""MetricsProcessor"")) as total_issues
              dc(data_host) AS ""Host Count""
              dc(data_source) AS ""Source Count""
              count(eval(component==""LineBreakingProcessor"")) AS ""Line Breaking Issues""
              count(eval(component==""DateParserVerbose"")) AS ""Timestamp Parsing Issues""
              count(eval(component==""AggregatorMiningProcessor"")) AS ""Aggregation Issues""
              count(eval(component==""MetricSchemaProcessor"")) AS ""Metric Schema Issues""
              count(eval(component==""MetricsProcessor"")) AS ""Metrics Issues"" by data_sourcetype
            | sort - total_issues
            | rename
              data_sourcetype as Sourcetype
              total_issues as ""Total Issues""
            ]]>
          ","source=*splunkd.log*
source = if","sourcetype = if","data_quality"
tbd,,"host = if",,,,"splunk_monitoring_console",,"
            <![CDATA[
            `dmc_set_index_internal` search_group=dmc_group_indexer splunk_server=$splunk_server$ source=*splunkd.log*
            (component=AggregatorMiningProcessor OR component=LineBreakingProcessor OR component=DateParserVerbose OR component=MetricSchemaProcessor OR component=MetricsProcessor)
            (log_level=WARN OR log_level=ERROR)
            | rex field=event_message ""Context: source(::|=)(?<context_source>[^\|]*?)\|host(::|=)(?<context_host>[^\|]*?)\|(?<context_sourcetype>[^\|]*?)\|""
            | eval data_source = if(isnull(data_source) AND isnotnull(context_source), context_source, data_source)
            | eval data_host = if(isnull(data_host) AND isnotnull(context_host), context_host, data_host)
            | eval data_sourcetype = if(isnull(data_sourcetype) AND isnotnull(context_sourcetype), context_sourcetype, data_sourcetype)
            | search data_sourcetype=$sourcetype$
            | stats
              count(eval(component==""LineBreakingProcessor"" OR component==""DateParserVerbose"" OR component==""AggregatorMiningProcessor"" OR component==""MetricSchemaProcessor"" OR component=""MetricsProcessor"")) as total_issues
              count(eval(component==""LineBreakingProcessor"")) AS ""Line Breaking Issues""
              count(eval(component==""DateParserVerbose"")) AS ""Timestamp Parsing Issues""
              count(eval(component==""AggregatorMiningProcessor"")) AS ""Aggregation Issues""
              count(eval(component==""MetricSchemaProcessor"")) AS ""Metric Schema Issues""
              count(eval(component==""MetricsProcessor"")) AS ""Metrics Issues"" by data_host, data_source
            | sort - total_issues
            | rename
              data_host as Host
              data_source as Source
              total_issues as ""Total Issues""
            ]]>
          ","source=*splunkd.log*
source = if","sourcetype = if","data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats sum(count) as ""Event Count"" by linecount
            | rename linecount as ""Line Count""
          ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | bin event_size span=log2
            | stats sum(count) as ""Event Count"" by event_size
            | rename event_size as ""Event Size (bytes)""
            ",,,"data_quality"
tbd,,,,,,"splunk_monitoring_console",,"
            | bin time_disparity span=1 start=-24 end=-24
            | stats sum(count) as ""Event Count"" by time_disparity
            | rename time_disparity as ""Observed Latency (hours)""
          ",,,"data_quality"
tbd,,,,,,"Splunk_SA_CIM",,"sort 18 - size | table datamodel,size(MB)",,,"datamodel_audit"
tbd,,,,,,"Splunk_SA_CIM",,"sort 18 - runDuration | table datamodel,runDuration",,,"datamodel_audit"
tbd,,,,,,"Splunk_SA_CIM",,"sort 100 + datamodel | fieldformat earliest=strftime(earliest, ""%m/%d/%Y %H:%M:%S"") | fieldformat latest=strftime(latest, ""%m/%d/%Y %H:%M:%S"") | table datamodel,app,cron,retention(days),earliest,latest,is_inprogress,complete(%),size(MB),runDuration(s),last_error",,,"datamodel_audit"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role | h$)` | search search_group!=""dmc_group_*""",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/search/distributed/peers
          | join type=outer title, splunk_server [
          | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/distributed
          | where title != ""per_searchhead_metrics"" AND title != ""window_metrics""
          ]
          | fields status, health_status, splunk_server, peerName
        ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            where status!=""Up""
            | stats dc(splunk_server) as down_count, values(splunk_server) as splunk_server
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            where health_status!=""Healthy""
            | stats dc(splunk_server) as unhealthy_count, values(splunk_server) as splunk_server
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/distributed/window_metrics
            | eval avg_baseline_mb = if(isNotNull(average_baseline_file_size), round(average_baseline_file_size /1024 / 1024, 2), ""Unknown"")
            | eval avg_sec = round(average_msecs / 1000, 2)
            | eval bundle_file_count = if(isNotNull(bundle_file_count), bundle_file_count, count)
            | fields splunk_server, bundle_file_count, avg_baseline_mb, avg_sec
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/search/distributed/peers
              | stats count as peer_count by splunk_server
            ]
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/dispatch/Bundle_Directory_Reaper
              | eval Bundle_Directory_Reaper_Average_Time(ms) = round('Bundle_Directory_Reaper_Average_Time(ms)')
              | eval Bundle_Directory_Reaper_Max_Time(ms) = round('Bundle_Directory_Reaper_Max_Time(ms)')
              | fields splunk_server, ""Bundle_Directory_Reaper_Average_Time(ms)"", ""Bundle_Directory_Reaper_Max_Time(ms)""
            ]
            | join type=outer splunk_server [
              | rest splunk_server_group=$role | h$ splunk_server_group=""$group$"" /services/server/introspection/search/dispatch/Dispatch_Directory_Reaper
              | eval Dispatch_Directory_Reaper_Average_Time(ms) = round('Dispatch_Directory_Reaper_Average_Time(ms)')
              | eval Dispatch_Directory_Reaper_Max_Time(ms) = round('Dispatch_Directory_Reaper_Max_Time(ms)')
              | fields splunk_server, ""Dispatch_Directory_Reaper_Average_Time(ms)"", ""Dispatch_Directory_Reaper_Max_Time(ms)""
            ]
            | $instance_fields_filter$
            | rename splunk_server as Instance, peer_count as ""Peer Count"", bundle_file_count as ""Knowledge Bundle Replication Count"", avg_baseline_mb as ""Average Size of Baseline Knowledge Bundle (MB)"", avg_sec as ""Average Time Spent on Knowledge Bundle Replication (sec)"", ""Bundle_Directory_Reaper_Average_Time(ms)"" as ""Average Time to Reap Knowledge Bundle Directory (ms)"", ""Bundle_Directory_Reaper_Max_Time(ms)"" as ""Max Time to Reap Knowledge Bundle Directory (ms)"", ""Dispatch_Directory_Reaper_Average_Time(ms)"" as ""Average Time to Reap Dispatch Directory (ms)"", ""Dispatch_Directory_Reaper_Max_Time(ms)"" as ""Max Time to Reap Dispatch Directory (ms)""
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat`
              avg(get_auth_$peer_heartbeat_no_split_agg$_ms) as get_auth_$peer_heartbeat_no_split_agg$_ms
              avg(get_bundleList_$peer_heartbeat_no_split_agg$_ms) as get_bundleList_$peer_heartbeat_no_split_agg$_ms
              avg(get_serverInfo_$peer_heartbeat_no_split_agg$_ms) as get_serverInfo_$peer_heartbeat_no_split_agg$_ms
            | eval get_auth_$peer_heartbeat_no_split_agg$_ms = round(get_auth_$peer_heartbeat_no_split_agg$_ms)
            | eval get_bundleList_$peer_heartbeat_no_split_agg$_ms = round(get_bundleList_$peer_heartbeat_no_split_agg$_ms)
            | eval get_serverInfo_$peer_heartbeat_no_split_agg$_ms = round(get_serverInfo_$peer_heartbeat_no_split_agg$_ms)
            | rename get_auth_$peer_heartbeat_no_split_agg$_ms as ""Get Authentication"",
                get_bundleList_$peer_heartbeat_no_split_agg$_ms as ""Get Bundle List"",
                get_serverInfo_$peer_heartbeat_no_split_agg$_ms as ""Get Peer Info""
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | where Peer_Count > $peer_count_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_count_agg$(Peer_Count) as peer_count by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat search_group=$role | h$ search_group=""$group$""
            | where get_$peer_heartbeat_metric$_$peer_heartbeat_split_agg$_ms > $peer_heartbeat_metric_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_heartbeat_split_agg$(get_$peer_heartbeat_metric$_$peer_heartbeat_split_agg$_ms) as metric by host
            | eval metric = round(metric)
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
          | `dmc_timechart_for_metrics_log` sum(total_count) as total_count,
            sum(success_count) as success_count, sum(failure_count) as failure_count,
            sum(baseline_count) as baseline_count, sum(delta_count) as delta_count,
            sum(already_present_count) as already_present_count, sum(unattempted_count) as unattempted_count,
            sum(total_msec_spent), as total_msec_spent,
            sum(baseline_msec_spent) as baseline_msec_spent, sum(delta_msec_spent) as delta_msec_spent,
            sum(total_bytes) as total_bytes,
            sum(baseline_bytes) as baseline_bytes, sum(delta_bytes) as delta_bytes
        ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, baseline_count, delta_count
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_size_mb = round(baseline_bytes / 1024 / 1024, 2)
            | eval delta_size_mb = round(delta_bytes / 1024 / 1024, 2)
            | fields _time, baseline_size_mb, delta_size_mb
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_sec_spent = round(baseline_msec_spent / 1000, 2)
            | eval delta_sec_spent = round(delta_msec_spent / 1000, 2)
            | fields _time, baseline_sec_spent, delta_sec_spent
          ",,,"distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_count) as count by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | eval $bundle_rep_mode$_mb = round($bundle_rep_mode$_bytes / 1024 / 1024, 2)
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_mb) as size by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ search_group=$role | h$ search_group=""$group$""
            | eval $bundle_rep_mode$_sec_spent = round($bundle_rep_mode$_msec_spent / 1000, 2)
            | `dmc_timechart_for_metrics_log` sum($bundle_rep_mode$_sec_spent) as msec_spent by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(dispatch_dir_reaper_mean_ms) as dispatch_dir_reaper_mean_ms, max(dispatch_dir_reaper_max_ms) as dispatch_dir_reaper_max_ms
            | eval dispatch_dir_reaper_mean_ms = round(dispatch_dir_reaper_mean_ms)
            | eval dispatch_dir_reaper_max_ms = round(dispatch_dir_reaper_max_ms)
            | rename dispatch_dir_reaper_mean_ms as Average, dispatch_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper search_group=$role | h$ search_group=""$group$""
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(bundle_dir_reaper_mean_ms) as bundle_dir_reaper_mean_ms, max(bundle_dir_reaper_max_ms) as bundle_dir_reaper_max_ms
            | eval bundle_dir_reaper_mean_ms = round(bundle_dir_reaper_mean_ms)
            | eval bundle_dir_reaper_max_ms = round(bundle_dir_reaper_max_ms)
            | rename bundle_dir_reaper_mean_ms as Average, bundle_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper search_group=$role | h$ search_group=""$group$""
            | where dispatch_dir_reaper_$reaper_agg$_ms > $dispatch_dir_filter$
            | eval dispatch_dir_reaper_$reaper_agg$_ms = round(dispatch_dir_reaper_$reaper_agg$_ms)
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $reaper_agg$(dispatch_dir_reaper_$reaper_agg$_ms) as dispatch_dir_reaper_ms by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper search_group=$role | h$ search_group=""$group$""
            | where bundle_dir_reaper_$reaper_agg$_ms > $bundle_dir_filter$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` $reaper_agg$(bundle_dir_reaper_$reaper_agg$_ms) as bundle_dir_reaper_$reaper_agg$_ms by host
          ",,"sourcetype=splunkd","distributed_search_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_instance_info($dmc_group$)`
          | where search_group=""$role$""
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/search/distributed/peers
      | join type=outer title [
        | rest splunk_server=$splunk_server$ /services/server/introspection/search/distributed
        | where title != ""per_searchhead_metrics"" AND title != ""window_metrics""
        | fields title, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures
        | eval get_auth_max_ms = round(get_auth_max_ms, 0)
        | eval get_auth_mean_ms = round(get_auth_mean_ms, 0)
        | eval get_bundleList_max_ms = round(get_bundleList_max_ms, 0)
        | eval get_bundleList_mean_ms = round(get_bundleList_mean_ms, 0)
        | eval get_serverInfo_max_ms = round(get_serverInfo_max_ms, 0)
        | eval get_serverInfo_mean_ms = round(get_serverInfo_mean_ms, 0)
      ]
    ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_internal` sourcetype=splunkd group=$bundles_group$ host=$host$
      | `dmc_timechart_for_metrics_log` sum(total_count) as total_count, sum(success_count) as success_count, sum(failure_count) as failure_count, sum(baseline_count) as baseline_count, sum(delta_count) as delta_count, sum(already_present_count) as already_present_count, sum(unattempted_count) as unattempted_count, sum(total_msec_spent), as total_msec_spent, sum(baseline_msec_spent) as baseline_msec_spent, sum(delta_msec_spent) as delta_msec_spent, sum(total_bytes) as total_bytes, sum(baseline_bytes) as baseline_bytes, sum(delta_bytes) as delta_bytes
    ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where status!=""Up""
            | stats count as down_count, values(peerName) as peer_name
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where health_status!=""Healthy""
            | stats count as unhealthy_count, values(peerName) as peer_name
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval mem_gb = round(physicalMemoryMB / 1024, 2)
            | eval disabled = if(disabled == 1, ""yes"", ""no"")
            | `dmc_time_format(startup_time)`
            | fields peerName, status, status_details, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures, replicationStatus, startup_time, disabled
            | rename peerName as Peer, status as Status, status_details as ""Status Details"", get_auth_max_ms as ""Authentication Max Time (ms)"", get_auth_mean_ms as ""Authentication Mean Time (ms)"", get_bundleList_max_ms as ""Get Bundle List Max Time (ms)"", get_bundleList_mean_ms as ""Get Bundle List Mean Time (ms)"", get_serverInfo_max_ms as ""Get Peer Info Max Time (ms)"", get_serverInfo_mean_ms as ""Get Peer Info Mean Time (ms)"", health_status as ""Health Condition"", health_check_failures as ""Health Details"", replicationStatus as ""Replication Status"", startup_time as ""Startup Time"", disabled as Disabled, version as Version, numberOfCores as ""CPU Cores"", mem_gb as ""Physical Memory (GB)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search peerName = $peer_drilldown$
            | fields - _timediff, peerName, status, status_details, get_auth_max_ms, get_auth_mean_ms, get_bundleList_max_ms, get_bundleList_mean_ms, get_serverInfo_max_ms, get_serverInfo_mean_ms, health_status, health_check_failures, replicationStatus, startup_time, disabled, server_roles, peerType, title, author, published, updated, remote_session, splunk_server, eai*
            | transpose
            | rename column as Property, ""row 1"" as Value
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/distributed/window_metrics
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval bundle_file_count = if(isNotNull(bundle_file_count), bundle_file_count, count)
            | fields bundle_file_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval avg_mb = if(isNotNull(average_baseline_file_size), round(average_baseline_file_size /1024 / 1024, 2), ""Unknown"")
            | fields avg_mb
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval avg_sec = average_msecs / 1000
            | fields avg_sec
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/dispatch/Bundle_Directory_Reaper
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Bundle_Directory_Reaper_Average_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Bundle_Directory_Reaper_Max_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/introspection/search/dispatch/Dispatch_Directory_Reaper
        ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Dispatch_Directory_Reaper_Average_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields ""Dispatch_Directory_Reaper_Max_Time(ms)""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=distributed_peer_heartbeat host=$host$
          | `dmc_timechart_for_metrics_reaper_and_heartbeat` $peer_heartbeat_agg$(Peer_Count) as peer_count,
              $peer_heartbeat_agg$(get_auth_$peer_heartbeat_agg$_ms) as get_auth_$peer_heartbeat_agg$_ms,
              $peer_heartbeat_agg$(get_bundleList_$peer_heartbeat_agg$_ms) as get_bundleList_$peer_heartbeat_agg$_ms,
              $peer_heartbeat_agg$(get_serverInfo_$peer_heartbeat_agg$_ms) as get_serverInfo_$peer_heartbeat_agg$_ms,
          | eval get_auth_$peer_heartbeat_agg$_ms = round(get_auth_$peer_heartbeat_agg$_ms)
          | eval get_bundleList_$peer_heartbeat_agg$_ms = round(get_bundleList_$peer_heartbeat_agg$_ms)
          | eval get_serverInfo_$peer_heartbeat_agg$_ms = round(get_serverInfo_$peer_heartbeat_agg$_ms)
        ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, peer_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, get_auth_$peer_heartbeat_agg$_ms, get_bundleList_$peer_heartbeat_agg$_ms, get_serverInfo_$peer_heartbeat_agg$_ms
            | rename get_auth_$peer_heartbeat_agg$_ms as ""Get Authentication"",
                get_bundleList_$peer_heartbeat_agg$_ms as ""Get Bundle List"",
                get_serverInfo_$peer_heartbeat_agg$_ms as ""Get Peer Info""
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields _time, baseline_count, delta_count
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_size_mb = round(baseline_bytes / 1024 / 1024, 2)
            | eval delta_size_mb = round(delta_bytes / 1024 / 1024, 2)
            | fields _time, baseline_size_mb, delta_size_mb
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval baseline_sec_spent = round(baseline_msec_spent / 1000, 2)
            | eval delta_sec_spent = round(delta_msec_spent / 1000, 2)
            | fields _time, baseline_sec_spent, delta_sec_spent
          ",,,"distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=dispatch_directory_reaper host=$host$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(dispatch_dir_reaper_mean_ms) as dispatch_dir_reaper_mean_ms, max(dispatch_dir_reaper_max_ms) as dispatch_dir_reaper_max_ms
            | eval dispatch_dir_reaper_mean_ms = round(dispatch_dir_reaper_mean_ms)
            | eval dispatch_dir_reaper_max_ms = round(dispatch_dir_reaper_max_ms)
            | rename dispatch_dir_reaper_mean_ms as Average, dispatch_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=search_health_metrics name=bundle_directory_reaper host=$host$
            | `dmc_timechart_for_metrics_reaper_and_heartbeat` avg(bundle_dir_reaper_mean_ms) as bundle_dir_reaper_mean_ms, max(bundle_dir_reaper_max_ms) as bundle_dir_reaper_max_ms
            | eval bundle_dir_reaper_mean_ms = round(bundle_dir_reaper_mean_ms)
            | eval bundle_dir_reaper_max_ms = round(bundle_dir_reaper_max_ms)
            | rename bundle_dir_reaper_mean_ms as Average, bundle_dir_reaper_max_ms as Max
          ",,"sourcetype=splunkd","distributed_search_instance"
tbd,,,,"inputlookup analytics_info",,"lame_analytic_documentation","outputlookup analytics_info","| inputlookup analytics_info | search _key = $the_key$
| eval description = ""$description$""
| outputlookup analytics_info append=true",,,"document_analytics"
tbd,,,,"inputlookup index_info",,"lame_analytic_documentation","outputlookup index_info","| inputlookup index_info | search _key = $the_key$
| eval description = ""description""
| outputlookup index_info append=true",,,"document_indexes"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_analytics"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_analytics"
tbd,,,,,,"lame_training","outputlookup Sourcetype_Analytics","| makeresults
| eval sourcetype = ""$src_type$""
| eval metric_description = ""$q_description$""
| eval metric_query = ""$query$""
| table sourcetype, metric_description, metric_query
| where metric_description != ""Change Me"" 
| outputlookup Sourcetype_Analytics append=true",,,"document_sourcetype_analytics"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training",,"index=$idx$ sourcetype=$src_type$ | head 100
| fieldsummary | fields field",,,"document_sourcetype_fields"
tbd,,,,,,"lame_training","outputlookup SourcetypeInfo","| makeresults
| eval foo = ""bar""
| eval sourcetype = ""$src_type$""
| eval rationale = ""$rationale$""
| eval fieldname = ""$fld$""
| eval fieldvalue = ""$fldvalue$""
| table foo  sourcetype, rationale, fieldname, fieldvalue
| where rationale != ""Change Me""

| outputlookup SourcetypeInfo append=true",,,"document_sourcetype_fields"
tbd,,,,"inputlookup sourcetype_info",,"lame_analytic_documentation","outputlookup sourcetype_info","| inputlookup sourcetype_info | search _key = $the_key$
| eval description = ""description""
| outputlookup sourcetype_info append=true",,,"document_sourcetypes"
tbd,,,,"inputlookup source_info",,"lame_analytic_documentation","outputlookup source_info","| inputlookup source_info | search _key = $the_key$
| eval description = ""description""
| outputlookup source_info append=true",,,"document_summary_indexes"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" | table level | dedup level","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/eventgen_metrics.log"" metric_type=""volume_sent""
            | fields volume_bytes
            | timechart per_second(eval(volume_bytes/1073741824)) AS volume_gigabytes","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/eventgen_metrics.log"" metric_type=""volume_sent"" metric_type=""volume_sent""
            | fields volume_bytes
            | timechart per_day(eval(volume_bytes/1073741824)) AS volume_gigabytes","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/eventgen_metrics.log"" metric_type=""events_sent""
            | fields event_count
            | timechart per_second(event_count) AS events_per_second","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" event_type=timer_startup | dedup sample_name | eval sample_per_day_volume_GB=sample_per_day_volume | table sample_name sample_count sample_interval sample_per_day_volume_GB","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" (""http bottleneck"" OR ""generator bottleneck"") | eval http_bottleneck=if(msg=""http bottleneck"", 1, http_bottleneck) | eval generator_bottleneck=if(match(msg, ""Generator\ bottleneck.*""), 1, generator_bottleneck) | timechart count(http_bottleneck) AS total_http_bottlenecks count(generator_bottleneck) AS total_generator_bottlenecks","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" event_type=startup | table generator_workers_count","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" event_type=startup | table output_workers_count","source=*",,"eventgen_status"
tbd,,,"index=_internal",,,"SA-Eventgen",,"index=""_internal"" source=""*/var/log/splunk/modinput_eventgen.log"" | fillnull value=unknown sub_group  |  search sub_group!=""metric"" | table time level pod msg _raw","source=*",,"eventgen_status"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
| inputlookup dmc_forwarder_assets
| makemv delim="" "" avg_tcp_kbps_sparkline
| eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
| eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
| eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
| eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
| `dmc_rename_forwarder_type(forwarder_type)`
| `dmc_time_format(last_connected)`
    ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
| search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
    ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/saved/searches/DMC%20Forwarder%20-%20Build%20Asset%20Table/history
      | eval endtime = strptime(updated, ""%Y-%m-%dT%H:%M:%S"")
      | sort 1 -endtime
      | fields updated
      | rename updated AS last_run_time
      | eval last_run_time = if(isnotnull(last_run_time), "" - As of "".replace(last_run_time, ""T"", "" ""), "" "")
    ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as ""count"" by $forwarderCountSplitBy$",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as count",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as ""count"" by $forwarderCountSplitBy$",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(guid) as count",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
| $forwarderNameFilter$
| $forwarderStatusFilter$
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
 search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
| fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
| $forwarderNameFilter$
| $forwarderStatusFilter$
| rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_get_forwarder_tcpin`
          | eval forwarder = guid
          | eval receiver = host."":"".destPort
          | stats dc(forwarder) as count_forwarder, dc(receiver) as count_receiver
        ",,,"forwarder_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
          `dmc_get_forwarder_tcpin`
           | eval forwarder = guid
           | eval receiver = host."":"".destPort
           | search NOT [| inputlookup dmc_assets | dedup serverName | rename serverName as hostname | fields hostname]
           | stats dc(forwarder) as count_forwarder, dc(receiver) as count_receiver
        ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_forwarder",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_forwarder",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"fields count_receiver",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin`
| eval connection = hostname."":"".sourcePort.""->"".host."":"".destPort
| eval event_count = tcp_eps * 30
| `dmc_timechart_for_metrics_log` dc(connection) as connection_count, per_second($funcForwarderCountOverlay$) as $funcForwarderCountOverlay$
| rename connection_count as ""Connections"", $funcForwarderCountOverlay$ as $forwarderCountOverlayLabel$
          ",,,"forwarder_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| eval source_uri = hostname."":"".sourcePort
| eval dest_uri = host."":"".destPort
| eval connection = source_uri.""->"".dest_uri
| stats values(fwdType) as fwdType, values(sourceIp) as sourceIp, latest(version) as version,  values(os) as os, values(arch) as arch, dc(dest_uri) as dest_count, dc(connection) as connection_count, avg(tcp_KBps) as avg_tcp_kbps, avg(tcp_eps) as avg_tcp_eps by hostname, guid
| eval avg_tcp_kbps = round(avg_tcp_kbps, 2)
| eval avg_tcp_eps = round(avg_tcp_eps, 2)
| `dmc_rename_forwarder_type(fwdType)`
| rename hostname as Instance, fwdType as ""Forwarder Type"", sourceIp as IP, version as ""Splunk Version"", os as OS, arch as Architecture, guid as GUID, dest_count as ""Receiver Count"", connection_count as ""Connection Count"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"forwarder_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| eval dest_uri = host."": "".destPort
| stats count by dest_uri
| fields dest_uri
| rename dest_uri as ""Receivers""
          ",,,"forwarder_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_forwarder_tcpin` hostname=$hostname$
| `dmc_timechart_for_metrics_log` $funcVolume$(eval(tcp_KBps)) as ""KB/s"", $funcVolume$(tcp_eps) as ""Events/s""
          ",,,"forwarder_instance"
tbd,,,,,,"lame_training",,"| rest services/data/models
| stats count by title",,,"help_document_datamodel_analytics"
tbd,,,,"inputlookup Datamodel_Analytics.csv",,"lame_training",,"| inputlookup Datamodel_Analytics.csv
| search datamodel=""$dm$""
| table metric_description, metric_query",,,"help_document_datamodel_analytics"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | stats count by index | fields index | sort index",,,"help_document_sourcetype_analytics"
tbd,,,"index=*","inputlookup base_analytics",,"lame_training",,"| inputlookup base_analytics
| makemv delim="","" index
| makemv delim="","" sourcetype
| search index=*
| stats count by sourcetype",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup SourcetypeInfo",,"lame_training",,"| inputlookup SourcetypeInfo
| search sourcetype=$st$
| table fieldname, rationale",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup Sourcetype_Analytics",,"lame_training",,"| inputlookup Sourcetype_Analytics
| search sourcetype=$st$
| table metric_description, metric_query",,,"help_document_sourcetype_analytics"
tbd,,,,"inputlookup autonumberAnalytics.csv","lookup analyticsV2.csv","lame_training",,"| inputlookup autonumberAnalytics.csv
| makemv delim="","" index
| makemv delim="","" sourcetype
| search index=$idx$ sourcetype=$st$
| lookup analyticsV2.csv BaseAnalytic as BaseAnalytic Output queries, myapp
| stats values(myapp) as app, values(BaseAnalytic) as BaseAnalytic count by queries
| fields - count",,,"help_document_sourcetype_analytics"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
      | timechart minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
      | eval data_received=data_received/pow(1024, 2) 
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval valid_requests_total = requests_total
    ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
      | bin _time minspan=1m 
      | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time
      | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
      | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval data_received=data_received/pow(1024, 2) 
      | eval valid_requests_total = requests_total
      | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
    ",,,"http_event_collector_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
      | bin _time minspan=1m 
      | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time, host
      | join host [ 
      | inputlookup dmc_assets 
      | mvcombine search_group 
      | search search_group=$role$ OR search_group=""$group$"" ]
      | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
      | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
      | eval data_indexed=data_indexed/pow(1024, 2) 
      | eval data_received=data_received/pow(1024, 2) 
      | eval valid_requests_total = requests_total
      | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
      | stats sum(events_total) as events_total, sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received, sum(data_indexed) as data_indexed
      | appendcols [search `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ | stats count as event_count]
    ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
        | `dmc_get_groups_containing_role($role$)` 
        | search search_group!=""dmc_group_*""
      ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
        | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/data/inputs/http 
        | eval token_name=substr('title', 8)
      ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(events_total) as events_total by _time
          | stats latest(events_total) as event_latest 
          | eval event_throughput = event_latest/60 
          | fields event_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(valid_requests_total) as valid_requests_total by _time
          | stats latest(valid_requests_total) as valid_request_latest 
          | eval valid_requests_throughput=valid_request_latest/60 
          | fields valid_requests_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats sum(invalid_requests_total) as invalid_requests_total by _time
          | stats latest(invalid_requests_total) as invalid_request_latest 
          | eval invalid_requests_throughput=invalid_request_latest/60 
          | fields invalid_requests_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_received) as data_received_latest 
          | eval data_received_throughput = round(data_received_latest/60, 3) 
          | fields data_received_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_indexed) as data_indexed_latest 
          | eval data_indexed_throughput = round(data_indexed_latest/60, 3) 
          | fields data_indexed_throughput
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          timechart minspan=""1m"" sum(valid_requests_total) as ""Valid Requests"" sum(invalid_requests_total) as ""Invalid Requests"" sum(data_received) as ""Data Received""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" /services/data/inputs/http 
          | search title=""http://$token_name$"" 
          | dedup splunk_server                          
          | fields splunk_server                          
          | join type=left splunk_server [|`dmc_get_instance_roles` | rename serverName as splunk_server]                                              
          | fields host, splunk_server, role                          
          | join type=left splunk_server [| rest /services/server/status/resource-usage/hostwide
          | eval mem_used=round(mem_used / mem * 100, 0)  
          | eval cpu_usage=cpu_system_pct + cpu_user_pct
          | fields splunk_server, mem_used, cpu_usage]
          | eval port = if(isnotnull(port), port, 8088)                                 
          | fields host, cpu_usage, mem_used, role, port
          | rename host as ""Instance"", cpu_usage as ""CPU Usage (%)"", mem_used as ""Memory Usage (%)"", role as Role, port as ""HTTP Event Collector Port Number""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval events_total = if(event_count==0, 0, events_total)
          | fields events_total
          | rename events_total as ""Event Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval valid_requests_total = if(event_count==0, 0, valid_requests_total)
          | fields valid_requests_total
          | rename valid_requests_total as ""Valid Request Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval invalid_requests_total = if(event_count==0, 0, invalid_requests_total)
          | fields invalid_requests_total
          | rename invalid_requests_total as ""Invalid Request Count""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_received = if(event_count==0, 0, data_received)
          | fields data_received
          | rename data_received as ""Data Received""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_indexed = if(event_count==0, 0, data_indexed)
          | fields data_indexed
          | rename data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token"" $token_clause$      
          | bin _time minspan=1m                     
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, values(data.token_name) as token_name by _time
          | eval valid_requests_total = requests_total
          | timechart partial=f minspan=1m sum($event_type$) as $event_type$ by token_name
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_auth_failures, num_of_requests_to_disabled_token, num_of_requests_to_incorrect_url, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_requests_to_disabled_token, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token"" $token_clause$
          | bin _time minspan=1m
          | timechart partial=f minspan=1m sum(data.$invalid_request_reason$) as $invalid_request_reason$ by data.token_name
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""http_event_collector_token""
          | bin _time minspan=1m 
          | stats sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received, values(data.token_name) as token_name by _time
          | eval data_received=data_received/pow(1024, 2)
          | eval data_indexed=data_indexed/pow(1024, 2)
          | timechart partial=f minspan=1m sum($data_type$) by token_name
        ",,,"http_event_collector_deployment"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" search_group=$role$ search_group=""$group$"" data.series=""$data_series$"" $token_clause$
          | bin _time minspan=1m 
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_errors) as error_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time, host
          | join host [ 
          | inputlookup dmc_assets 
          | mvcombine search_group 
          | search search_group=$role$ OR search_group=""$group$"" ]
          | eval incorrect_url_total=if(isnull(incorrect_url_total), 0, incorrect_url_total) 
          | eval auth_fail_total=if(isnull(auth_fail_total), 0, auth_fail_total)
          | eval data_indexed=round(data_indexed/pow(1024, 2), 2)
          | eval data_received=round(data_received/pow(1024, 2), 2)
          | eval valid_requests_total = requests_total
          | eval invalid_requests_total = disabled_token_total + incorrect_url_total + auth_fail_total
          | stats sum(events_total) as events_total, sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received, sum(data_indexed) as data_indexed by host
          | fields host, events_total, valid_requests_total, invalid_requests_total, data_received, data_indexed
          | rename host as Instance, events_total as ""Event Count"", valid_requests_total as ""Valid Request Count"", invalid_requests_total as ""Invalid Request Count"", data_received as ""Data Received (MB)"", data_indexed as ""Data Indexed (MB)""
        ",,,"http_event_collector_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
    `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$
    | bin _time span=1m
    | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_requests_to_disabled_token) as disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as incorrect_url_total, sum(data.num_of_auth_failures) as auth_fail_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by _time
    | eval incorrect_url_total=if(isnotnull(incorrect_url_total), incorrect_url_total, 0)
    | eval auth_fail_total=if(isnotnull(auth_fail_total), auth_fail_total, 0)
    | eval data_indexed=data_indexed/pow(1024, 2)
    | eval data_received=data_received/pow(1024, 2)
    | eval valid_requests_total = requests_total
    | eval invalid_requests_total = auth_fail_total + disabled_token_total + incorrect_url_total
  ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
    `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$
    | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_parser_errors) as parser_errors_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
    | eval requests_to_incorrect_url_total = if(isnotnull(requests_to_incorrect_url_total), requests_to_incorrect_url_total, 0)
    | eval auth_failures_total = if(isnotnull(auth_failures_total), auth_failures_total, 0)
    | eval data_received=data_received/pow(1024, 2) 
    | eval data_indexed=data_indexed/pow(1024, 2)
    | appendcols [search `dmc_set_index_introspection` component=""HttpEventCollector"" data.series=""$data_series$"" host=""$host$"" $token_clause$ | stats count as event_count]
  ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
        | rest splunk_server=* /services/data/inputs/http 
        | dedup splunk_server 
        | fields splunk_server 
        | join type=left splunk_server [|`dmc_get_instance_roles` | rename serverName as splunk_server]
      ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/inputs/http 
      | eval token_name=substr('title', 8)
    ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(events_total) as event_latest
          | eval event_throughput = event_latest/60
          | fields event_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(valid_requests_total) as valid_request_latest
          | eval valid_requests_throughput=valid_request_latest/60
          | fields valid_requests_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(invalid_requests_total) as invalid_request_latest
          | eval invalid_requests_throughput=invalid_request_latest/60
          | fields invalid_requests_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_received) as data_received_latest
          | eval data_received_throughput = round(data_received_latest/60, 3)
          | fields data_received_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats latest(data_indexed) as data_indexed_latest
          | eval data_indexed_throughput = round('data_indexed_latest'/60, 3)
          | fields data_indexed_throughput
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          timechart minspan=""1m"" sum(valid_requests_total) as valid_requests_total, sum(invalid_requests_total) as invalid_requests_total, sum(data_received) as data_received
          | rename valid_requests_total as ""Valid Requests"", invalid_requests_total as ""Invalid Requests"", data_received as ""Data Received""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval events_total = if(event_count==0, 0, events_total)
          | fields events_total
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval valid_request_count_total = if(event_count==0, 0, requests_total)
          | fields valid_request_count_total 
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval invalid_request_count_total = if(event_count==0, 0, auth_failures_total + requests_to_disabled_token_total + requests_to_incorrect_url_total)
          | fields invalid_request_count_total
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_received = if(event_count==0, 0, data_received)
          | fields data_received
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          eval data_indexed = if(event_count==0, 0, data_indexed)
          | fields data_indexed
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $event_type$
          | rename events_total as ""Events"", valid_requests_total as ""Valid Requests""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token"" $token_clause$      
          | bin _time minspan=1m                     
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, values(data.token_name) as token_name by _time
          | eval valid_requests_total = requests_total
          | timechart partial=f minspan=1m sum($event_type$) as $event_type$ by token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_auth_failures, num_of_requests_to_disabled_token, num_of_requests_to_incorrect_url, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, num_of_requests_to_disabled_token, num_of_parser_errors
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_auth_failures) as num_of_auth_failures, sum(data.num_of_requests_to_disabled_token) as num_of_requests_to_disabled_token, sum(data.num_of_requests_to_incorrect_url) as num_of_requests_to_incorrect_url, sum(data.num_of_parser_errors) as num_of_parser_errors
          | fields _time, $invalid_request_reason$
          | rename num_of_auth_failures as ""Authentication Failures"", num_of_requests_to_disabled_token as ""Requests to Disabled Token"", num_of_requests_to_incorrect_url as ""Requests to Incorrect URL"", num_of_parser_errors as ""Parser Errors""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token"" $token_clause$ 
          | bin _time minspan=1m
          | timechart partial=f minspan=1m sum(data.$invalid_request_reason$) as $invalid_request_reason$ by data.token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""$data_series$"" $token_clause$ 
          | timechart partial=f minspan=1m sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_error_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as auth_failures_total, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url_total, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received
          | eval data_received=data_received/pow(1024, 2) 
          | eval data_indexed=data_indexed/pow(1024, 2) 
          | eval valid_requests_total = requests_total
          | fields _time, $data_type$
          | rename data_received as ""Data Received"", data_indexed as ""Data Indexed""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` component=""HttpEventCollector"" host=$host$ data.series=""http_event_collector_token""
          | bin _time minspan=1m 
          | stats sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received, values(data.token_name) as token_name by _time
          | eval data_received=data_received/pow(1024, 2)
          | eval data_indexed=data_indexed/pow(1024, 2)
          | timechart partial=f minspan=1m sum($data_type$) by token_name
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/data/inputs/http
          | eval enabled = if(disabled == 1, ""No"", ""Yes"")
          | eval Token=substr(title, 8)
          | fields Token, enabled
          | join type=""left"" Token [search http_event_collector_token `dmc_set_index_introspection` host=$host$
          | stats sum(data.num_of_events) as events_total, sum(data.num_of_requests) as requests_total, sum(data.num_of_parser_errors) as parser_errors_total, sum(data.num_of_requests_to_disabled_token) as requests_to_disabled_token_total, sum(data.num_of_auth_failures) as requests_with_auth_failures, sum(data.num_of_requests_to_incorrect_url) as requests_to_incorrect_url, sum(data.total_bytes_indexed) as data_indexed, sum(data.total_bytes_received) as data_received by data.token_name
          | eval data_indexed=data_indexed/pow(1024, 2)
          | eval data_received=data_received/pow(1024, 2)
          | rename ""Data Indexed"" as ""Data Indexed (MB)"" ""Data Received"" as ""Data Received (MB)""
          | eval valid_requests_total = requests_total
          | eval invalid_request_count_total = requests_to_disabled_token_total + requests_with_auth_failures + requests_to_incorrect_url
          | fields data.token_name, events_total, valid_requests_total, invalid_request_count_total, data_received, data_indexed
          | rename data.token_name as ""Token""]
          | eval events_total=if(isnull(events_total), 0, events_total)
          | eval invalid_request_count_total=if(isnull(invalid_request_count_total), 0, invalid_request_count_total)
          | eval valid_requests_total=if(isnull(valid_requests_total), 0, valid_requests_total)
          | eval data_received=if(isnull(data_received), 0, data_received)
          | eval data_indexed=if(isnull(data_indexed), 0, data_indexed)
          | fields Token, enabled, data_indexed, data_received, events_total, invalid_request_count_total, valid_requests_total
          | rename enabled as ""Enabled"", data_indexed as ""Data Indexed (MB)"", data_received as ""Data Received (MB)"", events_total as ""Event Count"", invalid_request_count_total as ""Invalid Request Count"", valid_requests_total as ""Valid Request Count""
        ",,,"http_event_collector_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes/$index$
            | join title splunk_server type=outer [| rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes-extended/$index$]
            | eval bucketCount = coalesce(total_bucket_count, 0)
            | eval eventCount = coalesce(totalEventCount, 0)
            | eval coldBucketSize = coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size', 0)
            | eval coldBucketSizeGB = round(coldBucketSize/ 1024, 2)
            | eval coldBucketMaxSizeGB = if(isnull('coldPath.maxDataSizeMB') OR 'coldPath.maxDataSizeMB' = 0, ""unlimited"", round('coldPath.maxDataSizeMB' / 1024, 2))
            | eval coldBucketUsageGB = coldBucketSizeGB."" / "".coldBucketMaxSizeGB
            | eval homeBucketSizeGB = coalesce(round((total_size - coldBucketSize) / 1024, 2), 0.00)
            | eval homeBucketMaxSizeGB = round('homePath.maxDataSizeMB' / 1024, 2)
            | eval homeBucketMaxSizeGB = if(homeBucketMaxSizeGB > 0, homeBucketMaxSizeGB, ""unlimited"")
            | eval homeBucketUsageGB = homeBucketSizeGB."" / "".homeBucketMaxSizeGB
            | eval dataAgeDays = coalesce(round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0), 0)
            | eval frozenTimePeriodDays = round(frozenTimePeriodInSecs / 86400, 0)
            | eval frozenTimePeriodDays = if(frozenTimePeriodDays > 0, frozenTimePeriodDays, ""unlimited"")
            | eval freezeRatioDays = dataAgeDays."" / "".frozenTimePeriodDays
            | eval indexSizeGB = if(currentDBSizeMB >= 1 AND totalEventCount >=1, round(currentDBSizeMB/1024, 2), 0.00)
            | eval maxTotalDataSizeGB = round(maxTotalDataSizeMB / 1024, 2)
            | eval indexMaxSizeGB = if(maxTotalDataSizeGB > 0, maxTotalDataSizeGB, ""unlimited"")
            | eval indexSizeUsageGB = indexSizeGB."" / "".indexMaxSizeGB
            | eval indexSizeUsagePerc = if(isNum(indexMaxSizeGB) AND (indexMaxSizeGB > 0), round(indexSizeGB / indexMaxSizeGB * 100, 2).""%"", ""N/A"")
            | eval total_raw_size = coalesce(total_raw_size, 0)
        ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    | `dmc_get_groups_containing_role(dmc_group_indexer)`
                    | where search_group!=""dmc_group_indexer""
                ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes $datatype$
                    | `dmc_exclude_indexes`
                    | stats count by title
                    | fields - count
                ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) as instances
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalIndexSizeGB
                        | eval totalIndexSizeGB = totalIndexSizeGB."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        search title=$index$
                        | stats sum(eval(total_raw_size / 1024)) as totalRawSizeGB
                        | eval totalRawSizeGB = round(totalRawSizeGB, 2)."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalIndexSizeGB
                        sum(eval(total_raw_size / 1024)) as totalRawSizeGB
                        | eval compressionRatio = round(totalRawSizeGB / totalIndexSizeGB, 2)."":1""
                        | fields compressionRatio
                        | eval compressionRatio = if(isnotnull(compressionRatio), compressionRatio, ""N/A"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(eventCount) as totalEventCount
                        | eval totalEventCount = toString(totalEventCount, ""commas"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats min(minTime) as oldestEvent
                        | eval oldestEvent = replace(oldestEvent, ""T"", "" "")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats max(maxTime) as newestEvent
                        | eval newestEvent = replace(newestEvent, ""T"", "" "")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(indexSizeGB) as avgIndexSizeGB
                        | eval avgIndexSizeGB = round(avgIndexSizeGB, 2)."" GB""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(eventCount) as avgEventCount
                        | eval avgEventCount = toString(floor(avgEventCount), ""commas"")
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        eval dataAgeDays = if(isnotnull(dataAgeDays), dataAgeDays, 0)
                        | stats median(dataAgeDays) as medianDataAge
                        | eval medianDataAge = medianDataAge."" days""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats avg(bucketCount) as avgBucketCount
                        | eval avgBucketCount = floor(avgBucketCount)
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        eval avgBucketSize = round(indexSizeGB / bucketCount, 2)
                        | fields splunk_server, freezeRatioDays, indexSizeUsageGB, homeBucketUsageGB, coldBucketUsageGB, eventCount, bucketCount, avgBucketSize
                        | rename splunk_server as ""Indexer""
                        freezeRatioDays as ""Data Age vs Frozen Age (days)""
                        indexSizeUsageGB as ""Index Usage (GB)""
                        homeBucketUsageGB as ""Home Path Usage (GB)""
                        coldBucketUsageGB as ""Cold Path Usage (GB)""
                        eventCount as ""Total Event Count""
                        bucketCount as ""Total Bucket Count""
                        avgBucketSize as ""Average Bucket Size (GB)""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=hosts index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields host, totalCount
                        | sort - totalCount
                        | rename host as Host, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=sources index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields source, totalCount
                        | sort - totalCount
                        | rename source as Source, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | metadata type=sourcetypes index=$index$ splunk_server_group=dmc_group_indexer splunk_server_group=""$group$""
                        | fields sourcetype, totalCount
                        | sort - totalCount
                        | rename sourcetype as Sourcetype, totalCount as ""Event Count""
                    ",,,"index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | `dmc_set_bin_for_disk_usage`
                        | stats latest(data.total_size) as totalSize by host _time
                        | `dmc_timechart_for_disk_usage` $funcDiskSizeUsage$(eval(totalSize /1024)) as ""Index Size""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval pctDiskUsage = 'data.total_size' / 'data.total_capacity'
                        | bin _time minspan=10min
                        | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
                        | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
                        | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
                        | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval pctDiskUsage = 'data.total_size' / 'data.total_capacity'
                        | bin _time minspan=10min
                        | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
                        | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
                        | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
                        | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
                        index=_introspection sourcetype=splunk_disk_objects component=Indexes data.name=$index$ search_group=dmc_group_indexer search_group=""$group$""
                        | eval data_birth_date = if(isnotnull('data.bucket_dirs.cold.event_min_time'), 'data.bucket_dirs.cold.event_min_time', 'data.bucket_dirs.home.event_min_time')
                        | eval data_age_days = round((_time - data_birth_date) / 86400, 0)
                        | `dmc_timechart_for_disk_usage` $funcDataAge$(data_age_days) as ""Data Age""
                    ",,"sourcetype=splunk_disk_objects","index_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/data/indexes $datatype$
  | join type=outer title [
    | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
  ]
| `dmc_exclude_indexes`
| eval warm_bucket_size = coalesce('bucket_dirs.home.warm_bucket_size', 'bucket_dirs.home.size')
| eval cold_bucket_size = coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
| eval hot_bucket_size = if(isnotnull(cold_bucket_size), total_size - cold_bucket_size - warm_bucket_size, total_size - warm_bucket_size)
| eval thawed_bucket_size = coalesce('bucket_dirs.thawed.bucket_size', 'bucket_dirs.thawed.size')
| eval warm_bucket_size_gb = coalesce(round(warm_bucket_size / 1024, 2), 0.00)
| eval hot_bucket_size_gb = coalesce(round(hot_bucket_size / 1024, 2), 0.00)
| eval cold_bucket_size_gb = coalesce(round(cold_bucket_size / 1024, 2), 0.00)
| eval thawed_bucket_size_gb = coalesce(round(thawed_bucket_size / 1024, 2), 0.00)

| eval warm_bucket_count = coalesce('bucket_dirs.home.warm_bucket_count', 0)
| eval hot_bucket_count = coalesce('bucket_dirs.home.hot_bucket_count', 0)
| eval cold_bucket_count = coalesce('bucket_dirs.cold.bucket_count', 0)
| eval thawed_bucket_count = coalesce('bucket_dirs.thawed.bucket_count', 0)
| eval home_event_count = coalesce('bucket_dirs.home.event_count', 0)
| eval cold_event_count = coalesce('bucket_dirs.cold.event_count', 0)
| eval thawed_event_count = coalesce('bucket_dirs.thawed.event_count', 0)

| eval home_bucket_size_gb = coalesce(round((warm_bucket_size + hot_bucket_size) / 1024, 2), 0.00)
| eval homeBucketMaxSizeGB = coalesce(round('homePath.maxDataSizeMB' / 1024, 2), 0.00)
| eval home_bucket_capacity_gb = if(homeBucketMaxSizeGB > 0, homeBucketMaxSizeGB, ""unlimited"")
| eval home_bucket_usage_gb = home_bucket_size_gb."" / "".home_bucket_capacity_gb
| eval cold_bucket_capacity_gb = coalesce(round('coldPath.maxDataSizeMB' / 1024, 2), 0.00)
| eval cold_bucket_capacity_gb = if(cold_bucket_capacity_gb > 0, cold_bucket_capacity_gb, ""unlimited"")
| eval cold_bucket_usage_gb = cold_bucket_size_gb."" / "".cold_bucket_capacity_gb

| eval currentDBSizeGB = round(currentDBSizeMB / 1024, 2)
| eval maxTotalDataSizeGB = if((maxTotalDataSizeMB > 0) AND isNull(remotePath), round(maxTotalDataSizeMB / 1024, 2), ""unlimited"")
| eval disk_usage_gb = currentDBSizeGB."" / "".maxTotalDataSizeGB

| eval currentTimePeriodDay = coalesce(round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0), 0)
| eval frozenTimePeriodDay = coalesce(round(frozenTimePeriodInSecs / 86400, 0), 0)
| eval frozenTimePeriodDay = if(frozenTimePeriodDay > 0, frozenTimePeriodDay, ""unlimited"")
| eval freeze_period_viz_day = currentTimePeriodDay."" / "".frozenTimePeriodDay

| eval total_bucket_count = toString(coalesce(total_bucket_count, 0), ""commas"")
| eval totalEventCount = toString(coalesce(totalEventCount, 0), ""commas"")
| eval total_raw_size_gb = round(total_raw_size / 1024, 2)
| eval avg_bucket_size_gb = round(currentDBSizeGB / total_bucket_count, 2)
| eval compress_ratio = round(total_raw_size_gb / currentDBSizeGB, 2)."" : 1""

| fields title, datatype
    currentDBSizeGB, totalEventCount, total_bucket_count,  avg_bucket_size_gb,
    total_raw_size_gb, compress_ratio, minTime, maxTime
    freeze_period_viz_day, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb,
    hot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb,
    hot_bucket_count,   warm_bucket_count,   cold_bucket_count,   thawed_bucket_count,
    home_event_count,   cold_event_count,    thawed_event_count,
    homePath, homePath_expanded, coldPath, coldPath_expanded, thawedPath, thawedPath_expanded, summaryHomePath_expanded, tstatsHomePath, tstatsHomePath_expanded,
    maxTotalDataSizeMB, frozenTimePeriodInSecs, homePath.maxDataSizeMB, coldPath.maxDataSizeMB,
    maxDataSize, maxHotBuckets, maxWarmDBCount
    ",,,"index_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
      index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes data.name=$single_index_name$
      | eval data_birth_date = if(isnotnull('data.bucket_dirs.cold.event_min_time'), 'data.bucket_dirs.cold.event_min_time', 'data.bucket_dirs.home.event_min_time')
      | eval data_age_days = round((_time - data_birth_date) / 86400, 0)
      | eval data.total_capacity = if(isnotnull('data.total_capacity'), 'data.total_capacity', 500000)
      | eval disk_usage = round('data.total_size' / 1024, 2)
      | eval disk_capacity = round('data.total_capacity' / 1024, 2)
      | `dmc_timechart_for_disk_usage` $funcDataAge$(data_age_days) as data_age_days, $funcDiskUsage$(disk_usage) as disk_usage, $funcDiskUsage$(disk_capacity) as disk_capacity
    ",,"sourcetype=splunk_disk_objects","index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"| eval index_data_type_combined = title."":"".datatype | stats count by title index_data_type_combined
        ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields currentDBSizeGB | eval currentDBSizeGB = currentDBSizeGB."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields total_raw_size_gb | eval total_raw_size_gb = total_raw_size_gb."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields compress_ratio",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields total_bucket_count",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields avg_bucket_size_gb | eval avg_bucket_size_gb = avg_bucket_size_gb."" GB""",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields totalEventCount",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields minTime | eval minTime = replace(minTime, ""T"", "" "")",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"search title=$single_index_name$ | fields maxTime | eval maxTime = replace(maxTime, ""T"", "" "")",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
search title=$single_index_name$
| fields freeze_period_viz_day, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb
| rename freeze_period_viz_day as ""Data Age vs Frozen Age (days)"", disk_usage_gb as ""Index Usage (GB)"", home_bucket_usage_gb as ""Home Path Usage (GB)"", cold_bucket_usage_gb as ""Cold Path Usage (GB)""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
            | rest splunk_server=$splunk_server$ /services/data/indexes/$single_index_name$
            | `dmc_exclude_indexes`
            | rex field=homePath ""volume:(?<home_vol_name>[^/\\\]*)(?:/|\\\)""
            | rex field=coldPath ""volume:(?<cold_vol_name>[^/\\\]*)(?:/|\\\)""
            | fields home_vol_name cold_vol_name
            | eval home_vol_name = if(isnull(home_vol_name), ""N/A"", home_vol_name)
            | eval cold_vol_name = if(isnull(cold_vol_name), ""N/A"", cold_vol_name)
            | rename home_vol_name as ""home"" cold_vol_name as ""cold""
            | transpose | rename column as ""Index Directory"" ""row 1"" as title

            | join type=outer title [
              | rest splunk_server=$splunk_server$ /services/data/index-volumes
              | fields title, total_size, max_size, volume_path
              | `dmc_exclude_volumes`
              | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
              | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
              | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
              | eval remaining_capacity_pct = (max_size_gb - total_size_gb) / max_size_gb
              | fields title, disk_usage_gb, remaining_capacity_pct
            ]

            | eval disk_usage_gb = if(isnull(disk_usage_gb), ""N/A"", disk_usage_gb)
            | eval ""Volume Freezing Due to Size"" = if(isnull(remaining_capacity_pct), ""N/A"", if(remaining_capacity_pct < 0.05, ""Yes"", ""No""))
            | rename title as ""Volume Name"" disk_usage_gb as ""Volume Usage / Capacity (GB)""
            | fields - remaining_capacity_pct
            ]]>
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
search title=$single_index_name$
| fields hot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb
| rename hot_bucket_size_gb as Hot, warm_bucket_size_gb as Warm, cold_bucket_size_gb as Cold, thawed_bucket_size_gb as Thawed
| eval bucket_dir = """"
| fields bucket_dir, Hot, Warm, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields home_event_count, cold_event_count, thawed_event_count
            | rename home_event_count as Home, cold_event_count as Cold, thawed_event_count as Thawed
            | eval bucket_dir = """"
            | fields bucket_dir, Home, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields hot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count
            | rename hot_bucket_count as Hot, warm_bucket_count as Warm, cold_bucket_count as Cold, thawed_bucket_count as Thawed
            | eval bucket_dir = """"
            | fields bucket_dir, Hot, Warm, Cold, Thawed
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=hosts index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields host, totalCount
| sort - totalCount
| rename host as Host, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=sources index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields source, totalCount
| sort - totalCount
| rename source as Source, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| metadata type=sourcetypes index=$single_index_name$ splunk_server=$splunk_server$ datatype=$index_type_value$
| fields sourcetype, totalCount
| sort - totalCount
| rename sourcetype as Sourcetype, totalCount as ""Event Count""
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields homePath, homePath_expanded, coldPath, coldPath_expanded, thawedPath, thawedPath_expanded, summaryHomePath_expanded, tstatsHomePath, tstatsHomePath_expanded
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields maxTotalDataSizeMB, frozenTimePeriodInSecs, homePath.maxDataSizeMB, coldPath.maxDataSizeMB
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            search title=$single_index_name$
            | fields maxDataSize, maxHotBuckets, maxWarmDBCount
            | transpose
            | where column != ""_timediff"" AND column != ""_dmc_title""
            | rename column as Setting, ""row 1"" as Value
          ",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, disk_usage $capacityOverlay$",,,"index_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, data_age_days",,,"index_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes data.name=$single_index_name$

            | eval warm_bucket_total_size       = if(isnotnull('data.bucket_dirs.home.warm_bucket_size'), 'data.bucket_dirs.home.warm_bucket_size', 'data.bucket_dirs.home.size')
            | eval cold_bucket_total_size       = if(isnotnull('data.bucket_dirs.cold.bucket_size'), 'data.bucket_dirs.cold.bucket_size', 'data.bucket_dirs.cold.size')
            | eval thawed_bucket_total_size     = if(isnotnull('data.bucket_dirs.thawed.bucket_size'), 'data.bucket_dirs.thawed.bucket_size', 'data.bucket_dirs.thawed.size')
            | eval hot_bucket_total_size        = if(isnotnull(cold_bucket_total_size), 'data.total_size' - cold_bucket_total_size - warm_bucket_total_size, 'data.total_size' - warm_bucket_total_size)

            | eval warm_bucket_total_size_gb    = round(warm_bucket_total_size / 1024, 2)
            | eval cold_bucket_total_size_gb    = round(cold_bucket_total_size / 1024, 2)
            | eval thawed_bucket_total_size_gb  = round(thawed_bucket_total_size / 1024, 2)
            | eval hot_bucket_total_size_gb     = round(hot_bucket_total_size / 1024, 2)

            | eval warm_bucket_count            = 'data.bucket_dirs.home.warm_bucket_count'
            | eval hot_bucket_count             = 'data.bucket_dirs.home.hot_bucket_count'
            | eval cold_bucket_count            = 'data.bucket_dirs.cold.bucket_count'
            | eval thawed_bucket_count          = 'data.bucket_dirs.thawed.bucket_count'

            | eval home_event_count             = 'data.bucket_dirs.home.event_count'
            | eval cold_event_count             = 'data.bucket_dirs.cold.event_count'
            | eval thawed_event_count           = 'data.bucket_dirs.thawed.event_count'

            | eval hot_bucket_avg_size_gb       = round(hot_bucket_total_size_gb / hot_bucket_count, 2)
            | eval warm_bucket_avg_size_gb      = round(warm_bucket_total_size_gb / warm_bucket_count, 2)
            | eval cold_bucket_avg_size_gb      = round(cold_bucket_total_size_gb / cold_bucket_count, 2)
            | eval thawed_bucket_avg_size_gb    = round(thawed_bucket_total_size_gb / thawed_bucket_count, 2)

            | fields
            hot_bucket_total_size_gb, warm_bucket_total_size_gb, cold_bucket_total_size_gb, thawed_bucket_total_size_gb,
            hot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count,
            home_event_count, cold_event_count, thawed_event_count,
            hot_bucket_avg_size_gb, warm_bucket_avg_size_gb, cold_bucket_avg_size_gb, thawed_bucket_avg_size_gb

            | `dmc_timechart_for_disk_usage` $funcBucket$(*_$single_index_metric$) as ""* bucket""
          ",,"sourcetype=splunk_disk_objects","index_detail_instance"
tbd,,,,"inputlookup index_info",,"lame_analytic_documentation",,"| inputlookup index_info
| rename _key as the_key
| table the_key, index, description, usegroup",,,"index_information"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_indexer_cluster_groups`",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_indexer, Cluster*)` search_group=""$group$""
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_indexer, Cluster*)` punct=""$warningErrorPunct$"" search_group=""$group$""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/buckets?filter=search_state=PendingSearchable""
          | fields title, index, peers*server_name
          | eval peers = """"
          | foreach peers.*.server_name [eval peers = mvappend(peers, """", '&lt;&lt;FIELD&gt;&gt;')]
          | fields title, index, peers
        ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"stats count by index",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            $fixupProgressIndexFilter$
            | rename title as Bucket, index as Index, peers as Peers
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes
            | fields title
            | rename title as index
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=search_factor""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=replication_factor""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" ""/services/cluster/master/fixup?level=generation""
            | fields title, index, initial.reason, initial.timestamp, latest.reason
            | $fixupPendingIndexFilter$
            | `dmc_time_format(initial.timestamp)`
            | rename title as Bucket, index as Index, initial.reason as ""Trigger Condition"", initial.timestamp as ""Trigger Time"", latest.reason as ""Current State""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes
            | fields title, buckets_with_excess_copies, buckets_with_excess_searchable_copies, total_excess_bucket_copies, total_excess_searchable_copies
            | search (buckets_with_excess_copies > 0) OR (buckets_with_excess_searchable_copies > 0) OR (total_excess_bucket_copies > 0) OR (total_excess_searchable_copies > 0)
            | rename title as Index, buckets_with_excess_copies as ""Buckets with Excess Copies"", buckets_with_excess_searchable_copies as ""Buckets with Excess Searchable Copies"", total_excess_bucket_copies as ""Total Excess Copies"", total_excess_searchable_copies as ""Total Excess Searchable Copies""
          ",,,"indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=cmmaster_service search_group=dmc_group_cluster_master search_group=""$group$""
| fields - to_fix_total to_fix_added to_fix_removed
| `dmc_timechart_for_metrics_log` median(to_fix_*) as to_fix_*
| rename to_fix_sync AS Sync to_fix_data_safety AS ""Data safety"" to_fix_gen AS Generation to_fix_rep_factor AS ""Replication factor"" to_fix_search_factor AS ""Search factor"" to_fix_streaming AS Streaming
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=executor name=cmmaster_executor search_group=dmc_group_cluster_master search_group=""$group$""
| eval backlog_change = jobs_added - jobs_finished
| `dmc_timechart_for_metrics_log` sum(jobs_added) AS ""Jobs added"" sum(jobs_finished) AS ""Jobs finished"" sum(backlog_change) AS ""Backlog change""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=jobs name=cmmaster search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(CMRepJob) AS ""Bucket replication"" sum(CMChangeBucketJob_build) AS ""Make bucket searchable"" sum(CMChangeBucketJob_makePrimary) AS ""Make bucket primary"" sum(CMChangeBucketJob_removePrimary) AS ""Removing primary bucket site"" sum(CMTruncJob) AS ""Truncating size of bucket"" sum(CMSyncM2PJob) AS ""Syncing buckets for peers and master"" sum(CMSyncP2PJob) AS ""Syncing bucket between peers"" sum(CMBucketFrozenJob) AS ""Notifying peer that bucket is frozen""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_seconds name=cmmaster_endpoints search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(cluster*) as cluster*
| rename clustermaster*_* AS ""/services/cluster/master/*:*"" clusterconfig_* AS ""/services/cluster/config:*""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=cmmaster_endpoints search_group=dmc_group_cluster_master search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(clustermaster*) as clustermaster*
| rename clustermaster*_* AS ""/services/cluster/master/*:*"" clusterconfig_* AS ""/services/cluster/config:*""
          ",,"sourcetype=splunkd","indexer_clustering_service_activity"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_indexer_cluster_groups`",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"| rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/peers",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"| rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/indexes",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/generation/master
      | fields pending_last_reason, search_factor_met, replication_factor_met
      | eval all_data_searchable_icon = if(isnull(pending_last_reason) or pending_last_reason=="""", ""icon-check"", ""icon-alert"")
      | eval all_data_searchable = if (isnull(pending_last_reason) or pending_last_reason=="""", ""All Data Searchable"", ""Some Data Not Searchable"")
      | eval search_factor_met_icon = if(search_factor_met == 1 or search_factor_met == ""1"", ""icon-check"", ""icon-alert"")
      | eval search_factor_met = if (search_factor_met == 1 or search_factor_met == ""1"", ""Search Factor Met"", ""Search Factor Not Met"")
      | eval replication_factor_met_icon = if(replication_factor_met == 1 or replication_factor_met == ""1"", ""icon-check"", ""icon-alert"")
      | eval replication_factor_met = if (replication_factor_met == 1 or replication_factor_met == ""1"", ""Replication Factor Met"", ""Replication Factor Not Met"")
    ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/server/info
      | stats count
      | eval cluster_master_icon = if(count==1, ""icon-check"", ""icon-alert"")
      | eval cluster_master_indicator = if(count==1, ""Cluster Manager Reachable"", ""Cluster Manager Not Reachable"")
    ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 1 or is_searchable == ""1""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 0 or is_searchable == ""0""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 1 or is_searchable == ""1""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          where is_searchable == 0 or is_searchable == ""0""
          | stats count
        ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count by is_searchable
            | eval is_searchable = if(is_searchable == 1 or is_searchable == ""1"", ""Yes"", ""No"")
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by site",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            fields label, is_searchable, status, site, bucket_count, host_port_pair, last_heartbeat, replication_port, base_generation_id, title, bucket_count_by_index.*
            | eval is_searchable = if(is_searchable == 1 or is_searchable == ""1"", ""Yes"", ""No"")
            | `dmc_time_format(last_heartbeat)`
            | sort - last_heartbeat
            | $peerNameFilter$
            | $peerSearchableFilter$
            | $peerStatusFilter$
            | $peerSiteFilter$
            | fields label, is_searchable, status, site, bucket_count
            | rename label as Peer, is_searchable as ""Fully Searchable"", status as Status, site as Site, bucket_count as Buckets
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            search label = $peerDrilldown$
            | `dmc_time_format(last_heartbeat)`
            | fields host_port_pair, last_heartbeat, replication_port, base_generation_id, title
            | rename host_port_pair as Location, last_heartbeat as ""Last Heartbeat"", replication_port as ""Replication Port"", base_generation_id as ""Base Generation ID"", title as GUID
            | transpose
            | rename column as Property, ""row 1"" as Value
          ",,,"indexer_clustering_status"
tbd,,,"Index=replace",,,"splunk_monitoring_console",,"
            search label = $peerDrilldown$
            | fields bucket_count_by_index.*
            | transpose
            | rename column as Index, ""row 1"" as Buckets
            | eval Index=replace(Index, ""bucket_count_by_index."", """")
            | where isnotnull(Buckets) AND isnotnull(Index)
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count by is_searchable
            | eval is_searchable = if((is_searchable == 1) or (is_searchable == ""1""), ""Yes"", ""No"")
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, is_searchable, replicated_copies_tracker*, searchable_copies_tracker*, num_buckets, index_size
            | rename replicated_copies_tracker.*.* as rp**, searchable_copies_tracker.*.* as sb**
            | eval replicated_data_copies = """"
            | foreach rp*actual_copies_per_slot [eval replicated_data_copies = replicated_data_copies."" "".rp&lt;&lt;MATCHSTR&gt;&gt;actual_copies_per_slot.""/"".rp&lt;&lt;MATCHSTR&gt;&gt;expected_total_per_slot]
            | makemv replicated_data_copies
            | eval searchable_data_copies = """"
            | foreach sb*actual_copies_per_slot [eval searchable_data_copies = searchable_data_copies."" "".sb&lt;&lt;MATCHSTR&gt;&gt;actual_copies_per_slot.""/"".sb&lt;&lt;MATCHSTR&gt;&gt;expected_total_per_slot]
            | makemv searchable_data_copies
            | eval is_searchable = if((is_searchable == 1) or (is_searchable == ""1""), ""Yes"", ""No"")
            | eval index_size = round(index_size / 1024 / 1024 / 1024, 2)."" GB""
            | fields title, is_searchable, searchable_data_copies, replicated_data_copies, num_buckets, index_size
            | $indexNameFilter$
            | $indexSearchableFilter$
            | rename title as ""Index Name"", is_searchable as ""Fully Searchable"", searchable_data_copies as ""Searchable Data Copies"", replicated_data_copies as ""Replicated Data Copies"", num_buckets as Buckets, index_size as ""Cumulative Raw Data Size""
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_cluster_master splunk_server_group=""$group$"" /services/cluster/master/searchheads
          | fields label, status, site, host_port_pair, title
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"stats count by site",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
            $searchHeadNameFilter$
            | $searchHeadStatusFilter$
            | $searchHeadSiteFilter$
            | rename label as ""Search Head"", status as Status, site as Site, host_port_pair as URI, title as GUID
          ",,,"indexer_clustering_status"
tbd,,,,,,"splunk_monitoring_console",,"
                    | `dmc_get_groups_containing_role(dmc_group_indexer)`
                    | where search_group!=""dmc_group_indexer""
                ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes $datatype$
            | join title splunk_server type=outer [rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/indexes-extended $datatype$]
            | `dmc_exclude_indexes`
            | eval elapsedTime = now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")
            | eval dataAge = ceiling(elapsedTime / 86400)
            | eval indexSizeGB = if(currentDBSizeMB >= 1 AND totalEventCount >=1, currentDBSizeMB/1024, null())
            | eval maxSizeGB = maxTotalDataSizeMB / 1024
            | eval sizeUsagePerc = indexSizeGB / maxSizeGB * 100
        ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(title) as numIndexes
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(eval(if(isnotnull(indexSizeGB), title, null()))) as nonEmptyIndexes
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) AS instances
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalSize
                        | eval totalSize = if(isNum(totalSize), round(totalSize, 2)."" GB"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats sum(indexSizeGB) as totalSize
                        dc(splunk_server) as instances
                        | eval averageAggregateIndexesSizePerInstance = round(totalSize / instances, 2)."" GB""
                        | fields averageAggregateIndexesSizePerInstance
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats median(dataAge) AS medianDataAge
                        | eval medianDataAge = if(isNum(medianDataAge), medianDataAge."" days"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats max(dataAge) AS oldestDataAge
                        | eval oldestDataAge = if(isNum(oldestDataAge), oldestDataAge."" days"", ""N/A"")
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(title) as count",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        stats dc(splunk_server) AS Instances
                        count(indexSizeGB) as ""Non-Empty Instances""
                        sum(indexSizeGB) AS totalSize
                        avg(indexSizeGB) as averageSize
                        avg(sizeUsagePerc) as averageSizePerc
                        perc90(sizeUsagePerc) as ninetyPercentileSizePerc
                        count(eval(sizeUsagePerc > 95)) as instancesFreezingDueToSize
                        median(dataAge) as medianDataAge
                        max(dataAge) as oldestDataAge
                        count(eval(elapsedTime > frozenTimePeriodInSecs)) as instancesFreezingDueToAge
                        sum(frozenTimePeriodInSecs) as infiniteFreezingFlag
                        by title, datatype
                        | eval totalSize = if(isnotnull(totalSize), round(totalSize, 2), 0)
                        | eval averageSize = if(isnotnull(averageSize), round(averageSize, 2), 0)
                        | eval averageSizePerc = if(isnotnull(averageSizePerc), round(averageSizePerc, 2).""%"", ""N/A"")
                        | eval ninetyPercentileSizePerc = if(isnotnull(ninetyPercentileSizePerc), round(ninetyPercentileSizePerc, 2).""%"", ""N/A"")
                        | eval instancesFreezingDueToSize = if(averageSizePerc != ""N/A"", instancesFreezingDueToSize, ""N/A"")
                        | eval medianDataAge = if(isNum(medianDataAge), medianDataAge, ""N/A"")
                        | eval oldestDataAge = if(isNum(oldestDataAge), oldestDataAge, ""N/A"")
                        | eval instancesFreezingDueToAge = if(infiniteFreezingFlag > 0, instancesFreezingDueToAge, ""N/A"")
                        | rename title as ""Index""
                        datatype as ""Data Type""
                        totalSize as ""Total Size (GB)""
                        averageSize as ""Average Size (GB)""
                        averageSizePerc as ""Average Usage (%)""
                        ninetyPercentileSizePerc as ""90th Percentile Usage (%)""
                        instancesFreezingDueToSize as ""Instances Freezing Due To Size*""
                        medianDataAge as ""Median Data Age (days)""
                        oldestDataAge as ""Oldest Data Age (days)""
                        instancesFreezingDueToAge as ""Instances Freezing Due to Age""
                        | fields - infiniteFreezingFlag
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" services/data/index-volumes
                        | `dmc_exclude_volumes`
                        | eval volumeSizeGB = if(total_size > 1, round(total_size / 1024, 2), null())
                        | eval sizeUsagePerc = total_size / max_size * 100
                        | stats dc(splunk_server) as Instances
                        count(eval(total_size > 1)) as ""Non-Empty Instances""
                        sum(volumeSizeGB) as totalSize
                        avg(volumeSizeGB) as avgSize
                        avg(sizeUsagePerc) as avgSizePerc
                        perc90(sizeUsagePerc) as ninetyPercentileSizePerc
                        count(eval(total_size > max_size)) as volumesFreezingDueToSize
                        by title
                        | eval totalSize = if(isnotnull(totalSize), totalSize, 0)
                        | eval avgSize = if(isnotnull(avgSize), round(avgSize, 2), 0)
                        | eval avgSizePerc = if(isnotnull(avgSizePerc), round(avgSizePerc, 2).""%"", ""N/A"")
                        | eval ninetyPercentileSizePerc = if(isnotnull(ninetyPercentileSizePerc), round(ninetyPercentileSizePerc, 2).""%"", ""N/A"")
                        | eval volumesFreezingDueToSize = if(avgSizePerc != ""N/A"", volumesFreezingDueToSize, ""N/A"")
                        | rename title as ""Volume""
                        totalSize as ""Total Size (GB)""
                        avgSize as ""Average Size (GB)""
                        avgSizePerc as ""Average Usage (%)""
                        ninetyPercentileSizePerc as ""90th Percentile Usage (%)""
                        volumesFreezingDueToSize as ""Volumes Freezing Due To Size""
                    ",,,"indexes_and_volumes_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/data/indexes $datatype$
  | join title type=outer [
  | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
  | eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
  | fields title, cold_bucket_size, total_size, total_bucket_count]
| `dmc_exclude_indexes`
| fields title datatype maxTotalDataSizeMB currentDBSizeMB frozenTimePeriodInSecs minTime coldPath.maxDataSizeMB homePath.maxDataSizeMB, homePath, coldPath, cold_bucket_size, total_size, total_bucket_count, totalEventCount
| eval currentDBSizeGB = if(isnotnull(currentDBSizeMB), round(currentDBSizeMB / 1024, 2), 0)
| eval maxTotalDataSizeGB = if((maxTotalDataSizeMB == 0) OR isnull(maxTotalDataSizeMB), ""unlimited"", round(maxTotalDataSizeMB / 1024, 2))
| eval disk_usage_gb = currentDBSizeGB."" / "".maxTotalDataSizeGB
| eval currentTimePeriodDay = round((now() - strptime(minTime,""%Y-%m-%dT%H:%M:%S%z"")) / 86400, 0)
| eval currentTimePeriodDay = if(isnull(currentTimePeriodDay), 0, currentTimePeriodDay)
| eval frozenTimePeriodDay = round(frozenTimePeriodInSecs / 86400, 0)
| eval frozenTimePeriodDay = if(isnull(frozenTimePeriodDay) OR frozenTimePeriodDay == 0, ""unlimited"", frozenTimePeriodDay)
| eval freeze_period_viz = currentTimePeriodDay."" / "".frozenTimePeriodDay
| eval total_bucket_count = if(isnotnull(total_bucket_count), total_bucket_count, 0)
| eval totalEventCount = if(isnotnull(totalEventCount), totalEventCount, 0)
| eval home_bucket_size_gb = round((total_size - if(isnull(cold_bucket_size), 0, cold_bucket_size)) / 1024, 2)
| eval home_bucket_size_gb = if(isnull(home_bucket_size_gb), 0, home_bucket_size_gb)
| eval home_bucket_capacity_gb = if(isnull('homePath.maxDataSizeMB') OR 'homePath.maxDataSizeMB' = 0, ""unlimited"", round('homePath.maxDataSizeMB' / 1024, 2))
| eval home_bucket_usage_gb = home_bucket_size_gb."" / "".home_bucket_capacity_gb
| eval cold_bucket_size_gb = if(isnull(cold_bucket_size), 0, round(cold_bucket_size / 1024, 2))
| eval cold_bucket_capacity_gb = if(isnull('coldPath.maxDataSizeMB') OR 'coldPath.maxDataSizeMB' = 0, ""unlimited"", round('coldPath.maxDataSizeMB' / 1024, 2))
| eval cold_bucket_usage_gb = cold_bucket_size_gb."" / "".cold_bucket_capacity_gb
| fields title, datatype, freeze_period_viz, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb, total_bucket_count, totalEventCount, currentDBSizeGB,
      cold_bucket_size_gb, home_bucket_size_gb, homePath, coldPath
    ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(currentDBSizeGB) as total_gb
            | eval total_gb = case(
                total_gb > 1000, round(total_gb / 1024, 2)."" TB"",
                1 = 1, total_gb."" GB""
              )
            ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(totalEventCount) as totalEventCount
            | eval totalEventCount = tostring(totalEventCount, ""commas"")
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(total_bucket_count) as total_bucket_count
            | eval total_bucket_count = tostring(total_bucket_count, ""commas"")
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, datatype, freeze_period_viz, disk_usage_gb, home_bucket_usage_gb, cold_bucket_usage_gb, totalEventCount, total_bucket_count
            | eval total_bucket_count=tostring(total_bucket_count, ""commas"")
            | eval totalEventCount=tostring(totalEventCount, ""commas"")
            | rename title as Index, datatype as ""Data Type"", disk_usage_gb as ""Index Usage (GB)"", freeze_period_viz as ""Data Age vs Frozen Age (days)"", home_bucket_usage_gb as ""Home Path Usage (GB)"", cold_bucket_usage_gb as ""Cold Path Usage (GB)"", total_bucket_count as ""Total Bucket Count"", totalEventCount as ""Total Event Count""
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/data/index-volumes
            | fields title, total_size, max_size, volume_path
            | `dmc_exclude_volumes`
            | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
            | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
            | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
            | fields title, disk_usage_gb, max_size_gb, volume_path
            | rename title as Volume, disk_usage_gb as ""Volume Usage (GB)"", max_size_gb as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
fields title, cold_bucket_size_gb, home_bucket_size_gb, homePath, coldPath
| rex field=homePath ""volume:(?&lt;home_vol_name&gt;[^/\\\]*)(?:/|\\\)""
| rex field=coldPath ""volume:(?&lt;cold_vol_name&gt;[^/\\\]*)(?:/|\\\)""
| eval dir = ""home,cold""
| makemv dir delim="",""
| mvexpand dir
| eval dir_size = case(dir == ""home"", home_bucket_size_gb, dir == ""cold"", cold_bucket_size_gb)
| eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
| fields title dir dir_size vol_name
| where isnotnull(vol_name) AND isnotnull(dir_size) AND dir_size > 0
| eval index_dir_name = title."":"".dir
| chart sum(dir_size) AS dir_size over vol_name by index_dir_name
          ",,,"indexes_and_volumes_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Indexes
            | `dmc_exclude_indexes`
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | `dmc_timechart_for_disk_usage` $funcIndex$($index_metric$) as total by data.name
          ",,"sourcetype=splunk_disk_objects","indexes_and_volumes_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Volumes
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | `dmc_timechart_for_disk_usage` $funcVolume$(data.total_size) as total_size by data.name
          ",,"sourcetype=splunk_disk_objects","indexes_and_volumes_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/introspection/queues
      | eval current_fill_perc = round(current_size_bytes / max_size_bytes * 100, 0)
      | fields title, current_fill_perc
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
      eval a = ""to transpose table""
      | where title==""parsingQueue"" OR title==""aggQueue"" OR title==""typingQueue"" OR title==""indexQueue""
      | chart values(current_fill_perc) over a by title
      | fields parsingQueue, aggQueue, typingQueue, indexQueue
      | eval parsingQueue = if(isnotnull(parsingQueue), parsingQueue, ""N/A"")
      | eval aggQueue = if(isnotnull(aggQueue), aggQueue, ""N/A"")
      | eval typingQueue = if(isnotnull(typingQueue), typingQueue, ""N/A"")
      | eval indexQueue = if(isnotnull(indexQueue), indexQueue, ""N/A"")
    ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/introspection/indexer
            | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
            | fields splunk_server, average_KBps, status
            | eval average_KBps = round(average_KBps, 0)
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/pipelineSetSelectionPolicy
              | fields value
              | rename value as pipeline_set_selection_policy]
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines]
            | fields splunk_server, value, pipeline_set_selection_policy, average_KBps, status
            | rename splunk_server as Instance, value as ""Pipeline Set Count"", pipeline_set_selection_policy as ""Pipeline Set Selection Policy"", average_KBps as ""Indexing Rate (KB/s)"", status as Status, reason as Reason
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=pipeline
            | stats count by ingest_pipe
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" group=dutycycle
            | eval dutycycle_ratio_perc=ratio*100
            | `dmc_timechart_for_metrics_log` avg(dutycycle_ratio_perc) by thread limit=30
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" group=dutycycle name=ingest
            | fillnull value=0 ingest_pipe
            | search ingest_pipe=$log_pipe_scope$
            | eval dutycycle_ratio_perc=ratio*100
            | chart limit=30 avg(dutycycle_ratio_perc) over ingest_pipe by thread
            | rename ingest_pipe as ""Pipeline Set""
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | chart limit=30 count(busiest_thread_name) over ingest_pipe by busiest_thread_name
            | rename ingest_pipe as ""Pipeline Set""
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | eval dutycycle_ratio_perc=dutycycle_ratio*100
            | `dmc_timechart_for_metrics_log` max(dutycycle_ratio_perc) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | eval pipeline_set_selection_perc=share*100
            | `dmc_timechart_for_metrics_log` max(pipeline_set_selection_perc) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ ingest_pipe=$log_pipe_scope$ source=""*metrics.log"" group=pipelineset name=load_metrics
            | `dmc_timechart_for_metrics_log` max(requests_last_period) by ingest_pipe
          ","source=*metrics.log",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=pipeline NOT processor=sendout
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` per_second(eval(cpu_seconds*100)) AS pctCPU by processor useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=pipeline name=indexerpipe processor=indexer
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` sum(write_cpu_seconds) AS ""raw data write"" sum(service_cpu_seconds) AS ""index service""
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=subtask_seconds
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | fields replicate_semislice, sync_hotBkt, throttle_optimize, flushBlockSig, retryMove_1hotBkt, size_hotBkt, roll_hotBkt, chillOrFreeze, update_checksums, fork_recovermetadata, rebuild_metadata, update_bktManifest, service_volumes, service_maxSizes, service_externProc
            | `dmc_timechart_for_metrics_log` sum(*) AS *
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ source=*/metrics.log* group=per_$groupTypeCpuProfValue$_$processorTypeCpuProfValue$_cpu*
          | eval cpu_time_ms = if(isnotnull(cpu_time_ms), cpu_time_ms, cpu)
          | eval avg_cpu_time_per_event_ms = if(isnotnull(avg_cpu_time_per_event_ms), avg_cpu_time_per_event_ms, cpupe)
          | eval event_count = if(isnotnull(event_count), event_count, ev)
          | bin _time minspan=30s
          | stats $functionQueueCpuProf$(cpu_time_ms) AS cpu_time_ms $functionQueueCpuProf$(avg_cpu_time_per_event_ms) AS avg_cpu_time_per_event_ms $functionQueueCpuProf$(bytes) AS bytes $functionQueueCpuProf$(event_count) AS event_count by series, _time
        ","source=*",,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(cpu_time_ms) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(avg_cpu_time_per_event_ms) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(bytes) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_metrics_log` $functionQueueCpuProf$(event_count) by series
          ",,,"indexing_performance_advanced"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/server/introspection/queues
        | search title=parsingQueue* OR title=aggQueue* OR title=typingQueue* OR title=indexQueue*
        | eval fill_perc=round(current_size_bytes / max_size_bytes * 100,2)
        | fields splunk_server, title, fill_perc
        | rex field=title ""(?<queue_name>^\w+)(?:\.(?<pipeline_number>\d+))?""
        | eval fill_perc = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".fill_perc, fill_perc)
        | chart values(fill_perc) over splunk_server by queue_name
        | eval pset_count = mvcount(parsingQueue)
        | join type=outer splunk_server [
          | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/server/introspection/indexer
          | eval average_KBps = round(average_KBps, 0)
          | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
          | fields splunk_server, average_KBps, status]
        | fields splunk_server pset_count average_KBps status parsingQueue aggQueue typingQueue indexQueue
        | sort -average_KBps
      ]]>
    ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(average_KBps)",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
stats avg(average_KBps) as avg_indexing_rate
| eval avg_indexing_rate = round(avg_indexing_rate, 0)
          ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats median(average_KBps)",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            rename splunk_server as Instance, pset_count as ""Pipeline Set Count"", average_KBps as ""Indexing Rate (KB/s)"", status as ""Status"", parsingQueue as ""Parsing Queue Fill Ratio (%)"", aggQueue as ""Aggregation Queue Fill Ratio (%)"", ""typingQueue"" as ""Typing Queue Fill Ratio (%)"", indexQueue as ""Indexing Queue Fill Ratio (%)""
          ",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
                    | `dmc_timechart_for_metrics_log` partial=f limit=0 per_second(kb) AS kbps by host
                    | untable _time host kbps
                    | eval kbps = round(kbps,0)
                    | `dmc_indexing_rate_rangemap_and_timechart`
                ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
| `dmc_timechart_for_metrics_log` partial=f limit=0 per_second(kb) AS kbps by host
| untable _time host kbps
| eval kbps = round(kbps,0)
| `dmc_indexing_rate_rangemap_and_timechart`
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_indexing_rate(""$group$"", $drilldown_indexing_rate_metric$)`",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput
| `dmc_timechart_for_metrics_log` partial=f per_second(kb) AS kbps by host
| eval kbps = round(kbps, 0)
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` source=*metrics.log* sourcetype=splunkd search_group=dmc_group_indexer search_group=""$group$"" group=thruput name=index_thruput 
| `dmc_timechart_for_metrics_log` partial=f per_second(kb) AS kbps dc(host) AS num_hosts 
| eval kbps = kbps / $aggrIdxRateDivider$ 
| eval kbps = round(kbps, 0)
| rename kbps as ""KB/s""
| fields - num_hosts
          ","source=*metrics.log*","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
                    | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
                    | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
                    | eval fill_perc=round((curr/max)*100,2)
                    | bin _time minspan=30s
                    | stats $instanceWideFillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
                    | `dmc_queue_fill_ratio_rangemap_and_timechart`
                ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| bin _time minspan=30s
| stats $instanceWideFillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
| `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_queue_fill_ratio(""$group$"", $instanceWideFillRatioQueueType$, $instanceWideFillRatioAggrFunc$, $drilldown_queue_fill_ratio_metric$)`",,,"indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$instanceWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` partial=f limit=25 $instanceWideFillRatioAggrFunc$(fill_perc) AS fill_percentage by host
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=$deploymentWideFillRatioQueueType$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` partial=f $deploymentWideFillRatioAggrFunc$(fill_perc) AS fill_percentage
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/introspection/queues
      | eval current_fill_perc = round(current_size_bytes / max_size_bytes * 100, 0)
      | fields title, current_fill_perc
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      eval a = ""to transpose table""
      | where title==""parsingQueue"" OR title==""aggQueue"" OR title==""typingQueue"" OR title==""indexQueue""
      | chart values(current_fill_perc) over a by title
      | fields parsingQueue, aggQueue, typingQueue, indexQueue
      | eval parsingQueue = if(isnotnull(parsingQueue), parsingQueue, ""N/A"")
      | eval aggQueue = if(isnotnull(aggQueue), aggQueue, ""N/A"")
      | eval typingQueue = if(isnotnull(typingQueue), typingQueue, ""N/A"")
      | eval indexQueue = if(isnotnull(indexQueue), indexQueue, ""N/A"")
    ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/introspection/indexer
            | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason)
            | fields splunk_server, average_KBps, status
            | eval average_KBps = round(average_KBps, 0)
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/pipelineSetSelectionPolicy
              | fields value
              | rename value as pipeline_set_selection_policy]
            | join type=outer [
              | rest splunk_server=$splunk_server$ /services/properties/server/general/parallelIngestionPipelines]
            | fields splunk_server, value, pipeline_set_selection_policy, average_KBps, status
            | rename splunk_server as Instance, value as ""Pipeline Set Count"", pipeline_set_selection_policy as ""Pipeline Set Selection Policy"", average_KBps as ""Indexing Rate (KB/s)"", status as Status, reason as Reason
          ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
            search title=""parsingQueue.*"" OR title=""aggQueue.*"" OR title=""typingQueue.*"" OR title=""indexQueue.*""
            | rex field=title ""(?<queue_name>^\w+)\.(?<pipeline_number>\d+)""
            | chart values(current_fill_perc) over pipeline_number by queue_name
            | fields pipeline_number, parsingQueue, aggQueue, typingQueue, indexQueue
            | rename pipeline_number as ""Pipeline Number"", parsingQueue as ""Parsing Queue Fill Ratio (%)"", aggQueue as ""Aggregator Queue Fill Ratio (%)"", typingQueue as ""Typing Queue Fill Ratio (%)"", indexQueue as ""Index Queue Fill Ratio (%)""
            ]]>
          ",,,"indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=pipeline
            | stats count by ingest_pipe
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ source=""*metrics.log"" sourcetype=splunkd group=per_$groupTypeIdxPerf$_thruput
            | eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
            | search ingest_pipe=$log_pipe_scope$
            | `dmc_timechart_for_metrics_log` per_second(kb) by series useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` host=$host$ source=*metrics.log sourcetype=splunkd group=queue $queues$
| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, ""none"")
| search ingest_pipe=$log_pipe_scope$
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
| eval fill_perc=round((curr/max)*100,2)
| `dmc_timechart_for_metrics_log` $functionQueue$(fill_perc) by name useother=false limit=15
          ","source=*metrics.log","sourcetype=splunkd","indexing_performance_instance"
tbd,,,,,,search,,"| rest splunk_server=$splunk_server$ /services/server/info
| fields splunk_server version
| join type=outer splunk_server [rest splunk_server=$splunk_server$ /services/server/status/installed-file-integrity
    | fields splunk_server check_ready check_failures.fail]
| eval check_status = case(isnull('check_failures.fail') AND isnotnull(check_ready), ""enabled"", 'check_failures.fail' == ""check_disabled"", ""disabled"", isnull(check_ready), ""feature unavailable"")
| eval check_ready = if(check_status == ""enabled"", check_ready, ""N/A"")
| fields version check_status check_ready
| rename version AS ""Splunk version"" check_status AS ""Check status"" check_ready AS ""Results ready?""",,,"integrity_check_of_installed_files"
tbd,,,,,,search,,"| rest splunk_server=$splunk_server$ /services/server/status/installed-file-integrity
| fields check_failures.*
| untable splunk_server file_path check_result
| replace ""check_failures.*"" WITH ""*"" IN file_path
| fields file_path check_result
| rename file_path AS ""File path"" check_result AS ""Check result""",,,"integrity_check_of_installed_files"
tbd,,,,,,search,,"
            | rest /services/search/jobs/$sid$ splunk_server=local
        ",,,"job_details_dashboard"
tbd,,,,,,search,,"fields runDuration",,,"job_details_dashboard"
tbd,,,,,,search,,"fields scanCount",,,"job_details_dashboard"
tbd,,,,,,search,,"fields eventCount",,,"job_details_dashboard"
tbd,,,,,,search,,"fields resultCount",,,"job_details_dashboard"
tbd,,,,,,search,,"eval eventsScannedPerSec=scanCount / exact(runDuration) | fields eventsScannedPerSec",,,"job_details_dashboard"
tbd,,,,,,search,,"fields dispatchState | eval dispatchState=lower(dispatchState)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields author",,,"job_details_dashboard"
tbd,,,,,,search,,"eval searchMode=if (isnull('custom.display.page.search.mode'), ""smart"", 'custom.display.page.search.mode') | fields searchMode",,,"job_details_dashboard"
tbd,,,,,,search,,"fields splunk_server",,,"job_details_dashboard"
tbd,,,,,,search,,"fields search",,,"job_details_dashboard"
tbd,,,,,,search,,"fields optimizedSearch",,,"job_details_dashboard"
tbd,,,,,,search,,"fields remoteSearch",,,"job_details_dashboard"
tbd,,,,,,search,,"fields reduceSearch | eval reduceSearch=if (isnull(reduceSearch), ""N/A"", reduceSearch)",,,"job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval fields=mvzip(command, duration, "","") | mvexpand fields | rex field=fields ""(?&lt;Command&gt;[^,]+),(?&lt;Duration&gt;.+)$""
                        | table Command, Duration",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,,,,search,,"fields performance*dispatch.stream.remote.*.duration_secs, performance*dispatch.stream.local.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats max(Duration)",,,"job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields reduceSearch | rex field=reduceSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | eval Command=if(like(Command, ""%stats%""), ""stats"", Command) | eval Command=if(like(Command, ""%bin%""), ""prebin"", Command) | join Command [| search index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | stats sum(Duration)",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields remoteSearch | rex field=remoteSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | join Command [| search index=""_introspection"" sourcetype=search_telemetry search_id=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | table Command, Duration",,"sourcetype=search_telemetry","job_details_dashboard"
tbd,,,"index=_introspection",,,search,,"fields reduceSearch | rex field=reduceSearch max_match=0 ""(^(?&lt;first&gt;\w+){1})|((\| (?&lt;command&gt;\w+)))"" | eval Command = mvappend(first, command) | mvexpand Command | fields Command | eval Command=if(like(Command, ""%stats%""), ""stats"", Command) | eval Command=if(like(Command, ""%bin%""), ""prebin"", Command) | join Command [| search index=""_introspection"" sourcetype=search_telemetry source=*$sid$* | spath path=search_commands{}.name output=command | spath path=search_commands{}.duration output=duration | eval tempField= mvzip(command, duration) | stats count by tempField | eval Command = mvindex(split(tempField,"",""),0), Duration= mvindex(split(tempField,"",""),1) | fields Command, Duration] | table Command, Duration","source=*","sourcetype=search_telemetry","job_details_dashboard"
tbd,,,,,,search,,"fields performance.startup.handoff.duration_secs",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance*dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats count(Indexer)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance*dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as Duration | stats avg(Duration)",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance*dispatch.stream.remote.*.duration_secs | transpose | rename column as Indexer ""row 1"" as ""Duration (seconds)"" | rex field=Indexer ""performance*dispatch.stream.remote.(?&lt;Indexer&gt;.*).duration_secs""",,,"job_details_dashboard"
tbd,,,,,,search,,"fields performance*dispatch.stream.remote.$indexer$.* | transpose | rename column as Field ""row 1"" as Value | rex field=Field ""performance*dispatch.stream.remote.$indexer$.(?&lt;Field&gt;.*)$""",,,"job_details_dashboard"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) | table owner
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
	| rename eai:acl.owner as owner
	| table owner]
| append [
    | rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
	| rename eai:data as xml eai:acl.owner as owner
    ``` include only dashboards with customjs ```
    | regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
    ``` filter out dashboards with version=""1.1"" explicitly set ```
    | regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
	| table owner]
| dedup owner",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load)
| fields app
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
	| rename eai:acl.app as app
	| table app]
| append [
	| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
	| rename eai:data as xml eai:acl.app as app
    ``` include only dashboards with customjs ```
    | regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
    ``` filter out dashboards with version=""1.1"" explicitly set ```
    | regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
	| table app]
| dedup app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]
| strcat app_label "" ("" app "")"" app_dropdown",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) owner=$field2$ app=$field3$ | timechart count",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,"index=_internal",,,search,,"index=_internal sourcetype=splunk_web_service TERM(dashboard_migrate_type=v1.0_load) owner=$field2$ app=$field3$
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]
| stats count by view_name, owner, app, app_label
| sort - count",,"sourcetype=splunk_web_service","jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
| rename eai:data as xml
  title as view_name
  eai:acl.app as app
  eai:acl.owner as owner
| search owner=$field2$ app=$field3$
``` include only dashboards with customjs ```
| regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
``` filter out dashboards with version=""1.1"" explicitly set ```
| regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
| stats count",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""rootNode=form OR rootNode=dashboard"" count=0
| rename eai:data as xml
  title as view_name
  eai:acl.app as app
  eai:acl.owner as owner
| search owner=$field2$ app=$field3$
``` include only dashboards with customjs ```
| regex xml=""^&lt;(dashboard|form)(.|\n)*script[ ]*=[ ]*(?:\'|\"").*\.js(?:\'|\"")(.|\n)*&gt;(.|\n)*""
``` filter out dashboards with version=""1.1"" explicitly set ```
| regex xml!=""^&lt;(dashboard|form)(.|\n)*version[ ]*=[ ]*(?:\'|\"")1.1(?:\'|\"")(.|\n)*&gt;(.|\n)*""
| table view_name, owner, app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
          | rename eai:acl.app as app eai:acl.owner as owner
          | search owner=$field2$ app=$field3$
          | stats count",,,"jquery_upgrade"
tbd,,,,,,search,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views search=""eai:type=html"" count=0
| rename title as view_name eai:acl.app as app eai:acl.owner as owner
| search owner=$field2$ app=$field3$
| table view_name, owner, app
| join type=inner app
	[ | rest splunk_server=local /servicesNS/-/-/apps/local count=0
	| rename title as app label as app_label
	| table app app_label]",,,"jquery_upgrade"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_kv_store)`
          | where search_group!=""dmc_group_kv_store""
        ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_kv_store, KVStore*)` search_group=""$group$""
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(dmc_group_kv_store, KVStore*)` punct=""$warningErrorPunct$"" search_group=""$group$""
          ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | eval server = host
                    | rename data.opcounters.command as c, data.opcounters.update as u, data.opcounters.queries as q, data.opcountes.deletes as d, data.opcounters.getmore as g, data.opcounters.inserts as i | eval commands=if(isNotNull('c'), 'c',0)
                    | eval totalops=if(isNotNull('c'), 'c',0)+if(isNotNull('u'), 'u', 0)+if(isNotNull('q'), 'q', 0)+if(isNotNull('d'), 'd', 0)+if(isNotNull('g'), 'g', 0)+if(isNotNull('i'), 'i', 0)
                    | bin _time minspan=30s
                    | stats latest(totalops) AS ops latest(data.extra_info.page_faults) AS pf by server _time
                    | eval  percent=if(ops==0, 0, round(abs(pf/ops), 2))
                    | bin _time minspan=30s
                    | stats $countPageFaultsFunc$(percent) as percent by server _time
                    | rangemap field=percent ""0-0.7""=0-0.7 ""0.7-1.3""=0.7001-1.3 ""1.3+""=1.3001-999999 default=abnormal
                    | timechart minspan=30s partial=f dc(server) as server_count by range
                    | fields _time, ""1.3+"", ""0.7-1.3"", ""0-0.7""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_page_faults(""$group$"", $countPageFaultsFunc$, $drilldown_page_fault_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_network(""$group$"")`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | eval source=host
                    | eval ratio='data.mem.virtual'/'data.mem.mappedWithJournal'
                    | bin _time minspan=30s
                    | stats avg(ratio) AS myratio by source _time
                    | eval myratio = round(myratio, 2)
                    | rangemap field=myratio ""0-2x""=0-2 ""2-3x""=2.001-3 "">3x""=3.001-10000 default=abnormal
                    | timechart minspan=30s partial=f dc(source) as server_count by range
                    | fields _time "">3x"", ""2-3x"", ""0-2x""
                ","source=host",,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_memory_ratio(""$group$"", $drilldown_mapped_memory_ratio_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstorereplicasetstats data.replSetStats.myState=1
                    | spath data.replSetStats.members{}.name output=name
                    | spath data.replSetStats.members{}.optimeDate output=optimeDate
                    | spath data.replSetStats.members{}.stateStr output=stateStr
                    | eval prop_key_val=mvzip(mvzip(name, optimeDate, ""---""), stateStr, ""---"")
                    | fields _time, prop_key_val
                    | mvexpand prop_key_val
                    | eval name=mvindex(split(prop_key_val, ""---""), 0)
                    | eval optimeDate=mvindex(split(prop_key_val, ""---""), 1)
                    | eval stateStr=mvindex(split(prop_key_val, ""---""), 2)
                    | where stateStr=""SECONDARY""
                    | join _time
                    [ search `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstorereplicasetstats data.replSetStats.myState=1
                       | spath data.replSetStats.members{}.name output=name
                       | spath data.replSetStats.members{}.optimeDate output=optimeDate
                       | spath data.replSetStats.members{}.stateStr output=stateStr
                       | eval prop_key_val=mvzip(mvzip(name, optimeDate, ""---""), stateStr, ""---"")
                       | fields _time, prop_key_val
                       | mvexpand prop_key_val
                       | eval name=mvindex(split(prop_key_val, ""---""), 0)
                       | eval optimeDate=mvindex(split(prop_key_val, ""---""), 1)
                       | eval stateStr=mvindex(split(prop_key_val, ""---""), 2)
                       | where stateStr=""PRIMARY""
                       | stats max(optimeDate) as primary by _time]
                    | eval difference=(primary-optimeDate)/1000
                    | bin _time minspan=1m
                    | stats avg(difference) AS lag by _time name
                    | eval lag = round(lag, 2)
                    | `dmc_replication_lag_rangemap`
                    | timechart minspan=1m partial=f dc(name) as server_count by range
                    | fields _time "">30s"", ""10-30s"", ""0-10s""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_replication_lag(""$group$"", $drilldown_rep_latency_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_primary_oplog_window(""$group$"")`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=dmc_group_kv_store search_group=""$group$"" component=kvstoreserverstats
                    | bin _time minspan=30s
                    | stats range(data.backgroundFlushing.total_ms)  AS diff by _time host
                    | timechart minspan=30s per_minute(diff) as ms_per_min by host
                    | untable _time host ms_per_min
                    | eval percent = round(ms_per_min / (60 * 1000) * 100, 2)
                    | `dmc_background_flush_rangemap`
                    | timechart minspan=30s partial=f dc(host) as server_count by range
                    | fields _time, ""50-100%"", ""10-50%"", ""0-10%""
                ",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_kv_store_deployment_background_flush(""$group$"", $drilldown_background_flush_metric$)`",,,"kv_store_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_get_groups_containing_role(dmc_group_kv_store)`
            | search search_group!=""dmc_group_*""
          ",,,"kv_store_instance"
tbd,,,,,,"splunk_secure_gateway",,"| rest ""services/ssg/kvstore_client""
app=$app$
collection=$collection$
owner=$owner$
sort=$sort$
limit=$limit$
",,,"kvstore_dashboard_client"
tbd,,,,,,"splunk_secure_gateway",,"| rest /services/apps/local | search disabled=0 | table label title",,,"kvstore_dashboard_client"
tbd,,,,,,"splunk_secure_gateway",,"| rest /servicesNS/nobody/$app$/storage/collections/config |   table title eai:acl.app | rename eai:acl.app as acl | where acl != ""system""",,,"kvstore_dashboard_client"
tbd,,,,,,"splunk_secure_gateway",,"| stats count
| fields - count
| eval username=""nobody""
| append [|rest /services/authentication/current-context/context | table username]",,,"kvstore_dashboard_client"
tbd,,,,,,"splunk_secure_gateway",," | table result",,,"kvstore_dashboard_client"
tbd,,,,,,"lame_training",,"
      | makeresults | eval foo=""bar""
      | table foo
    ",,,"lame_channel_timebasetokens"
tbd,,,,,,"lame_training",,"$data_source$ 
| eval src_ip = coalesce(src_ip, src)
| eval dest_ip = coalesce(dest_ip, dest)
| table sourcetype, src_ip, dest_ip",,,"lame_channel_timebasetokens"
tbd,,,"index=lame_training",,"lookup ip_inventory.csv","lame_training",,"
    index=lame_training sourcetype=lame_conn src_ip=$ip$ | lookup ip_inventory.csv ip as src_ip output hostname, os | head 1
| table src_ip, hostname, os
  ",,"sourcetype=lame_conn","lame_channel_token_usage"
tbd,,,,,,"splunk_monitoring_console",,"
      |rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/saved/searches/DMC%20License%20Usage%20Data%20Cube | fields auto_summarize | eval no_acceleration=if(auto_summarize == 1, NULL, ""yes"")
    ",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
					|  `dmc_get_instance_info(""dmc_group_license_master"")`
					| fields host, serverName
				",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval name=pool | eval value=""pool=\"""". pool . ""\"""" | table name value
        ",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$daily_usage_search$($splunk_server$, $size_search$, $host$, ""$pool$"", ""$split_by_field_name$"")` $overlay_remove$",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$daily_usage_pct_search$($splunk_server$, $sz_clause$, $host$, ""$split_by_field_name$"")`",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"`$base_search$($host$,""$pool$"")` | `$max_avg_search$($splunk_server$, ""$split_by_field_name$"", ""$split_by$"")`",,,"license_usage_historic"
tbd,,,,,,"splunk_monitoring_console",,"
					|  `dmc_get_instance_info(""dmc_group_license_master"")`
					| fields host, serverName
				",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | join type=outer stack_id [rest splunk_server=$splunk_server$ /services/licenser/stacks | eval stack_id=title | eval stack_quota=quota | fields stack_id stack_quota] | stats sum(used_bytes) as used max(stack_quota) as total | eval usedGB=round(used/1024/1024/1024,3) | eval totalGB=round(total/1024/1024/1024,3) | eval gauge_base=0 | eval gauge_danger=totalGB*0.8 | eval gauge_top=totalGB+0.001 | gauge usedGB gauge_base gauge_danger totalGB gauge_top
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval ""Used""=round(used_bytes/1024/1024/1024, 3) | eval ""Quota""=round(quota/1024/1024/1024, 3) | fields Pool ""Used"" ""Quota""
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/pools | rename title AS Pool | search [rest splunk_server=$splunk_server$ /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields stack_id] | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval ""% used""=round(used_bytes/quota*100,2) | fields Pool ""% used""
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/messages | where (category==""license_window"" OR category==""pool_over_quota"") AND create_time >= now() - (30 * 86400) | rename pool_id AS pool | eval warning_day=if(category==""pool_over_quota"",""("".strftime(create_time,""%B %e, %Y"")."")"",strftime(create_time-43200,""%B %e, %Y"")) | fields pool warning_day | join outer pool [rest splunk_server=$splunk_server$ /services/licenser/slaves | mvexpand active_pool_ids | eval slave_name=label | eval pool=active_pool_ids | fields pool slave_name | stats values(slave_name) as ""members"" by pool] | join outer pool [rest splunk_server=$splunk_server$ /services/licenser/pools | eval pool=title | eval quota=if(isnull(effective_quota),quota,effective_quota) | eval quotaGB=round(quota/1024/1024/1024,3) | fields pool stack_id, quotaGB] | stats first(pool) as ""Pool"" first(stack_id) as ""Stack ID"" first(members) as ""Current Members"" first(quotaGB) as ""Curent Quota (GB)"" values(warning_day) AS ""Warning Days - (Soft)/Hard"" by pool | fields - pool
          ",,,"license_usage_today"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/licenser/slaves | mvexpand active_pool_ids | where warning_count>0 | eval pool=active_pool_ids | join type=outer pool [rest splunk_server=$splunk_server$ /services/licenser/pools | eval pool=title | fields pool stack_id] | eval in_violation=if(warning_count>4 OR (warning_count>2 AND stack_id==""free""),""yes"",""no"") | fields label, title, pool, warning_count, in_violation | fields - _timediff | rename label as ""Slave"" title as ""GUID"" pool as ""Pool"" warning_count as ""Hard Warnings"" in_violation AS ""In Violation?""
          ",,,"license_usage_today"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) $severity$ | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)"" | fillnull severity value=""UNDEFINED"" | timechart count(severity) as count by severity",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)"" | fillnull value=""undefined"" vendor_severity | stats sparkline count by severity | sort -count",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal (sourcetype=lookup_editor_rest_handler OR sourcetype=lookup_backups_rest_handler) $severity$
          | rex field=_raw ""(?&lt;severity&gt;(DEBUG)|(ERROR)|(WARNING)|(INFO)|(CRITICAL)) (?&lt;message&gt;.*)""
          | sort -_time
          | eval time=_time
          | convert ctime(time)
          | table time severity message",,"sourcetype=lookup_editor_rest_handler
sourcetype=lookup_backups_rest_handler","lookup_editor_logs"
tbd,,,,,,"lookup_editor",,"| stats count as value | eval value=""Offline"" | append [rest /services/data/lookup_edit/ping | fields value] | stats last(value) as status | eval range=if(status==""Offline"", ""severe"", ""low"")",,,"lookup_editor_status"
tbd,,,,,,"lookup_editor",,"| stats count as value | eval value=""Offline"" | append [rest /services/data/lookup_backup/ping | fields value] | stats last(value) as status | eval range=if(status==""Offline"", ""severe"", ""low"")",,,"lookup_editor_status"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal sourcetype=lookup_editor_rest_handler | timechart count",,"sourcetype=lookup_editor_rest_handler","lookup_editor_status"
tbd,,,"index=_internal",,,"lookup_editor",,"index=_internal sourcetype=lookup_backups_rest_handler | timechart count",,"sourcetype=lookup_backups_rest_handler","lookup_editor_status"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=""secure_gateway_app_internal_log"" ""Dashboard Source Subscription Created"" | eval temp=split(dashboard_id, ""/"") 
| eval dashboard_user=mvindex(temp,0)| eval dashboard_app=mvindex(temp,1) 
| eval dashboard_name=mvindex(temp,2) | fields _time, search_key, request_id, device_id, current_user, dashboard_id, shard_id, is_alert, subscription_id, search_type_id, dashboard_user, dashboard_app, dashboard_name",,"sourcetype=secure_gateway_app_internal_log","recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| rest ""servicesNS/$dashboardUser$/$dashboardApp$/data/ui/views/$dashboardName$?output_mode=json"" 
| fields label, name, tags",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| rest ""services/ssg/kvstore_client"" 
method=get
app=splunk_secure_gateway
collection=registered_devices
owner=$currentUser$
sort=_key
limit=1000
post_data=""{}"" | spath input=result path={}. output=data 
| mvexpand data
| spath input=data 
| search device_id=""$deviceId$""
| fields app_id, app_name, auth_method, device_management_method, device_name, device_registered_timestamp, device_type, platform, registration_method, device_id, user",,,"recent_ssg_dashboard_activity"
tbd,,,"index=_internal
index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=""secure_gateway_app_internal_log"" [search index=_internal sourcetype=""secure_gateway_app_internal_log"" ""Dashboard Source Subscription Created"" dashboard_id=""$dashboardId$"" | dedup subscription_id | fields subscription_id | format] | fields log_level subscription_id",,"sourcetype=secure_gateway_app_internal_log
sourcetype=secure_gateway_app_internal_log","recent_ssg_dashboard_activity"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=""secure_gateway_app_internal_log"" subscription_id=$subscriptionId$ | fields log_level, type, _time",,"sourcetype=secure_gateway_app_internal_log","recent_ssg_dashboard_activity"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=secure_gateway_app_internal_log subscription_id=$subscriptionId$ | transaction update_id startswith=(""Sending subscription update"" OR ""Start Post Process Single Subscription Update"") endswith=(""Subscription Update Sent"" OR ""Send Post Process Single Subscription Update"") | eval latency = duration | eval dispatch_state_enum=case(dispatch_state==0, ""NONE"", dispatch_state==1, ""QUEUED"", dispatch_state==2, ""PARSING"", dispatch_state==3, ""RUNNING"", dispatch_state==4, ""FINALIZING"", dispatch_state==5, ""DONE"", dispatch_state==6, ""FAILED"", dispatch_state==7, ""PAUSED"") | table _time, update_id, type, latency, done_progress, dispatch_state_enum",,"sourcetype=secure_gateway_app_internal_log","recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search ""Finished process_unsubscribe_request"" | fields count",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| stats latest(_time) as _time, values(subscription_id) as subscription_ids, values(search_key) as search_keys by dashboard_user dashboard_app dashboard_name dashboard_id device_id current_user | stats count(search_keys) as search_count, latest(_time) as _time by dashboard_user dashboard_app dashboard_name dashboard_id device_id current_user| sort - _time",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table label",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table tags",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table user",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| eval device_name_str = if(device_name=="""", ""None"", device_name)| table device_name_str",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table device_type",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| eval device = substr(device_id, 0, 6) | table device",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table platform",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table auth_method",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| table registration_method",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| eval mdm = if(device_management_method == ""not_mdm"", ""NO"", ""YES"") | table mdm",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"|search log_level=ERROR | stats count as error_count | eval errors = if(error_count &gt; 0, ""YES"", ""NONE"") | eval range=if(error_count &gt; 0, ""severe"", ""low"") | table errors, range",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search log_level=ERROR | stats count by subscription_id",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search dashboard_id=""$dashboardId$"" device_id=""$deviceId$"" current_user=""$currentUser$"" | table _time, search_type_id, subscription_id, search_key, request_id",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search log_level=ERROR | stats count as error_count | eval errors = if(error_count &gt; 0, ""YES"", ""NONE"") | eval range=if(error_count &gt; 0, ""severe"", ""low"") | table errors, range",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search ""Subscription Update Sent"" OR ""Skipping subscription update"" OR ""Send Post Process Single Subscription Update"" | head 1 | table type",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| stats count as count
                 | eval ok = if(count &gt; 0, ""NO"", ""YES"")
                 | eval range=if(count &gt; 0, ""severe"", ""low"")
                 | table ok, range
          ",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"|stats count| table count",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| search ""Skipping subscription update"" | stats count | table count",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| stats avg(latency) as avg_latency by update_id | table avg_latency",,,"recent_ssg_dashboard_activity"
tbd,,,,,,"splunk_secure_gateway",,"| stats max(_time) as maxtime min(_time) as mintime | eval difference=maxtime-mintime | table difference",,,"recent_ssg_dashboard_activity"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
      index=_internal sourcetype=secure_gateway_app_internal_log | transaction request_id startswith=""Incoming message"" endswith=""message=SENT_BACK"" | eval latency = duration | fields request_id, type, current_user, device_id, latency
    ",,"sourcetype=secure_gateway_app_internal_log","request_tracing_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=secure_gateway_app_internal_log request_id=$requestId$ | fields log_level, count, time_taken",,"sourcetype=secure_gateway_app_internal_log","request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| table _time, request_id, type, current_user, device_id, latency",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search log_level=ERROR | stats count as error_count | eval errors = if(error_count &gt; 0, ""YES"", ""NONE"") | eval range=if(error_count &gt; 0, ""severe"", ""low"") | table errors, range",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search request_id=$requestId$ | table type",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search request_id=$requestId$ | table current_user",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search request_id=$requestId$ | eval device = substr(device_id, 0, 6) | table device",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"  | rex field=_raw ""(?&lt;uri&gt;(GET|POST|DELETE) uri=https.*?), "" | chart sum(time_taken) over uri",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search request_id=$requestId$ | table latency",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| bucket time_taken | stats count by time_taken",,,"request_tracing_dashboard"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_cpu_deployment"
tbd,,,,,"lookup dmc_assets","splunk_monitoring_console",,"
      | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/hostwide
      | lookup dmc_assets serverName AS splunk_server OUTPUT search_group
      | where $role_post_filter$
      | join type=outer splunk_server [
          | `dmc_get_instance_roles` 
          | fields role serverName
          | rename serverName as splunk_server]
      | eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
      | eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
      | eval cpu_usage = 'cpu_system_pct' + 'cpu_user_pct'
      | fields splunk_server, role, normalized_load_avg_1min, core_info, cpu_usage, cpu_count, virtual_cpu_count, cpu_system_pct, cpu_user_pct
      | eval role = replace(role, "" $"", """")
      | eval role = split(role, "" "")
    ",,,"resource_usage_cpu_deployment"
tbd,,,,,"lookup dmc_assets","splunk_monitoring_console",,"
      <![CDATA[
      | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
      | lookup dmc_assets serverName AS splunk_server OUTPUT search_group
      | where $role_post_filter$
      | fields normalized_pct_cpu pct_cpu process process_type search_props.mode search_props.type search_props.provenance splunk_server
      | join type=outer splunk_server [
          | `dmc_get_instance_roles`
          | fields role serverName
          | `dmc_get_primary_role`
          | rename serverName AS splunk_server]
      | fillnull normalized_pct_cpu pct_cpu
      | eval process_type_l2 = if(match(process_type, ""^search$""), 'process_type'."":"".'search_props.type', 'process_type')
      | `dmc_pretty_print_role(primary_role)`
      | stats sum(normalized_pct_cpu) AS normalized_pct_cpu by process_type_l2, splunk_server, primary_role
      | chart avg(normalized_pct_cpu) AS normalized_pct_cpu over primary_role by process_type_l2
      | foreach * [eval <<FIELD>> = if(isnum('<<FIELD>>'), round('<<FIELD>>', 2), '<<FIELD>>')]
      ]]>
    ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" $role_post_filter$ sourcetype=splunk_resource_usage component=Hostwide
      | eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" $role_post_filter$ sourcetype=splunk_resource_usage component=PerProcess
      | `dmc_rename_introspection_fields`
      | `dmc_classify_processes`
      | `dmc_resource_usage_by_processes_timechart(normalized_pct_cpu, $funcPerpCPU$)`
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats sum(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_get_primary_role`
            | stats sum(virtual_cpu_count) by primary_role
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""search_head""
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""search_head""
            | stats median(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats dc(splunk_server) AS instance_count by role
            | where role = ""search_head""
            | fields instance_count
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""indexer""
            | stats avg(cpu_usage) AS cpu_usage
            | eval cpu_usage = round(cpu_usage, 2).""%""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | where role = ""indexer""
            | stats median(virtual_cpu_count)
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats dc(splunk_server) AS instance_count by role
            | where role = ""indexer""
            | fields instance_count
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | fields splunk_server, role, core_info, cpu_usage, normalized_load_avg_1min
            | `dmc_pretty_print_role(role)`
            | sort - cpu_usage
            | rename splunk_server AS Instance, role as Role, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)""
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | `dmc_set_bin`
            | stats latest(total_cpu_usage) as dedup_total_cpu_usage by host _time
            | `dmc_timechart` $avgCPUFunc$(dedup_total_cpu_usage) as cpu_usage
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | timechart minspan=10s bins=200 partial=f $countCPUFunc$(total_cpu_usage) as cpu_usage by host
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | eval server = host
            | `dmc_set_bin_for_timechart`
            | stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
            | `dmc_cpu_usage_rangemap_and_timechart`
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_drilldown_resource_usage_cpu_deployment_usage(""$role$"", ""$group$"", $role_post_filter$, $countCPUFunc$, $drilldown_cpu_usage_metric$)`
          ",,,"resource_usage_cpu_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/hostwide
      | join type=outer splunk_server [
          | `dmc_get_instance_roles` 
          | fields role serverName
          | rename serverName as splunk_server]
      | eventstats min(eval(if(isnull(normalized_load_avg_1min), ""0"", ""1""))) as _load_avg_full_availability
      | eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
      | eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
      | eval cpu_usage = 'cpu_system_pct' + 'cpu_user_pct'
      | fields splunk_server, role, normalized_load_avg_1min, core_info, cpu_usage, cpu_count, virtual_cpu_count, cpu_system_pct, cpu_user_pct
      | eval role = replace(role, "" $"", """")
      | eval role = split(role, "" "")
    ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
      | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
      | fields normalized_pct_cpu pct_cpu process process_type search_props.mode search_props.type search_props.provenance splunk_server
      | fillnull normalized_pct_cpu pct_cpu
      | eval process_type_l2 = if(match(process_type, ""^search$""), 'process_type'."":"".'search_props.type', 'process_type')
      | stats sum(normalized_pct_cpu) AS normalized_pct_cpu by process_type_l2, splunk_server
      | chart avg(normalized_pct_cpu) AS normalized_pct_cpu over splunk_server by process_type_l2
      | foreach * [eval <<FIELD>> = if(isnum('<<FIELD>>'), round('<<FIELD>>', 2), '<<FIELD>>')]
      ]]>
    ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess
      | `dmc_rename_introspection_fields`
      | `dmc_classify_processes`
      | `dmc_resource_usage_by_processes_timechart(normalized_pct_cpu, $funcPerpCPU$)`
    ",,"sourcetype=splunk_resource_usage","resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | fields splunk_server, role, core_info, cpu_usage, normalized_load_avg_1min
            | `dmc_pretty_print_role(role)`
            | sort - cpu_usage
            | rename splunk_server AS Instance, role as Role, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)""
          ",,,"resource_usage_cpu_instance"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role$)` | search search_group!=""dmc_group_*""",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/hostwide
| join type=outer splunk_server [
  | rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/iostats
  | eval iops = round(reads_ps + writes_ps)
  | eval iops_mountpoint = iops."" ("".mount_point."")""
  | eval cpupct_mountpoint = cpu_pct.""% ("".mount_point."")""
  | stats values(iops_mountpoint) as iops_mountpoint, values(cpupct_mountpoint) as cpupct_mountpoint by splunk_server]
| eventstats min(eval(if(isnull(normalized_load_avg_1min), ""0"", ""1""))) as _load_avg_full_availability
| eval normalized_load_avg_1min = if(isnull(normalized_load_avg_1min), ""N/A"", normalized_load_avg_1min)
| eval core_info = if(isnull(cpu_count), ""N/A"", cpu_count)."" / "".if(isnull(virtual_cpu_count), ""N/A"", virtual_cpu_count)
| eval cpu_usage = cpu_system_pct + cpu_user_pct
| eval mem_used_pct = round(mem_used / mem * 100 , 2)
| eval mem_used = round(mem_used, 0)
| eval mem = round(mem, 0)
| fields splunk_server, normalized_load_avg_1min, core_info, cpu_usage, mem, mem_used, mem_used_pct, iops_mountpoint, cpupct_mountpoint
| sort - cpu_usage, -mem_used
| rename splunk_server AS Instance, normalized_load_avg_1min AS ""Load Average"", core_info AS ""CPU Cores (Physical / Virtual)"", cpu_usage AS ""CPU Usage (%)"", mem AS ""Physical Memory Capacity (MB)"", mem_used AS ""Physical Memory Usage (MB)"", mem_used_pct AS ""Physical Memory Usage (%)"", iops_mountpoint as ""I/O Operations per second (Mount Point)"", cpupct_mountpoint as ""Storage I/O Saturation (Mount Point)""
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countLoadAvgFunc$(data.normalized_load_avg_1min) as load_average by server _time
                    | `dmc_load_average_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countLoadAvgFunc$(data.normalized_load_avg_1min) as load_average by server _time
| `dmc_load_average_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_load_average(""$role$"", ""$group$"", $countLoadAvgFunc$, $drilldown_load_average_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
|`dmc_timechart` partial=f limit=25 $countLoadAvgFunc$(data.normalized_load_avg_1min) AS load_average by host
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval server = host
| `dmc_set_bin`
| stats latest(data.normalized_load_avg_1min) as dedup_load_average by server _time
| `dmc_timechart` $loadAvgFunc$(dedup_load_average) as load_average
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
                    | `dmc_cpu_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct' 
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countCPUFunc$(total_cpu_usage) as cpu_usage by server _time
| `dmc_cpu_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_cpu_usage(""$role$"", ""$group$"", $countCPUFunc$, $drilldown_cpu_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = 'data.cpu_system_pct' + 'data.cpu_user_pct'
|`dmc_timechart` partial=f limit=25 $countCPUFunc$(total_cpu_usage) AS cpu_usage by host
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval total_cpu_usage = ('data.cpu_system_pct' + 'data.cpu_user_pct')
| eval server = host
| `dmc_set_bin`
| stats latest(total_cpu_usage) as dedup_total_cpu_usage by server _time
| `dmc_timechart` $avgCPUFunc$(dedup_total_cpu_usage) as cpu_usage
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
                    | eval pct_mem_used = 'data.mem_used' / 'data.mem'
                    | eval server = host
                    | `dmc_set_bin_for_timechart`
                    | stats $countMemFunc$(pct_mem_used) as pct_mem_used by server _time
                    | `dmc_memory_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_used = 'data.mem_used' / 'data.mem' 
| eval server = host 
| `dmc_set_bin_for_timechart`
| stats $countMemFunc$(pct_mem_used) as pct_mem_used by server _time
| `dmc_memory_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_memory_usage(""$role$"", ""$group$"", $countMemFunc$, $drilldown_memory_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_used = 'data.mem_used' / 'data.mem' * 100
| eval server = host
| `dmc_set_bin_for_timechart`
| `dmc_timechart` partial=f limit=25 $countMemFunc$(pct_mem_used) AS pct_mem_used by server
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=Hostwide
| eval pct_mem_usage = 'data.mem_used' / 'data.mem' * 100 
| eval server = host 
| `dmc_set_bin`
| stats latest(pct_mem_usage) as dedup_pct_mem_usage by server _time 
| `dmc_timechart` $avgMemFunc$(dedup_pct_mem_usage) as pct_mem_usage
          ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage component=IOStats
      | eval mount_point = 'data.mount_point'
      | eval cpu_pct = 'data.cpu_pct'
      | eval host_mountpoint = host."":"".mount_point
      | `dmc_set_bin_for_iostats`
    ",,"sourcetype=splunk_resource_usage","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint, _time
                    | `dmc_iostats_rangemap(cpu_pct)`
                    | `dmc_timechart_for_iostats` dc(host_mountpoint) as count by range
                    | fields _time, ""80-100%"", ""60-80%"", ""0-60%"", ""abnormal""
                ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint, _time
            | `dmc_iostats_rangemap(cpu_pct)`
            | `dmc_timechart_for_iostats` dc(host_mountpoint) as count by range
            | fields _time, ""80-100%"", ""60-80%"", ""0-60%"", ""abnormal""
          ",,,"resource_usage_deployment"
tbd,,"host = replace",,,,"splunk_monitoring_console",,"
            stats $countIOstatsFunc$(cpu_pct) as cpu_pct by host_mountpoint
            | `dmc_iostats_rangemap(cpu_pct)`
            | where range==$drilldown_iostats_metric|s$
            | eval host = replace(host_mountpoint, "":.*"", """")
            | eval mount_point = replace(host_mountpoint, "".*:"", """")
            | `dmc_drilldown_join_peers_by_peerURI`
            | eval Action = Action."" $role$""
            | fields serverName, machine, mount_point, cpu_pct, range, numberOfCores, ram, version, Action
            | rename serverName as Instance, machine as Machine, mount_point as ""Mount Point"", cpu_pct as ""I/O Bandwidth Utilizatio (%)"", range as ""Storage I/O Saturation Range"", version as Version, numberOfCores as Cores, ram as RAM
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` limit=25 $countIOstatsFunc$(cpu_pct) by host_mountpoint
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` $aggIOstatsFunc$(cpu_pct) as cpu_pct
          ",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
                    | eval mount_point = 'data.mount_point'
                    | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
                    | eval pct_disk_usage = 1 - free / 'data.capacity'
                    | `dmc_set_bin_for_timechart_for_disk_usage`
                    | eval server_mount_point = host."":"".mount_point
                    | stats $countDiskFunc$(pct_disk_usage) as pct_disk_usage by server_mount_point _time
                    | `dmc_disk_usage_rangemap_and_timechart`
                ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = 1 - free / 'data.capacity'
            | `dmc_set_bin_for_timechart_for_disk_usage`
            | eval server_mount_point = host."":"".mount_point
            | stats $countDiskFunc$(pct_disk_usage) as pct_disk_usage by server_mount_point _time
            | `dmc_disk_usage_rangemap_and_timechart`
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_resource_usage_deployment_disk_usage(""$role$"", ""$group$"", $countDiskFunc$, $drilldown_disk_usage_metric$)`",,,"resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = (1 - free / 'data.capacity') * 100
            | `dmc_set_bin_for_timechart_for_disk_usage`
            | eval server_mount_point = host."":"".mount_point
            | `dmc_timechart` partial=f limit=25 $countDiskFunc$(pct_disk_usage) AS pct_disk_usage by server_mount_point
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_disk_objects component=Partitions
            | eval server = host
            | eval mount_point = 'data.mount_point'
            | eval free = if(isnotnull('data.available'), 'data.available', 'data.free')
            | eval pct_disk_usage = (1 - free / 'data.capacity') * 100
            | `dmc_set_bin_for_disk_usage`
            | stats latest(pct_disk_usage) as dedup_pct_disk_usage by server mount_point _time
            | `dmc_timechart_for_disk_usage` $avgDiskFunc$(dedup_pct_disk_usage) as pct_disk_usage
          ",,"sourcetype=splunk_disk_objects","resource_usage_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups`
          | search search_group=""dmc_group_*""
        ",,,"resource_usage_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"resource_usage_instance"
tbd,,,,"inputlookup dmc_assets",,"splunk_monitoring_console",,"
| inputlookup dmc_assets
| search machine=$machine$
| mvcombine search_group
| join type=outer peerURI
  [| rest splunk_server=local /services/search/distributed/peers
   | rename title as peerURI]
| join type=outer peerURI [|rest splunk_server=local /services/server/info
  | eval peerURI = ""localhost""
  | eval status = ""Up""
  | fields peerURI, status, version]
| join type=outer peerURI [|rest splunk_server=local /services/server/settings
  | eval peerURI = ""localhost""
  | fields peerURI, mgmtHostPort]
| join peerURI
  [| `dmc_get_instance_roles` ]
| eval status = if(status == ""Up"", status, ""Unreachable"")
| makemv role
| fields serverName, peerURI, role, version, status, mgmtHostPort
| eval peerURI = if(peerURI == ""localhost"", ""localhost:"".mgmtHostPort, peerURI)
| fields - mgmtHostPort
| rename serverName as Instance, peerURI as URI, status as ""Status"", role as Role, version as ""Version""
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$splunk_server$ /services/server/status/resource-usage/hostwide 
| stats first(normalized_load_avg_1min) as load_average first(cpu_system_pct) as system, first(cpu_user_pct) as user first(mem) AS mem first(mem_used) AS mem_used by splunk_server
        ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"fields load_average",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"fields splunk_server system user",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
fields mem mem_used
| eval perc_80=mem*0.8 
| eval perc_90=mem*0.9 
| eval mem_used=round(mem_used, 0) 
| gauge mem_used 0 perc_80 perc_90, mem
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$instanceDiskUsageSnapshot$ /services/server/status/partitions-space
| join type=outer splunk_server, mount_point [
  | rest splunk_server=$instanceDiskUsageSnapshot$ /services/server/status/resource-usage/iostats
  | eval iops = round(reads_ps + writes_ps)
  | fields splunk_server, mount_point, iops, cpu_pct]
| eval free = if(isnotnull(available), available, free)
| eval usage = round((capacity - free) / 1024, 2)
| eval capacity = round(capacity / 1024, 2)
| eval compare_usage = usage."" / "".capacity
| eval pct_usage = round(usage / capacity * 100, 2) 
| stats first(fs_type) as fs_type first(compare_usage) as compare_usage first(pct_usage) as pct_usage, first(iops) as iops, first(cpu_pct) as cpu_pct by mount_point
| rename mount_point as ""Mount Point"", fs_type as ""File System Type"", compare_usage as ""Disk Usage (GB)"", capacity as ""Capacity (GB)"", pct_usage as ""Disk Usage (%)"", iops as ""I/O operations per second"", cpu_pct as ""Storage I/O Saturation(%)""
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` sourcetype=splunk_resource_usage component=IOStats host=$host$
          | eval mount_point = 'data.mount_point'
          | eval reads_ps = 'data.reads_ps'
          | eval writes_ps = 'data.writes_ps'
          | eval interval = 'data.interval'
          | eval op_count = (reads_ps + writes_ps) * interval
          | eval avg_service_ms = 'data.avg_service_ms'
          | eval avg_wait_ms = 'data.avg_total_ms'
          | eval cpu_pct = 'data.cpu_pct'
          | eval network_pct = 'data.network_pct'
        ",,"sourcetype=splunk_resource_usage","resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            | search data.mount_point=$io_mount_point|s$
            | `dmc_timechart_for_iostats` per_second(op_count) as iops, avg(data.cpu_pct) as avg_cpu_pct, avg(data.avg_service_ms) as avg_service_ms, avg(data.avg_total_ms) as avg_wait_ms, avg(data.network_pct) as avg_network_pct
            | eval iops = round(iops)
            | eval avg_cpu_pct = round(avg_cpu_pct)
            | eval avg_service_ms = round(avg_service_ms)
            | eval avg_wait_ms = round(avg_wait_ms)
            | eval avg_network_pct = round(avg_network_pct)
            | fields _time, iops $io_overlay$
            | rename $io_overlay$ as $io_overlay_label|s$
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` per_second(op_count) as op_count by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_timechart_for_iostats` avg($io_perf_metric$) by mount_point
          ",,,"resource_usage_machine"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=t count values(All_Risk.risk_message) as risk_message values(All_Risk.risk_score) as risk_score values(All_Risk.savedsearch_description) as description values(All_Risk.annotations._all) as annotation values(source) as source values(All_Risk.src) as src values(All_Risk.user) as user values(All_Risk.threat_object) as threat_object from datamodel=Risk.All_Risk where All_Risk.risk_object=$field_rv$ groupby _time,All_Risk.risk_score,All_Risk.risk_message span=1s | table * | dedup _time risk_message ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
    | search NOT risk_score=0
    ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=t count as event_count from datamodel=Risk.All_Risk where All_Risk.risk_object=$field_rv$ groupby All_Risk.risk_object   ",,,"risk_investigation_dashboard"
tbd,,,"index=risk",,,"threat_object_fun",,"index=risk (NOT source=""/opt/*"" risk_object=""$field_rv$"")
| stats count values(source) as source dc(risk_message) as risk_messages by threat_object
| eventstats dc(threat_object) as dc_threats
",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
| eval risk_score = round(risk_score,0) , risk_message = risk_score."" - "".risk_message
| streamstats sum(risk_score) as score_original values(source) as sources values(risk_message) as risk_messages by risk_object
| dedup risk_message risk_score
| stats sum(risk_score) as risk_score",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
| makeresults | eval count = ""$event_count$"" | eval count = if(isnum(count),count,""0"")",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
| makeresults | eval count = ""$notable_count$"" | eval count = if(isnum(count),count,""0"")",,,"risk_investigation_dashboard"
tbd,,,,"inputlookup identity_lookup_expanded",,"threat_object_fun",,"| inputlookup identity_lookup_expanded | search identity=""$field_rv$"" | table first last identity bunit startDate endDate watchlist_name | eval startDate=strftime(startDate,""%Y-%m-%d"") , endDate=strftime(endDate,""%Y-%m-%d"")",,,"risk_investigation_dashboard"
tbd,,,,"inputlookup asset_lookup_by_str",,"threat_object_fun",,"| inputlookup asset_lookup_by_str | search nt_host=""$field_rv$"" | table wks_ip wks_usr wks_dom category city",,,"risk_investigation_dashboard"
tbd,,,,,"lookup update
lookup update
lookup update
lookup update
lookup update","threat_object_fun",,"
`get_notable_index` risk_object=$field_rv$
| eval indexer_guid=replace('_bkt',"".*~(.+)"",""\\1""), event_hash=md5(('_time' . '_raw')), event_id=((((indexer_guid . ""@@"") . index) . ""@@"") . event_hash), rule_id=event_id 
| eval latest=orig_time+1800 , earliest=orig_time-1800
| search event_id=""*"" 
| fields - ""host_*"" 
| tags outputfield=tag 
| eval tag=mvdedup(mvappend(tag,NULL,orig_tag)) 
| dedup rule_id 
| lookup update=true notable_xref_lookup event_id OUTPUTNEW xref_name as notable_xref_name,xref_id as notable_xref_id 
| eval notable_xref=mvzip(notable_xref_name,notable_xref_id,"":"") 
| eval temp_time=(time() + 86400) 
| lookup update=true event_time_field=temp_time incident_review_lookup rule_id OUTPUT owner as new_owner, urgency as new_urgency, status as new_status, disposition as new_disposition 
| lookup update=true event_time_field=temp_time incident_review_comment_lookup rule_id OUTPUT time as review_time,user as reviewer,comment 
| eval owner=if(isnotnull(new_owner),new_owner,owner), status=case(isnotnull(new_status),new_status,isnotnull(status),status,true(),default_status), urgency=if(isnotnull(new_urgency),new_urgency,urgency), disposition=if(isnotnull(new_disposition),new_disposition,default_disposition) 
| fields - temp_time, new_owner, new_status, new_urgency, new_disposition 
| eval temp_status=if(isnull(status),-1,status) 
| lookup update=true reviewstatuses_lookup _key as temp_status OUTPUT status,label as status_label,description as status_description,default as status_default,end as status_end 
| eval status=if(isnull(status_label),0,status), status_label=if(isnull(status_label),""Unassigned"",status_label), status_description=if(isnull(status_description),""unknown"",status_description), status_default=case(match(status_default,""1|[Tt]|[Tt][Rr][Uu][Ee]""),""true"",match(status_default,""0|[Ff]|[Ff][Aa][Ll][Ss][Ee]""),""false"",true(),status_default), status_end=case(match(status_end,""1|[Tt]|[Tt][Rr][Uu][Ee]""),""true"",match(status_end,""0|[Ff]|[Ff][Aa][Ll][Ss][Ee]""),""false"",true(),status_end), status_group=case((status_default == ""true""),""New"",(status_end == ""true""),""Closed"",(status == 0),""New"",true(),""Open"") 
| fields - temp_status 
| eval temp_disposition=if(isnull(disposition),-3,disposition) 
| lookup update=true disposition_lookup _key as temp_disposition OUTPUT status as disposition,label as disposition_label,description as disposition_description,default as disposition_default 
| eval disposition=if(isnull(disposition),""disposition:0"",disposition), disposition_label=if(isnull(disposition_label),""Unassigned"",disposition_label), disposition_description=if(isnull(disposition_description),""An error is preventing the event from having a valid disposition."",disposition_description), disposition_default=case(match(disposition_default,""1|[Tt]|[Tt][Rr][Uu][Ee]""),""true"",match(disposition_default,""0|[Ff]|[Ff][Aa][Ll][Ss][Ee]""),""false"",true(),disposition_default) 
| fields - temp_disposition 
| table _time, orig_time, search_name, orig_source, status_label, status_end, comment, owner, reviewer, risk_object, risk_object_type, risk_messages, src, user, dest, rule_id, event_hash, earliest, latest
| eventstats count as notable_count count(eval(status_label=""Incident"")) as incident_count | eval incident_count = if(isnull(incident_count),""0"",incident_count) | rename status_label as status
| eval ir_link = ""https://i-0f475f545da6fbdc2.splunk.show/en-US/app/SplunkEnterpriseSecuritySuite/incident_review?earliest="".earliest.""&amp;latest="".latest.""&amp;status=*&amp;urgency=*&amp;search=event_hash%3D"".event_hash
",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| stats count as Events sum(risk_score) as risk_sum dc(risk_message) as ""Distinct Events"" values(description) as Description values(annotation) as Annotation by source | rename source as Rule | eval risk_sum = round(risk_sum,0) | rename risk_sum as ""Risk Sum"" | sort - ""Risk Sum"" ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
  | rest splunk_server=local count=0 /services/saved/searches 
  | search title=""$risk_rule_drilldown$"" | rename dispatch.earliest_time as early_time
  | table title qualifiedSearch early_time
",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",," | stats count as count by _time,risk_score,risk_message  | timechart span=30min limit=0 values(risk_score) as risk_score by risk_message
        ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| makeresults 
    | eval count=""0"" | eval count= if(isint($addriskmsg_count$),""$addriskmsg_count$"",0)
    | fields count | table count | sort - count ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| makeresults 
    | eval count=""0"" | eval count= if(isint($dc_threats$),""$dc_threats$"",0)
    | fields count | table count | sort - count ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
            | eval risk_score = round(risk_score)
| eval risk_message = risk_score."" - "".risk_message
| table _time source risk_message risk_score threat_object threat_short
| sort + _time
| eventstats count(risk_message) as riskmsg_count
| search $search_filter$ | eventstats count(risk_message) as addriskmsg_count
          ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
             | where _time &gt; $selection.earliest$ AND _time &lt; $selection.latest$
          ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"
",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval num_objects = if(isint($num_objects$),""$num_objects$"",0) | table num_objects
          ",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval source_count = if(isint($source_count$),""$source_count$"",0) | table source_count",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| makeresults 
| eval risk_objects = if(isint($ro_count$),""$ro_count$"",0)
| table risk_objects",,,"risk_investigation_dashboard"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=t count values(All_Risk.risk_message) as risk_message values(All_Risk.risk_score) as risk_score values(source) as sources values(All_Risk.risk_object_type) as risk_object_type from datamodel=Risk.All_Risk where All_Risk.threat_object=""$threat_token$"" groupby All_Risk.risk_object _time span=30d | rename All_Risk.risk_object as risk_object | table * | eval risk_score = mvindex(risk_score,0) , risk_score=round(risk_score,0) , risk_message = mvindex(risk_message,0) , risk_message = risk_score."" - "".risk_message | eventstats dc(risk_object) as ro_count dc(sources) as source_count sum(count) as num_objects | rename risk_message AS sample_message ",,,"risk_investigation_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| rest services/ssg/registration/test_jwt dashboard_format=true ",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| makeresults | eval x = ""Test JWT Integration"" | fields x",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|rest services/ssg/registration/tokens_enabled dashboard_format=true
| fields tokens_enabled
| eval are_tokens_enabled=if( tokens_enabled=""1"",""Yes"",""No"" )
| eval range=if(are_tokens_enabled=""Yes"", ""low"", ""severe"")
| table are_tokens_enabled, range",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
| fields JWT
| eval result=if(isnull(JWT), ""Failed"", ""Success"")
| eval range=if(result=""Success"", ""low"", ""severe"")
| table  result, range",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| rest services/ssg/registration/test_jwt dashboard_format=true 
| fields token_expiration
| eval result=if(isnull(token_expiration), ""Error Occured"", token_expiration)
| eval range=if(result=""Error Occured"", ""severe"", ""low"")
| table  result, range",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
| fields error_type, message
| eval is_error = if(isnull(error_type), ""false"", ""true"")
| eval msg = if(is_error==""true"", error_type, ""No Error"")
| eval range=if(msg=""No Error"", ""low"", ""severe"")
| table msg, range, is_error",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
| fields error_type, message
| eval is_error = if(isnull(message), ""false"", ""true"")
| eval msg = if(is_error==""true"", message, ""No Error"")
| table msg",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| makeresults | eval x = ""Validate SAML Configuration"" | fields x",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|rest services/ssg/registration/correct_saml_setup dashboard_format=true
| fields attributeQueryRequestSigned, attributeQueryResponseSigned, attributeQuerySoapUsername, idpAttributeQueryUrl, scriptPath, scriptTimeout, getUserInfoTtl, useAuthExtForTokenAuthOnly,scriptFunctions, scriptSecureArguments, SAML
| eval attributeQueryRequestSigned=if(attributeQueryRequestSigned=0, ""false"", ""true"")
| eval attributeQueryResponseSigned=if(attributeQueryResponseSigned=0, ""false"", ""true"")
| eval attributeQuerySoapUsername=if(attributeQuerySoapUsername=0, ""false"", ""true"")
| eval idpAttributeQueryUrl=if(idpAttributeQueryUrl=0, ""false"", ""true"")
| eval scriptPath=if(scriptPath=0, ""false"", ""true"")
| eval scriptTimeout=if(scriptTimeout=0, ""false"", ""true"")
| eval getUserInfoTtl=if(getUserInfoTtl=0, ""false"", ""true"")
| eval useAuthExtForTokenAuthOnly=if(useAuthExtForTokenAuthOnly=0, ""false"", ""true"")
| eval scriptFunctions=if(scriptFunctions=0, ""false"", ""true"")
| eval scriptSecureArguments=if(scriptSecureArguments=0, ""false"", ""true"")
| eval SAML=if(SAML=0, ""false"", ""true"")",,,"saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| makeresults | eval x = ""SAML Error Logs"" | fields x",,,"saml_troubleshooting_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=splunkd ERROR AuthenticationProviderSAML",,"sourcetype=splunkd","saml_troubleshooting_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=splunkd ERROR UserManagerPro",,"sourcetype=splunkd","saml_troubleshooting_dashboard"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group=""dmc_customgroup_*"" OR search_group=""dmc_indexerclustergroup_*"" OR search_group=""dmc_searchheadclustergroup_*""
        ",,,"scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /servicesNS/-/-/saved/searches preserve_args_case=t noProxy=t search=""is_scheduled=1"" search=""disabled=0""
            | stats count by splunk_server
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/status/limits/search-concurrency
              | fields splunk_server max_hist_scheduled_searches, max_rt_scheduled_searches]
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
              | search search_props.role=""head"" `dmc_match_all_scheduled_search_types`
              | dedup search_props.sid
              | stats count(eval('search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical"")) as count_hist_search, count(eval('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")) as count_rt_search by splunk_server ]
            | join splunk_server type=outer [
              | rest splunk_server_group=dmc_group_search_head splunk_server_group=""$group$"" /services/server/info
              | fields splunk_server, numberOfCores, numberOfVirtualCores]
            | eval count_hist_search = if(isnull(count_hist_search), 0, count_hist_search)
            | eval count_rt_search = if(isnull(count_rt_search), 0, count_rt_search)
            | eval hist_concur_vs_limit = count_hist_search."" / "".max_hist_scheduled_searches
            | eval rt_concur_vs_limit = count_rt_search."" / "".max_rt_scheduled_searches
            | `dmc_get_core_info`
            | fields splunk_server, core_info, hist_concur_vs_limit, rt_concur_vs_limit, count
            | rename splunk_server as Instance, core_info AS ""CPU Cores (Physical / Virtual)"", hist_concur_vs_limit as ""Concurrency of Historical Scheduled Report (Running/Limit)"", rt_concur_vs_limit as ""Concurrency of Real-time Scheduled Report (Running/Limit)"", count as ""Unique Scheduled Reports""
          ",,,"scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | timechart partial=f limit=20 count by $scheduler_execution_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | bin _time minspan=1min
            | stats count by host, _time
            | timechart partial=f $scheduler_execution_metric$
            | rename total as Total, min_count as Minimum, avg_count as Average, median_count as Median, max_count as Maximum
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats dc(sid) AS distinct_search_count by _time, host
            | `dmc_timechart` partial=f limit=20 $agg_report_concurrency_by_instance$(distinct_search_count) AS agg_distinct_search_count by host
            | eval agg_distinct_search_count = round(agg_distinct_search_count, 0)
            | rename agg_distinct_search_count as ""Concurrent Report Count""
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats dc(sid) AS distinct_search_count by _time, host
            | stats sum(distinct_search_count) as count by _time
            | `dmc_timechart` partial=f $agg_report_concurrency_all_instance$
            | rename min_count as Minimum, avg_count as Average, median_count as Median, max_count as Maximum
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | stats count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec by _time, host, app, savedsearch_name, user, savedsearch_id
            | `dmc_timechart` partial=f limit=20 eval(round(sum(skipped_exec) / sum(total_exec) * 100, 2)) as skip_ratio by host
            | rename skip_ratio as ""Skip Ratio""
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
            | stats count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec by _time, savedsearch_id
            | `dmc_timechart` partial=f eval(round(sum(skipped_exec) / sum(total_exec) * 100, 2)) as skip_ratio
            | rename skip_ratio as ""Skip Ratio""
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler status=""completed""
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart partial=f limit=20 eval(round(avg(execution_latency), 0)) as latency by $exec_lat_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=scheduler status=""completed""
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart partial=f $exec_lat_agg$
          ",,"sourcetype=scheduler","scheduler_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group!=""dmc_group_*""
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/server/info
            | eval core_info = if(isnull(numberOfCores), ""N/A"", numberOfCores).""/"".if(isnull(numberOfVirtualCores), ""N/A"", numberOfVirtualCores)
            | fields core_info
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
          | dedup search_props.sid
          | search `dmc_match_all_scheduled_search_types`
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          stats count(eval('search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical"")) as count_hist_scheduled_search, count(eval('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")) as count_rt_scheduled_search by splunk_server
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/limits/search-concurrency
          | fields max_hist_scheduled_searches, max_rt_scheduled_searches
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_search_limit = ""$count_hist_scheduled_search$"".""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_search_limit
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_search_limit = ""$count_rt_scheduled_search$"".""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_search_limit
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where 'search_props.mode'==""historical batch"" OR 'search_props.mode'==""historical""
            | fields search_props.name, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed, search_props.sid
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.name as ""Scheduled Report Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Spent (sec)"", search_props.sid as ""SID""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where 'search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""
            | fields search_props.name, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed, search_props.sid
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.name as ""Scheduled Report Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Spent (sec)"", search_props.sid as ""SID""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | table splunk_server eai:acl.app eai:acl.owner cron_schedule title scheduled_times
            | mvexpand scheduled_times
            | rename scheduled_times as _time eai:acl.app as app eai:acl.owner as user title as search
            | stats count
            | eval count = `dmc_convert_count_unit(count)`
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR status=""deferred"")
          | eval window_time = if(isnotnull(window_time), window_time, 0)
          | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
          | timechart span=1h partial=f avg(execution_latency) AS avg_exec_latency, count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec
          | eval skip_ratio = round(skipped_exec / total_exec * 100, 2)
          | eval avg_exec_latency = round(avg_exec_latency, 2)
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, skip_ratio",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"fields _time, avg_exec_latency",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" search=""is_scheduled=1"" search=""disabled=0""
          | fields title, eai:acl.app, eai:acl.owner, cron_schedule, dispatch.earliest_time, dispatch.latest_time,
          schedule_window, actions
        ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by eai:acl.app",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by eai:acl.owner",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            $inventory_app_filter$
            | $inventory_user_filter$
            | eval dispatch.earliest_time = if(isnull('dispatch.earliest_time') OR 'dispatch.earliest_time' == ""0"" OR
            'dispatch.earliest_time'=="""", ""not set"", 'dispatch.earliest_time')
            | eval dispatch.latest_time = if(isnull('dispatch.latest_time') OR 'dispatch.latest_time' == """", ""not set"",
            'dispatch.latest_time')
            | eval actions = if(isnull(actions) OR actions == """", ""none"", actions)
            | rename title as ""Report Name"", eai:acl.app as App, eai:acl.owner as User, cron_schedule as ""Cron
            Schedule"", dispatch.earliest_time as ""Earliest Time"", dispatch.latest_time as ""Latest Time"", schedule_window as ""Schedule Window (minutes)"", actions as Actions
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
          status=""deferred"")
          | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
          | stats count by $scheduler_execution_split_by$
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename status as Status, app as App, user as User, savedsearch_name as ""Report Name"", alert_actions as ""Alert Actions"", count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"")
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | timechart partial=f count by $scheduler_execution_timechart_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd group=""searchscheduler"" host=$host$
            | `dmc_timechart_for_metrics_log` sum(completed) as completed_count,
            $scheduler_completion_runtime_aggregation$(total_runtime) as total_runtime
            | eval total_runtime=round(total_runtime, 0)
          ",,"sourcetype=splunkd","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess data.search_props.role=""head"" data.search_props.mode=""$searchMode$"" data.search_props.sid::* `dmc_match_all_scheduled_search_types`
            | `dmc_rename_introspection_fields`
            | `dmc_set_bin`
            | stats dc(sid) AS distinct_search_count by _time, $concurrencySplitBy$
            | `dmc_timechart` $concurrencyFunction$(distinct_search_count) AS ""$concurrencyFunction$ of search
            concurrency"" by $concurrencySplitBy$
            | eval search_mode = ""$searchMode$""
            | eval scheduled_search_limit = case(match(search_mode, ""historical""), $max_hist_scheduled_searches$,
            match(search_mode, ""RT""), $max_rt_scheduled_searches$, true(), NULL)
            | fields - search_mode
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` host=$host$ sourcetype=splunk_resource_usage component=PerProcess data.search_props.sid::* data.search_props.role=""head"" `dmc_match_all_scheduled_search_types`
            | `dmc_set_bin`
            | `dmc_rename_introspection_fields`
            | stats max(elapsed) as ELAPSED by sid $runtimeSplitBy$ _time
            | streamstats current=t global=f window=2 earliest(ELAPSED) as prev_ELAPSED latest(ELAPSED) as curr_ELAPSED by sid $runtimeSplitBy$
            | eval `dmc_collection_interval`
            | eval delta_ELAPSED = curr_ELAPSED - prev_ELAPSED
            | eval runtime = if(delta_ELAPSED = 0, min(curr_ELAPSED, collection_interval), delta_ELAPSED)
            | `dmc_timechart` sum(runtime) by $runtimeSplitBy$
          ",,"sourcetype=splunk_resource_usage","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"")
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | stats avg(run_time) as runtime, avg(execution_latency) AS avg_exec_latency, count(eval(status==""completed"" OR status==""skipped"")) AS total_exec, count(eval(status==""skipped"")) AS skipped_exec count(eval(status==""deferred"")) AS deferred_exec by app, savedsearch_name, user, savedsearch_id
            | join savedsearch_id type=outer [
            | rest splunk_server=$splunk_server$ ""/servicesNS/-/-/saved/searches/"" earliest_time=`time_modifier(-0s@s)` latest_time=`time_modifier(+8d@d)` search=""is_scheduled=1"" search=""disabled=0""
            | search NOT (dispatch.earliest_time=rt* OR dispatch.latest_time=rt*)
            | stats dc(scheduled_times) as count max(scheduled_times) as max_t min(scheduled_times) as min_t by title, eai:acl.app, eai:acl.owner cron_schedule
            | eval schedule_interval=round((max_t-min_t)/(count-1), 0)
            | eval savedsearch_id = 'eai:acl.owner'."";"".'eai:acl.app'."";"".title
            | fields savedsearch_id, cron_schedule, schedule_interval ]
            | eval runtime = round(runtime, 0)
            | eval avg_exec_latency = round(avg_exec_latency, 0)
            | eval search_workload = round(runtime / schedule_interval * 100, 2)."" %""
            | eval skip_ratio = round(skipped_exec / total_exec * 100, 2)."" %""
            | fields savedsearch_name, app, user, cron_schedule, schedule_interval, runtime, search_workload, total_exec, skipped_exec, skip_ratio, deferred_exec, avg_exec_latency
            | sort - search_workload
            | rename savedsearch_name as ""Report Name"", app as App, user as User, cron_schedule as ""Cron Schedule"", runtime as ""Average Runtime (sec)"", total_exec as ""Total Executions"", skip_ratio as ""Skip Ratio"", skipped_exec as ""Skipped Executions"", deferred_exec AS ""Deferred Executions"", schedule_interval as ""Schedule Interval (sec)"", search_workload as ""Interval Load Factor"", avg_exec_latency AS ""Average Execution Latency (sec)""
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (status=""completed"" OR status=""skipped"" OR
            status=""deferred"") savedsearch_name=$runtime_statistics_drilldown|s$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""completed""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval window_time = if(isnotnull(window_time), window_time, 0)
            | eval execution_latency = max(dispatch_time - (scheduled_time + window_time), 0)
            | timechart eval(round($exe_lag_agg$(execution_latency), 0)) as latency by $exe_lag_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
          | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
          | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
          | stats count by $count_skipped_split_by$
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename reason as Reason, savedsearch_name as ""Report Name"", alert_actions as ""Alert Actions"", app as App, user as User, count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
            | timechart count by $count_skipped_timechart_split_by$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler status=""skipped""
            | eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
            | eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
            | stats count AS count values(alert_actions) AS alert_actions by savedsearch_name, reason
            | eval reason_and_count = reason."" ("".count."")""
            | stats values(reason_and_count) AS reasons first(alert_actions) AS alert_actions by savedsearch_name
            | rename reasons AS ""Skip Reason (Skip Count)"" alert_actions AS ""Alert Actions"" savedsearch_name AS ""Report
            Name""
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_internal` host=$host$ sourcetype=scheduler (log_level=""ERROR"" OR log_level=""WARN*"")
          | eval event_message=coalesce(event_message, message)
          | cluster t=0.7 field=event_message showcount=t countfield=count
          | table event_message, count, punct
        ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats sum(count) as total",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            sort - count
            | eventstats sum(count) AS total
            | eval percent = round(count / total * 100, 2)."" %""
            | fields - total
            | rename event_message as Message, count as Count, percent as ""Percent of Total""
          ",,,"scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$host$ sourcetype=scheduler (log_level=""ERROR"" OR log_level=""WARN*"") punct=$warningErrorPunct|s$
          ",,"sourcetype=scheduler","scheduler_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"| `dmc_get_groups_containing_role($role$)` | search search_group!=""dmc_group_*""",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/limits/search-concurrency
| join splunk_server type=outer [
| rest splunk_server_group=$role$ splunk_server_group=""$group$"" /services/server/status/resource-usage/splunk-processes
| eval search_pct_cpu  = if(isnotnull('search_props.sid'), pct_cpu, 0)
| eval search_mem_used = if(isnotnull('search_props.sid'), mem_used, 0)
| eventstats sum(search_pct_cpu) as search_pct_cpu, sum(search_mem_used) as search_mem_used by search_props.sid
| dedup search_props.sid
| stats dc(search_props.sid) as search_count, sum(search_pct_cpu) as sum_pct_cpu, sum(search_mem_used) as sum_mem_used, sum(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as sum_auto_summary_search, sum(eval('search_props.type'==""ad-hoc"" AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as sum_rt_search, sum(eval('search_props.type'==""ad-hoc"" AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as sum_hist_search, sum(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as sum_rt_scheduled_search, sum(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as sum_hist_scheduled_search by splunk_server]
| eval sum_pct_cpu  = round(sum_pct_cpu, 2)
| eval sum_mem_used = round(sum_mem_used, 2)
| sort -search_count, -sum_pct_cpu
| eval search_count = if(isnull(search_count) OR search_count=="""", 0, search_count)
| eval sum_auto_summary_search = if(isnull(sum_auto_summary_search) OR sum_auto_summary_search=="""", 0, sum_auto_summary_search)
| eval sum_hist_scheduled_search = if(isnull(sum_hist_scheduled_search) OR sum_hist_scheduled_search=="""", 0, sum_hist_scheduled_search)
| eval sum_hist_search = if(isnull(sum_hist_search) OR sum_hist_search=="""", 0, sum_hist_search)
| eval sum_rt_scheduled_search = if(isnull(sum_rt_scheduled_search) OR sum_rt_scheduled_search=="""", 0, sum_rt_scheduled_search)
| eval sum_rt_search = if(isnull(sum_rt_search) OR sum_rt_search=="""", 0, sum_rt_search)
| eval sum_pct_cpu = if(isnull(sum_pct_cpu) OR sum_pct_cpu=="""", 0, sum_pct_cpu)
| eval count_cpu = round(sum_pct_cpu / 100.0, 2)
| eval sum_mem_used = if(isnull(sum_mem_used) OR search_count=="""", 0, sum_mem_used)
| eval scheduled_vs_total_auto_summary_search_limit = sum_auto_summary_search.""/"".max_auto_summary_searches
| eval scheduled_vs_total_hist_scheduled_search_limit = sum_hist_scheduled_search.""/"".max_hist_scheduled_searches
| eval scheduled_vs_total_hist_search_limit = sum_hist_search.""/"".max_hist_searches
| eval scheduled_vs_total_rt_scheduled_search_limit = sum_rt_scheduled_search.""/"".max_rt_scheduled_searches
| eval scheduled_vs_total_rt_search_limit = sum_rt_search.""/"".max_rt_searches
| fields splunk_server, search_count, scheduled_vs_total_auto_summary_search_limit, scheduled_vs_total_hist_scheduled_search_limit, scheduled_vs_total_hist_search_limit, scheduled_vs_total_rt_scheduled_search_limit, scheduled_vs_total_rt_search_limit, count_cpu, sum_mem_used
| rename splunk_server as Instance, search_count as ""Count of Searches"", scheduled_vs_total_auto_summary_search_limit as ""Summarization"", scheduled_vs_total_hist_scheduled_search_limit as ""Historical Schedule Report"", scheduled_vs_total_hist_search_limit  as ""Historical Search"", scheduled_vs_total_rt_scheduled_search_limit as ""Real-time Schedule Report"",  scheduled_vs_total_rt_search_limit  as ""Real-time Search"", count_cpu as ""CPU Usage (# cores)"", sum_mem_used as ""Memory Usage (MB)""
          ",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
                    | `dmc_set_bin`
                    | stats dc(data.search_props.sid) AS distinct_search_count by host, _time
                    | bin _time minspan=10s
                    | stats $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host, _time
                    | `dmc_search_count_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats dc(sid) AS distinct_search_count by host, _time
          | bin _time minspan=10s
          | stats $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host, _time
          | `dmc_search_count_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_search_activity_deployment_search_concurrency(""$role$"", ""$group$"", $searchConcurrencyAggrFunc$, $drilldown_search_concurrency_metric$)`",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats dc(sid) AS distinct_search_count by host, _time
          | `dmc_timechart` partial=f limit=25 $searchConcurrencyAggrFunc$(distinct_search_count) as search_count by host
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,"host = round",,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=""$role$"" search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_set_bin`
          | stats dc(data.search_props.sid) AS distinct_search_count by host, _time
          | timechart minspan=10s bins=200 partial=f $aggrSearchConcurFunc$(distinct_search_count) AS distinct_search_count_per_host
          | eval distinct_search_count_per_host = round(distinct_search_count_per_host, 2)
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
                    `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
                    | `dmc_set_bin`
                    | eval data.search_props.sid = if(component==""Hostwide"", ""n/a"", 'data.search_props.sid')
                    | eval data.pid = if(component==""Hostwide"", ""n/a"", 'data.pid')
                    | eval data.$resourceType$ = if(component==""Hostwide"", 0, 'data.$resourceType$')
                    | stats latest(data.$resourceType$) AS resource_usage_dedup by _time, data.search_props.sid, data.pid, host
                    | stats sum(resource_usage_dedup) AS sum_resource_usage by _time, host
                    | eval sum_resource_usage = if(""data.$resourceType$"" == ""data.pct_cpu"", round(sum_resource_usage / 100.0, 2), sum_resource_usage)
                    | bin _time minspan=10s
                    | stats $resourceAggrFunc$(sum_resource_usage) as resource_usage by _time, host
                    | `dmc_$resourceType$_rangemap_and_timechart`
                ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | eval sid = if(component==""Hostwide"", ""n/a"", sid)
          | eval pid = if(component==""Hostwide"", ""n/a"", pid)
          | eval $resourceType$ = if(component==""Hostwide"", 0, $resourceType$)
          | stats latest($resourceType$) AS resource_usage_dedup by _time, sid, pid, host
          | stats sum(resource_usage_dedup) AS sum_resource_usage by _time, host
          | eval sum_resource_usage = if(""$resourceType$"" == ""pct_cpu"", round(sum_resource_usage / 100.0, 2), sum_resource_usage)
          | `dmc_set_bin_for_timechart`
          | stats $resourceAggrFunc$(sum_resource_usage) as resource_usage by _time, host
          | `dmc_$resourceType$_rangemap_and_timechart`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_search_activity_deployment_resource_usage(""$role$"", ""$group$"", $resourceType$, $resourceAggrFunc$, $drilldown_resource_usage_metric$)`",,,"search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
          | `dmc_rename_introspection_fields`
          | `dmc_set_bin`
          | stats latest($resourceType$) AS resource_usage by _time, sid, pid, host
          | eval resource_usage = if(""$resourceType$"" == ""pct_cpu"", round(resource_usage / 100.0, 2), resource_usage)
          | `dmc_timechart_for_metrics_log` partial=f limit=25 $resourceAggrFunc$(resource_usage) AS resource_usage by host
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_introspection` search_group=$role$ search_group=""$group$"" sourcetype=splunk_resource_usage data.search_props.sid::* data.search_props.mode!=RT
            | `dmc_rename_introspection_fields`
            | stats max(elapsed) as runtime max(mem_used) as mem_used earliest(_time) as _time by sid, label, provenance, type, mode, app, role, user, host
            | eval mem_used = round(mem_used, 2)
            | sort 20 - mem_used, runtime
            | fields sid, label, provenance, mem_used, host, runtime, _time, type, mode, app, user, role
            | eval _time=strftime(_time,""%+"")
            | rename sid as SID, label as Name, provenance as Provenance, mem_used as ""Memory Usage (MB)"", host as Instance, runtime as Runtime, _time as Started, type as Type, mode as Mode, app as App, user as User, role as Role
            | fieldformat Runtime = `dmc_convert_runtime(Runtime)`
          ",,"sourcetype=splunk_resource_usage","search_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/resource-usage/splunk-processes
          | where process_type = ""search""
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | stats count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_total_hist,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_total_rt,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_hist_scheduled_search,
          count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_rt_scheduled_search,
          count(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as count_auto_summary_search by splunk_server,      
          | eval count_total_hist = if(isnull(count_total_hist) OR count_total_hist=="""", 0, count_total_hist)
          | eval count_total_rt = if(isnull(count_total_rt) OR count_total_rt=="""", 0, count_total_rt)
          | eval count_hist_scheduled_search = if(isnull(count_hist_scheduled_search) OR count_hist_scheduled_search=="""", 0, count_hist_scheduled_search)
          | eval count_rt_scheduled_search = if(isnull(count_rt_scheduled_search) OR count_rt_scheduled_search=="""", 0, count_rt_scheduled_search)
          | eval count_auto_summary_search = if(isnull(count_auto_summary_search) OR count_auto_summary_search=="""", 0, count_auto_summary_search)
          | eval count_total_adhoc_scheduled_search = count_total_hist + count_total_rt
          | eval count_total_scheduled_search = count_hist_scheduled_search + count_rt_scheduled_search
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server=$splunk_server$ /services/server/status/limits/search-concurrency
        ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_historical_vs_limit = ""$count_total_hist$"".""/"".max_hist_searches
            | fields total_historical_vs_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_rt_vs_limit = ""$count_total_rt$"".""/"".max_rt_searches
            | fields total_rt_vs_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_scheduled_search_limit = ""$count_hist_scheduled_search$"".""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_scheduled_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_scheduled_search_limit = ""$count_rt_scheduled_search$"".""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_scheduled_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_auto_summary_search_limit = ""$count_auto_summary_search$"".""/"".max_auto_summary_searches
            | fields scheduled_vs_total_auto_summary_search_limit
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"")
            | eval mem_used = round(mem_used, 0)
            | eval count_cpu = round(pct_cpu / 100.0, 2)
            | eval elapsed = round(elapsed, 0)
            | fields search_props.sid, search_props.label, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, count_cpu, elapsed
            | rename search_props.sid as SID, search_props.label as ""Search Name"", search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", count_cpu as ""CPU Usage (# cores)"", elapsed as ""Time Elapsed (sec)""
          ",,,"search_activity_instance"
tbd,,,"index=summary",,"lookup dashboard_details","lame_training",,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| search eai:acl.app=* AND author!=""nobody""
| lookup dashboard_details id as id output details, mitre, usecase
| fillnull value=""TBD"" usecase, mitre
| rex field=""id"" ""(?&lt;urlField&gt;[^\/]+)$""

``` extract sourcetype, source, or eventtype field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?&lt;!(?#Skip excluded sourcetypes)\bNOT\s)(?i)(?&lt;sourcetypes&gt;sourcetype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded sources)\bNOT\s)(?i)(?&lt;sources&gt;source(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?&lt;!(?#Skip excluded eventtypes)\bNOT\s)(?i)(?&lt;eventtypes&gt;eventtype(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract host and index field &amp; values; exclude negated matches ```
| rex field=eai:data ""(?#Skip excluded hosts)(?&lt;!\bNOT\s)(?i)(?&lt;hosts&gt;host(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0
| rex field=eai:data ""(?#Skip excluded indexes)(?&lt;!\bNOT\s)(?i)(?&lt;indexes&gt;index(?:\s*(?:=|::)[\s\x22]*[-.:\w\x2a]+|\s+IN\s+\x28[-.:\w,\s]+\x29))"" max_match=0

``` extract Lookup, InputLookup, and OutputLookup commands &amp; values; must always be after a pipe character ```
| rex field=eai:data ""(?i)\x7c\s*(?&lt;lookups&gt;lookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;inputlookups&gt;inputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0
| rex field=eai:data ""(?i)\x7c\s*(?&lt;outputlookups&gt;outputlookup\b[\s\x22]+[-.:\w]+)"" max_match=0

``` extract the whole query ```
| rex field=eai:data ""(?s)&lt;query&gt;(?&lt;queries&gt;.*?)&lt;\/query&gt;.*?"" max_match=0

``` Trim extraneous double quotes from captured fields ```
| rex mode=sed field=sourcetypes ""s/\x22//g""
| rex mode=sed field=sources ""s/\x22//g""
| rex mode=sed field=eventtypes ""s/\x22//g""
| rex mode=sed field=hosts ""s/\x22//g""
| rex mode=sed field=indexes ""s/\x22//g""
| rex mode=sed field=lookups ""s/\x22//g""
| rex mode=sed field=inputlookups ""s/\x22//g""
| rex mode=sed field=outputlookups ""s/\x22//g""

| eval datasources=mvdedup(mvappend(sourcetypes, sources, eventtypes, indexes, hosts, lookups, inputlookups, outputlookups))
| table queries, sources, sourcetypes, eventtypes, datasources, app.owner, urlField, eai:acl.app author, eai:acl.sharing details, mitre, usecase

| rename eai:acl.app as myapp

| appendcols

  [ search index=summary source=""dashboard_views""
  | table myapp, file, method, status,  user
  | stats dc(user) as dc_user count by myapp, file
  | rename file as urlField
  | table myapp, urlField, count, dc_user
  ]
| search urlField=""*$c-input$*""  OR datasources=""*$c-input$*"" OR eai.data=""*$c-input$*""
| fillnull value=""N/A"" datasources
| table urlField, datasources, myapp  author, eai:acl.sharing, count, dc_user","source=dashboard_views",,"search_for_dashboard"
tbd,,,,"inputlookup splunk_commands.csv",,"lame_training",,"| inputlookup splunk_commands.csv | fields command",,,"search_lame_youtube"
tbd,,,,"inputlookup lame_youtube_channel.csv",,"lame_training",,"| inputlookup lame_youtube_channel.csv 
|  search (title=""$command_name$"" 
AND about=""*$key_phrase$*"") 
OR (title=$command_name$ 
AND title=""*$key_phrase$*"")
|  table title, about, url",,,"search_lame_youtube"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group=""dmc_customgroup_*"" OR search_group=""dmc_indexerclustergroup_*"" OR search_group=""dmc_searchheadclustergroup_*""
        ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_audit_get_searches_for_groups(""$group$"")`
      | stats min(_time) as _time, values(user) as user, max(total_run_time) as total_run_time, first(search) as search, first(search_type) as search_type, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime by search_id, host
      | where isnotnull(search) $filter_out_non_adhoc$
    ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(user) as user_count",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(host) as search_head_count",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats dc(host) as count_host, median(total_run_time) as median_runtime, sum(total_run_time) as cum_runtime, count(search) as count, max(_time) as last_use by user
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval cum_runtime = if(isnotnull(cum_runtime), cum_runtime, ""-"")
            | `dmc_time_format(last_use)`
            | fields user, count, count_host, median_runtime, cum_runtime, last_use
            | sort - count
            | rename user as User, count_host as ""Search Head Count"", count as ""Search Count"", median_runtime as ""Median Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=$user_drilldown_hosts$
            | stats count(search) as count by host
            | sort - count
            | fields host, count
            | rename host as ""Search Head"", count as ""Search Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=$user_drilldown_searches$
            | stats count by search
            | sort - count
            | fields count, search
            | rename search as ""Report Name/Search String"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats median(total_run_time) as median_runtime sum(total_run_time) as cum_runtime count(search) as count_search max(_time) as last_use dc(user) as count_user by host
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval cum_runtime = if(isnotnull(cum_runtime), cum_runtime, ""-"")
            | `dmc_time_format(last_use)`
            | fields host, count_search, count_user, median_runtime, cum_runtime, last_use
            | sort - count_search
            | rename host as ""Search Head"", count_search as ""Search Count"", count_user as ""User Count"", median_runtime as ""Median Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search host=$host_drilldown_users|s$
            | stats count(search) as count by user
            | sort - count
            | fields user, count
            | rename user as ""User"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search host=$host_drilldown_searches|s$
            | stats count by search
            | sort - count        
            | fields count, search
            | rename search as ""Report Name/Search String"", count as ""Count""
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=""*""
            | stats count(search) as count, median(total_run_time) as median_runtime, max(total_run_time) as max_runtime, values(user) as user, values(host) as host, values(search_type) as search_type by search
            | eval median_runtime = if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval max_runtime = if(isnotnull(max_runtime), max_runtime, ""-"")
            | sort - count
            | rename search as ""Report Name/Search String"", count as ""Count"", median_runtime as ""Median Runtime"", max_runtime as ""Max Runtime"", user as Users, host as Hosts, search_type as Type
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            search user=""*"" search=""*""
            | eval earliest = case(like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"", like(apiStartTime, ""%ZERO_TIME%""), ""-"", 1 == 1, apiStartTime )
            | eval latest = case(like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"", like(apiEndTime, ""%ZERO_TIME%""), ""-"", 1 == 1, apiEndTime ) 
            | `dmc_time_format(_time)`
            | stats max(total_run_time) as total_run_time by search, _time, earliest, latest, search_type, user, host, search_id
            | sort - total_run_time 
            | eval total_run_time = if(isnotnull(total_run_time), total_run_time, ""-"")
            | fields search, total_run_time, _time, earliest, latest, search_type, user, host, search_id
            | rename search as ""Report Name/Search String"", total_run_time as ""Search Runtime"", _time as ""Search Start"", earliest as ""Earliest Time"", latest as ""Latest Time"", search_type as Type, user as ""User"", host as ""Host"", search_id as SID
            | fieldformat ""Search Runtime"" = `dmc_convert_runtime('Search Runtime')`
          ",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"where search_id=$long_running_sid|s$",,,"search_usage_statistics_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_search_head)`
          | search search_group!=""dmc_group_*""
        ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      `dmc_audit_get_searches($host$)`
      | stats min(_time) as _time, values(user) as user, max(total_run_time) as total_run_time, first(search) as search, first(search_type) as search_type, first(apiStartTime) as apiStartTime, first(apiEndTime) as apiEndTime by search_id
      | where isnotnull(search) $filter_out_non_adhoc$
    ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats dc(user) as user_count",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats median(total_run_time) as median_runtime Perc90(total_run_time) as Perc90_runtime sum(total_run_time) as cum_runtime count(search) as count max(_time) as last_use by user
            | eval last_use = strftime(last_use, ""%F %T"")
            | fields user, count, median_runtime, Perc90_runtime, cum_runtime, last_use
            | rename user as User, count as ""Search Count"", median_runtime as ""Median Runtime"", Perc90_runtime as ""90th Percentile Runtime"", cum_runtime as ""Cumulative Runtime"", last_use as ""Last Search""
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""90th Percentile Runtime"" = `dmc_convert_runtime('90th Percentile Runtime')`
            | fieldformat ""Cumulative Runtime"" = `dmc_convert_runtime('Cumulative Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_audit_get_searches($host$)`
            | stats count by user
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            $common_search_user_filter$
            | stats count median(total_run_time) as median_runtime max(total_run_time) as max_runtime values(user) as user by search
            | eval median_runtime=if(isnotnull(median_runtime), median_runtime, ""-"")
            | eval max_runtime=if(isnotnull(max_runtime), max_runtime, ""-"")
            | sort - count
            | rename search as ""Search"", count as ""Count"", median_runtime as ""Median Runtime"", max_runtime as ""Max Runtime"", user as User
            | fieldformat ""Median Runtime"" = `dmc_convert_runtime('Median Runtime')`
            | fieldformat ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_audit_get_searches($host$)`
            | stats count by user
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
$long_running_search_user_filter$
| fields search, total_run_time, _time, apiStartTime, apiEndTime, search_type, user
| eval earliest = case(
    like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"",
    like(apiStartTime, ""%ZERO_TIME%""), ""-"",
    1 == 1, apiStartTime
)
| eval latest = case(
    like(apiStartTime, ""%ZERO_TIME%"") AND like(apiEndTime, ""%ZERO_TIME%""), ""all time"",
    like(apiEndTime, ""%ZERO_TIME%""), ""-"",
    1 == 1, apiEndTime
)
| eval search = if(isnotnull(search), search, ""N/A"")
| `dmc_time_format(_time)`
| sort - total_run_time
| fields search, total_run_time, _time, earliest, latest, search_type, user
| rename search as Search, total_run_time as ""Search Runtime"", _time as ""Search Start"", earliest as ""Earliest Time"", latest as ""Latest Time"", search_type as Type, user as ""User""
| fieldformat ""Search Runtime"" = `dmc_convert_runtime('Search Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,,,,"splunk_monitoring_console",,"
eval commands = commands(search)
| streamstats window=1 values(commands) as commands
| stats count avg(total_run_time) as avg_runtime max(total_run_time) as max_runtime by commands
| eval avg_runtime = round(avg_runtime, 2)
| eval max_runtime = round(max_runtime, 2)
| sort - count, - max_runtime, - avg_runtime
| rename commands as Command, avg_runtime as ""Average Runtime"", max_runtime as ""Max Runtime"", count as ""Count""
| eval ""Average Runtime"" = `dmc_convert_runtime('Average Runtime')`
| eval ""Max Runtime"" = `dmc_convert_runtime('Max Runtime')`
          ",,,"search_usage_statistics_instance"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
              index=""_internal"" source=""*secure_gateway*"" aiohttp_wss_protocol ""Received Pong""
              | head 1 | stats count as pong_count
              | eval spacebridge_up = if(pong_count &gt; 0, ""Connected"", ""Not Connected"")
              | eval range=if(pong_count &gt; 0, ""low"", ""severe"")
              | table spacebridge_up, range
          ","source=*secure_gateway*",,"secure_gateway_status_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
              | rest services/kvstore/status
              | search current.disabled = 0
              | fields current.status
              | eval upper=upper('current.status')
              | eval range=if(upper=""READY"", ""low"", ""severe"")
              | table upper range
          ",,,"secure_gateway_status_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
              | noop | checkssgmobilewss
              | eval ok = if(websocket &gt; 0, ""OK"", ""Failed"")
              | eval range=if(websocket &gt; 0, ""low"", ""severe"")
              | table ok, range
          ",,,"secure_gateway_status_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
              | noop | checkssgmobilehttps
              | eval ok = if(https_sync &gt; 0, ""OK"", ""Failed"")
              | eval range=if(https_sync &gt; 0, ""low"", ""severe"")
              | table ok, range
          ",,,"secure_gateway_status_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"
              | noop | checkssgmobileasync
              | eval ok = if(https_async &gt; 0, ""OK"", ""Failed"")
              | eval range=if(https_async &gt; 0, ""low"", ""severe"")
              | table ok, range
          ",,,"secure_gateway_status_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
              index=""_internal"" source=""*secure_gateway*"" message=""RECEIVED_ENVELOPE"" OR message=""SENT_BACK""
              | bin span=2min _time
              | timechart span=2m count by message
          ","source=*secure_gateway*",,"secure_gateway_status_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
               index=""_internal"" sourcetype=""secure_gateway_app_internal_log"" message_processor process_request_list OR process_message NOT ERROR
               | transaction request_id startswith=""process_request_list"" endswith=""Finished processing""
               | stats count(duration), avg(duration) by type
               | rename avg(duration) as latency
               | rename count(duration) as count
               | table type, count, latency
           ",,"sourcetype=secure_gateway_app_internal_log","secure_gateway_status_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
              index=""_internal"" sourcetype=""secure_gateway_app_internal_log"" log_level=ERROR
              | bin span=2min _time
              | timechart span=2m count by app_name
          ",,"sourcetype=secure_gateway_app_internal_log","secure_gateway_status_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
              index=""_internal"" source=""*secure_gateway*"" protocol ""Websocket connection was closed""
              | bin span=1h _time
              | timechart span=1h count
          ","source=*secure_gateway*",,"secure_gateway_status_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"
              index=""_internal"" sourcetype=""secure_gateway_app_internal_log"" log_level=ERROR | head 10
          ",,"sourcetype=secure_gateway_app_internal_log","secure_gateway_status_dashboard"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/local splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
| search install_source_checksum=*
| fields title, label, splunk_server, install_source_checksum
| rename title as app_name, install_source_checksum as checksum
  | append [ | rest /services/apps/deploy splunk_server_group=""$group$"" splunk_server_group=dmc_group_shc_deployer
  | fields title, splunk_server, checksum
| rename title as app_name]
| stats values(checksum) as checksum, values(label) as label by app_name
| eval status = if(mvcount(checksum) > 1, ""Out of Synchronization!"", ""Synchronized"")
| fields label status app_name
| rename label as App, status as ""Status""
| sort status
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/local splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
| search install_source_checksum=* AND title=""$appLocalDrilldown$""
| fields splunk_server, install_source_checksum
| rename splunk_server as Instance, install_source_checksum as Checksum
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
| rest /services/apps/deploy splunk_server_group=""$group$"" splunk_server_group=dmc_group_shc_deployer
| search title=""$appLocalDrilldown$""
| fields splunk_server, checksum
| rename splunk_server as Instance, checksum as Checksum
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=dmc_group_shc_deployer sourcetype=splunkd_access uri_path=""/services/apps/deploy"" method=POST
| stats values(spent) as spent by _time, user, status
        ",,"sourcetype=splunkd_access","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by user",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
$deploymentRequestStatusScope$
| $deploymentRequestStatusScope$
| `dmc_timechart` $deploymentRequestMetric$ $deploymentRequestSplitBy$
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=dmc_group_shc_deployer sourcetype=splunkd_conf component=ConfDeployment data.task=sendDeployableApps
| stats count by _time, data.target_label, data.status
        ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by data.status",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"stats count by data.target_label",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
$sendDeployableAppsStatusFilter$
| $sendDeployableAppsTargetFilter$
| `dmc_timechart` count by $sendDeployableAppsSplitBy$
          ",,,"shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | table data.apps{}.action, data.apps{}.name
            | eval apps = mvzip('data.apps{}.action', 'data.apps{}.name', "","")
            | fields - data.apps{}.action, data.apps{}.name
            | mvexpand apps
            | rex field=apps ""(?&lt;action&gt;\w+),(?&lt;app_name&gt;.+)""
            | fields action, app_name
            | where action!=""preserved""
            | stats count by app_name
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | stats count by host
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd_conf component=ConfDeployment data.task=downloadDeployableApps
            | table _time, data.apps{}.action, data.apps{}.name, host
            | eval apps = mvzip('data.apps{}.action', 'data.apps{}.name', "","")
            | mvexpand apps
            | rex field=apps ""(?&lt;action&gt;\w+),(?&lt;app_name&gt;.+)""
            | fields _time, host, action, app_name
            | where action!=""preserved""
            | $actionFilter$
            | $appNameFilter$
            | $hostFilter$
            | timechart count(action) as count_action by $appDeploymentSplitBy$
          ",,"sourcetype=splunkd_conf","shc_app_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", *Artifact*)`
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", *Artifact*)` punct=""$warningErrorPunct$""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/artifacts
          | fields title, status, splunk_server, label, user, eai:acl.app
          | rename splunk_server as member, label as search_name, eai:acl.app as app
        ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by status",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by member",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by app",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by search_name",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"stats count by user",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            $count_artifact_status_filter$
            | $count_artifact_member_filter$
            | $count_artifact_app_filter$
            | $count_artifact_search_name_filter$
            | $count_artifact_user_filter$
            | stats count by $count_artifact_split_by$
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
            $count_artifact_status_filter$
            | $count_artifact_member_filter$
            | search $count_artifact_drilldown_name$ = ""$count_artifact_drilldown_value$""
          ",,,"shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=subtask_counts name=shccaptain_artifact search_group=""$group$""
| `dmc_timechart_for_metrics_log` median(to_fix_rep_factor) as artifacts
          ",,"sourcetype=splunkd","shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=executor name=poolmember_executor search_group=""$group$""
| eval backlog_change = jobs_added - jobs_finished
| `dmc_timechart_for_metrics_log` sum(jobs_added) AS ""jobs added"" sum(jobs_finished) AS ""jobs finished"" sum(backlog_change) AS ""backlog change""
          ",,"sourcetype=splunkd","shc_artifact_replication"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd_conf search_group=""$group$"" component=ConfOp data.task=*addCommit
| stats count by _time, host, optype_desc, object_name, object_type, app, owner
    ",,"sourcetype=splunkd_conf","shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/members
          | join label type=outer [
            | rest /services/replication/configuration/health check_share_baseline=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
            | stats values(server_name) as baselines, count(server_name) as num_baselines by splunk_server, check_share_baseline
            | eval shared_common_baseline = if(check_share_baseline == ""Yes"", baselines, """")
            | eval no_shared_common_baseline = if(check_share_baseline == ""No"", baselines, """")
            | eval unable_to_connect = if(check_share_baseline == ""Connection error"", baselines, """")
            | eval num_shared_common_baseline = if(check_share_baseline == ""Yes"", num_baselines, 0)
            | eval num_no_shared_common_baseline = if(check_share_baseline == ""No"", num_baselines, 0)
            | eval num_unable_to_connect = if(check_share_baseline == ""Connection error"", num_baselines, 0)
            | stats sum(num_shared_common_baseline) as total_shared_common_baseline, sum(num_no_shared_common_baseline) as total_no_shared_common_baseline, sum(num_unable_to_connect) as total_unable_to_connect, values(shared_common_baseline) as shared_common_baseline, values(no_shared_common_baseline) as no_shared_common_baseline, values(unable_to_connect) as unable_to_connect by splunk_server
            | eval ratio = total_shared_common_baseline . ""/"" . (total_shared_common_baseline+total_no_shared_common_baseline+total_unable_to_connect)
            | rename splunk_server as label
          ]
          | where total_no_shared_common_baseline+total_unable_to_connect > 0
        ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", ConfReplication*)`
| cluster showcount=t
| table cluster_count, _time, log_level, component, event_message, punct
| sort - cluster_count
| `dmc_time_format(_time)`
| rename cluster_count AS Count, _time AS ""Latest Time"", log_level as ""Log Level"", component as Component, event_message as ""Latest Message""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_get_warnings_and_errors(""$group$"", ConfReplication*)` punct=""$warningErrorPunct$""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by host
            | eval host_scope = ""where host == \"""".host.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by app
            | eval app_scope = ""where app == \"""".app.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by optype_desc
            | eval optype_desc_scope = ""where optype_desc == \"""".optype_desc.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by object_type
            | eval object_type_scope = ""where object_type == \"""".object_type.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by object_name
            | eval object_name_scope = ""where object_name == \"""".object_name.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            | stats count by owner
            | eval owner_scope = ""where owner == \"""".owner.""\""""
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            $confOpAppScope$
            | $confOpOperationTypeScope$
            | $confOpObjTypeScope$
            | $confOpObjNameScope$
            | $confOpOwnerScope$
            | $confOpHostScope$
            | timechart count by $confOpBy$ usenull=f
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
            fields - count
| $confOpOperationTypeScope$
| $confOpObjTypeScope$
| $confOpObjNameScope$
| $confOpOwnerScope$
| $confOpHostScope$
| sort - _time
| rename _time as Time, host as Instance, optype_desc as ""Operation Type"", object_name as ""Object Name"", object_type as ""Object Type"", app as App, owner as Owner
| `dmc_time_format(Time)`
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd search_group=""$group$"" group=conf
| `dmc_set_bin_for_metrics_log`
| stats sum(count) as count, sum(wallclock_ms_total) as sum_total_time, max(wallclock_ms_max) as max_max_time by _time, action, host
        ",,"sourcetype=splunkd","shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_timechart_for_metrics_log` sum(count) as count latest(eval(sum_total_time / count)) as avg_time_per_action latest(max_max_time) as max_time_per_action
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by action",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
$confRepPerfCountHostScope$
| $confRepPerfCountActionScope$
| `dmc_timechart_for_metrics_log` sum(count) by $confRepPerfCountSplitBy$
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"stats count by action",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
$confRepPerfTimeHostScope$
| $confRepPerfTimeActionScope$
| `dmc_timechart_for_metrics_log` $confRepPerfMetric$ by $confRepPerfTimeSplitBy$
          ",,,"shc_conf_rep"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` sourcetype=splunkd group=searchscheduler search_group=""$group$""
| `dmc_timechart_for_metrics_log` sum(dispatched) as dispatched, sum(skipped) as skipped, sum(delegated) as delegated $funcStatus$(delegated_waiting) as delegated_waiting, sum(delegated_scheduled) as delegated_scheduled, $funcStatus$(max_pending) as max_pending, $funcStatus$(max_running) as max_running
    ",,"sourcetype=splunkd","shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
          | dedup label
        ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=$captain_server_name$ /services/shcluster/captain/jobs
| eval peer = if(isnotnull(peer_servername), peer_servername, peer)
| fields dispatch_time, job_state, peer, saved_search, savedsearchtype, search_app, search_owner, sid, splunk_server, success, title
| eval sid = if(isnotnull(sid), sid, ""N/A"")
        ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by $jobsSplitBy$",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by job_state",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"stats count by saved_search",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
$jobStateFilter$
| $jobSidFilter$
| $jobSavedSearchNameFilter$
| `dmc_time_format(dispatch_time)`
| sort - dispatch_time
| fields dispatch_time, job_state, success, splunk_server, peer, saved_search, savedsearchtype, search_app, search_owner, sid
| eval success = if (success == 1 or success == ""1"", ""Succeeded"", ""Failed"")
| rename dispatch_time as ""Dispatch Time"", job_state as ""Job State"", success as Success, splunk_server as ""Delegating Instance (Captain)"" peer as ""Delegated Instance"", saved_search as ""Scheduled Search"", savedsearchtype as ""Saved Search Type"", search_app as App, search_owner as Owner, sid as SID
          ",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"fields - delegated_waiting delegated_scheduled max_pending max_running",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"fields - dispatched skipped delegated",,,"shc_scheduler_delegation_statistics"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
      | dedup peer_scheme_host_port
      | fields label
    ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
| rest splunk_server=local /services/search/distributed/peers
| where search_groups=""$group$"" AND server_roles=""search_head""
| eval label = host
| join guid type=outer [
  | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/member/members count=0
  | dedup label
  | eval guid = title
]
| join label type=outer [
  | rest splunk_server=$captain_name|s$ /services/shcluster/captain/members count=0
  | where splunk_server == label
  | fields label, last_heartbeat
  | rename last_heartbeat as last_heartbeat_captain
]
| eventstats values(last_heartbeat_captain) as last_heartbeat_captain
| join label type=outer [
  | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/shcluster/captain/info
  | where splunk_server == label
  | eval age = now() - elected_captain
  | eval captain_age = case(age &lt; 60, ""&lt; 1m"", age &gt;= 60 AND age &lt; 3600, round(age / 60, 0).""m"", age &gt;= 3600 AND age &lt; 86400, round(age / 3600, 0).""h"", age &gt;= 86400, round(age / 86400, 0).""d"")
  | `dmc_time_format(elected_captain)`
  | eval role = ""Captain ("" . captain_age . "")""
  | fields label captain_age elected_captain role
]
| join label type=outer [
  | rest /services/replication/configuration/health check_share_baseline=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
  | stats values(server_name) as baselines, count(server_name) as num_baselines by splunk_server, check_share_baseline
  | eval shared_common_baseline = if(check_share_baseline == ""Yes"", baselines, """")
  | eval no_shared_common_baseline = if(check_share_baseline == ""No"", baselines, """")
  | eval unable_to_connect = if(check_share_baseline == ""Connection error"", baselines, """")
  | eval num_shared_common_baseline = if(check_share_baseline == ""Yes"", num_baselines, 0)
  | eval num_no_shared_common_baseline = if(check_share_baseline == ""No"", num_baselines, 0)
  | eval num_unable_to_connect = if(check_share_baseline == ""Connection error"", num_baselines, 0)
  | stats sum(num_shared_common_baseline) as total_shared_common_baseline, sum(num_no_shared_common_baseline) as total_no_shared_common_baseline, sum(num_unable_to_connect) as total_unable_to_connect, values(shared_common_baseline) as shared_common_baseline, values(no_shared_common_baseline) as no_shared_common_baseline, values(unable_to_connect) as unable_to_connect by splunk_server
  | eval ratio = total_shared_common_baseline . ""/"" . (total_shared_common_baseline+total_no_shared_common_baseline+total_unable_to_connect)
  | rename splunk_server as label
]
| join label type=outer [
  | rest /services/replication/configuration/health unpublished=1 splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head
  | rename ""Number of unpublished changes"" as unpublished_changes
  | eval unpublished_changes=if(unpublished_changes==""0 (this instance is the captain)"", 0, unpublished_changes)
  | rename splunk_server as label
]
| eval role = if(isnotnull(role), role, ""Member"")
| sort role
    ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          fields label, status, last_heartbeat, last_heartbeat_captain, unable_to_connect
          | join label type=outer [
            | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/configs/conf-server/shclustering
            | fields splunk_server, heartbeat_timeout
            | rename splunk_server as label
          ]
          | where (status != ""Up"" OR (last_heartbeat_captain - last_heartbeat) > heartbeat_timeout) OR unable_to_connect != """"
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/search/distributed/peers
          | fields splunk_server, peerName
          | stats values(peerName) as peers by splunk_server
          | nomv peers
          | stats values(splunk_server) AS search_heads by peers
          | makemv peers
          | fields search_heads peers
          | rename search_heads as ""Search Head Cluster Member"", peers as ""Search Peer List""
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          where total_no_shared_common_baseline+total_unable_to_connect > 0
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            fields label, last_heartbeat, last_heartbeat_captain, heartbeat_timeout, status, unable_to_connect
            | eval heartbeat_timeout = if(isnotnull(heartbeat_timeout), heartbeat_timeout, ""N/A"")
            | `dmc_time_format(last_heartbeat)`
            | `dmc_time_format(last_heartbeat_captain)`
            | eval last_heartbeat = if(isnotnull(last_heartbeat), last_heartbeat, ""N/A"")
            | rename label as Member, last_heartbeat as ""Last Heartbeat Sent to Captain"", last_heartbeat_captain as ""Last Heartbeat Received by Captain"", heartbeat_timeout as ""Heartbeat Timeout (sec)"", status as ""Status"", unable_to_connect as ""Member Unreachable""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head /services/server/status/resource-usage/splunk-processes
          | search search_props.role=""head""
          | dedup search_props.sid
        ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_total_hist,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_total_rt,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch""))) as count_hist_scheduled_search,
            count(eval(('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed""))) as count_rt_scheduled_search,
            count(eval(('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration""))) as count_auto_summary_search
            | eval count_total_adhoc_scheduled_search = count_total_hist + count_total_rt
            | eval count_total_scheduled_search = count_hist_scheduled_search + count_rt_scheduled_search
            | eval dummy_key = ""dummy_key""
            | fields count_total_hist, count_hist_scheduled_search,
                     count_total_rt, count_rt_scheduled_search,
                     count_auto_summary_search,
                     count_total_adhoc_scheduled_search, count_total_scheduled_search,
                     dummy_key
            | join dummy_key type=outer [
              | rest splunk_server_group=""$group$"" splunk_server_group=dmc_group_search_head ""/services/server/status/limits/search-concurrency?cluster_wide_quota=1""
              | stats max(max_hist_searches) as max_hist_searches, max(max_hist_scheduled_searches) as max_hist_scheduled_searches, max(max_rt_searches), as max_rt_searches, max(max_rt_scheduled_searches) as max_rt_scheduled_searches, max(max_auto_summary_searches) as max_auto_summary_searches
              | eval dummy_key = ""dummy_key""
              | fields max_hist_searches, max_hist_scheduled_searches,
                  max_rt_searches, max_rt_scheduled_searches,
                  max_auto_summary_searches,
                  dummy_key
            ]
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_historical_vs_limit = count_total_hist.""/"".max_hist_searches
            | fields total_historical_vs_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval total_rt_vs_limit = count_total_rt.""/"".max_rt_searches
            | fields total_rt_vs_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_hist_scheduled_search_limit = count_hist_scheduled_search.""/"".max_hist_scheduled_searches
            | fields scheduled_vs_total_hist_scheduled_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_rt_scheduled_search_limit = count_rt_scheduled_search.""/"".max_rt_scheduled_searches
            | fields scheduled_vs_total_rt_scheduled_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            eval scheduled_vs_total_auto_summary_search_limit = count_auto_summary_search.""/"".max_auto_summary_searches
            | fields scheduled_vs_total_auto_summary_search_limit
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"" OR 'search_props.type'==""ad-hoc"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""historical"" OR 'search_props.mode'==""historical batch"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""scheduled"" OR 'search_props.type'==""summary indexing"" OR 'search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"") AND ('search_props.mode'==""RT"" OR 'search_props.mode'==""RT indexed"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where ('search_props.type'==""report acceleration"" OR 'search_props.type'==""datamodel acceleration"")
            | fields search_props.sid, search_props.label, splunk_server, search_props.app, search_props.user, search_props.type, search_props.mode, search_props.role, mem_used, pct_cpu, elapsed
            | eval mem_used = round(mem_used, 0)
            | eval pct_cpu = round(pct_cpu, 0)
            | eval elapsed = round(elapsed, 0)
            | rename search_props.sid as SID, search_props.label as ""Search Name"", splunk_server as Member, search_props.app as App, search_props.user as User, search_props.type as Type, search_props.mode as Mode, search_props.role as Role, mem_used as ""Memory Used (MB)"", pct_cpu as ""CPU Usage (%)"", elapsed as ""Time Elapsed (sec)""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
fields label role status last_heartbeat ratio unpublished_changes artifact_count
| eval ratio = if(isnotnull(ratio), ratio, ""N/A"")
| eval unpublished_changes = if(isnotnull(unpublished_changes), unpublished_changes, ""N/A"")
| eval artifact_count = if(isnotnull(artifact_count), artifact_count, ""N/A"")
| `dmc_time_format(last_heartbeat)`
| `dmc_time_format(last_heartbeat_captain)`
| eval last_heartbeat = if(isnotnull(last_heartbeat), last_heartbeat, ""N/A"")
| rename label as Instance, role as Role, status as Status, last_heartbeat as ""Last Heartbeat Sent to Captain"", ratio as ""Configuration Baseline Consistency"", unpublished_changes as ""Number of Unpublished Changes"", artifact_count as ""Artifact Count""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
where label == ""$SHCMemberDrilldown$""
| eval elected_captain = if(isnotnull(elected_captain), elected_captain, ""N/A"")
| fields - _timediff
| fields elected_captain guid advertise_restart_required advertise_restart_required_reason delayed_artifacts_to_discard fixup_set pending_job_count replication_count status_counter.Complete status_counter.PendingDiscard peer_scheme_host_port adhoc_searchhead kv_store_host_port replication_port replication_use_ssl site
| transpose
| rename column as ""Configuration and Status"", ""row 1"" as Value
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
            where label == ""$SHCMemberBaselineDrilldown$""
            | fields shared_common_baseline no_shared_common_baseline unable_to_connect
            | rename shared_common_baseline as ""Shares Common Baseline With"", no_shared_common_baseline as ""Does Not Share Common Baseline With"", unable_to_connect as ""No Response From""
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
        `dmc_set_index_introspection` search_group=dmc_group_search_head search_group=""$group$"" sourcetype=splunk_resource_usage ((component=PerProcess data.search_props.sid::*) OR component=Hostwide)
        | `dmc_rename_introspection_fields`
        | `dmc_set_bin`
        | stats dc(sid) AS distinct_search_count by provenance, mode, app, type, user, host, _time
        ",,"sourcetype=splunk_resource_usage","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          $shcSearchConHostScope$
          | stats sum(distinct_search_count) as total_distinct_search_count by provenance, mode, app, type, user, host, _time
          | `dmc_timechart` partial=false $shcSearchConFunc$(total_distinct_search_count) as search_count by $shcSearchConSplitBy$
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability upgrades_to_captain=1
| stats count by _time, upgrades_to_captain, host
        ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"stats count by host",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
$captainElectionHostScope$
| `dmc_timechart_for_metrics_log` count(upgrades_to_captain) as captain_election_event
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
$captainElectionHostScope$
| eval event = host."" was elected as captain.""
| fields _time, event
| sort - _time
| rename _time as Time, event as Event
| `dmc_time_format(Time)`
          ",,,"shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability (upgrades_to_captain=1 OR downgrades_from_captain=1 OR num_pollled_candidates&gt;0)
| `dmc_timechart_for_metrics_log` count(num_polled_captain) as candidate sum(upgrades_to_captain) as upgrades_to_captain sum(downgrades_from_captain) as downgrades_from_captain
          ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
`dmc_set_index_internal` search_group=""$group$"" sourcetype=splunkd component=Metrics group=captainstability
| `dmc_set_bin_for_metrics_log`
| eval status=case(
(upgrades_to_captain=0) AND (downgrades_from_captain=0) AND (num_polled_candidate=0), ""stable"",
1=1, ""perturbing"")
| eval reason=case(
upgrades_to_captain=1 AND num_polled_candidate&gt;0, host."" became candidate and was elected as captain"",
upgrades_to_captain=0 AND num_polled_candidate&gt;0, host."" became candidate"",
upgrades_to_captain=1 AND num_polled_candidate=0, host."" was elected as captain"",
downgrades_from_captain=1, host."" changed from captain to a member"",
1=1, """")
| stats count by _time, status, reason
| sort - _time
| fields - count
| $captainHistoryFilter$
          ",,"sourcetype=splunkd","shc_status_and_conf"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log CacheManager action=* earliest=-1h 
                | stats count(eval(status=""failed"")) as txn_failed, count(eval(status=""succeeded"")) as txn_succeeded 
                | eval available=case(txn_failed=0 AND txn_succeeded=0, ""IDLE"", txn_succeeded>0, ""ONLINE"", true(), ""OFFLINE"") 
                | fields - txn_failed - txn_succeeded","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server_group=$search_group$ /services/admin/cacheman
            | eval stable_size = if('cm:bucket.stable'=1, 'cm:bucket.estimated_size', 0)
            | stats sum(stable_size) as stable_bytes, sum(cm:bucket.estimated_size) as all_bytes
            | eval migration_progress=round((stable_bytes / all_bytes) * 100)
            | fields - stable_bytes - all_bytes
          ",,,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$search_group$ group=cmmaster source=*metrics.log subgroup=buckets_re_creation
            | stats sum(success_count) as successes, sum(in_progress_count) as in_progress
            | eval total = successes + in_progress
            | eval percent = if(total=0, 100, (1 - ((in_progress - successes) / total)) * 100)
            | fields percent
          ","source=*metrics.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log ((action=download AND download_set=*journal*) OR action=upload) status=succeeded component=CacheManager
            | eval mb = kb / 1024
            | timechart $funcAggregate$(mb) by action
          ","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log (action=download OR action=upload) status=failed component=CacheManager
              | rex field=reason ""HTTP Error (?<statuscode>\d*): (?<description>.*)""
              | eval reason_str = statuscode."": "".description
              | fillnull reason_str value=""Non-HTTP""
              | timechart count by $aggregation$
            ]]>
          ","source=*splunkd.log",,"smartstore_activity_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log CacheManager AND action=* AND earliest=-1h 
                | stats count(eval(status=""failed"")) as txn_failed, count(eval(status=""succeeded"")) as txn_succeeded 
                | eval available=case(txn_failed=0 AND txn_succeeded=0, ""IDLE"", txn_succeeded>0, ""ONLINE"", true(), ""OFFLINE"") 
                | fields - txn_failed - txn_succeeded","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest splunk_server=$splunk_server$ /services/admin/cacheman
            | eval stable_size = if('cm:bucket.stable'=1, 'cm:bucket.estimated_size', 0)
            | stats sum(stable_size) as stable_bytes, sum(cm:bucket.estimated_size) as all_bytes
            | eval migration_progress=round((stable_bytes / all_bytes) * 100)
            | fields - stable_bytes - all_bytes
          ",,,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$splunk_server$ group=cmmaster source=*metrics.log subgroup=buckets_re_creation
            | stats sum(success_count) as successes, sum(in_progress_count) as in_progress
            | eval total = successes + in_progress
            | eval percent = if(total=0, 100, (1 - ((in_progress - successes) / total)) * 100)
            | fields percent
          ","source=*metrics.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log ((action=download AND download_set=*journal*) OR action=upload) status=succeeded component=CacheManager
            | eval mb = kb / 1024
            | timechart $funcAggregate$(mb) by action
          ","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$ source=*splunkd.log (action=download OR action=upload) status=failed component=CacheManager
              | rex field=reason ""HTTP Error (?<statuscode>\d*): (?<description>.*)""
              | eval reason_str = statuscode."": "".description
              | fillnull reason_str value=""Non-HTTP""
              | timechart count by $aggregation$
            ]]>
          ","source=*splunkd.log",,"smartstore_activity_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/diskUsage/minFreeSpace | eval label = IF(match(value,""\%""), value, tostring(value, ""commas"").""MB"") | fields label",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/eviction_padding | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/max_cache_size | eval size=if(value=0, ""No Max"", tostring(value, ""commas"").""MB"") | fields size",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/hotlist_recency_secs | appendpipe [stats count | eval value=86500 | where count=0] | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server_group=$search_group$ /services/properties/server/cachemanager/hotlist_bloom_filter_recency_hours | appendpipe [stats count | eval value=360 | where count=0] | fields value",,,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ source=*metrics.log | timechart sum(evicted) as Evicted","source=*metrics.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
             `dmc_set_index_internal` search_group=$search_group$  source=*/splunkd_access.log
              | rex field=uri ""/services/admin/cacheman/bid|(?<bid>[^|]*)|/close""
              | search uri=*/close*
              | eval mytime=strftime(_time, ""%Y-%m-%d %H:%M:%S"")
              | stats last(_time) as _time count as buckets, sum(miss_ms) as miss_ms sum(search_ms) as search_ms, min(mytime) as issuetime by sid
              | fillnull value=0 search_ms
              | eval overheadRatio=(miss_ms/search_ms)
              | fillnull value=0 overheadRatio
              | eval searchSpeed=case(overheadRatio > 2,""More than 50%"", overheadRatio < .2, ""Less than 10%"", true(), ""10%-50%"")
              | timechart count by searchSpeed
            ]]>
          ","source=*",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` search_group=$search_group$ group=cachemgr_bucket source=*metrics.log | timechart count(cache_hit) as Hits count(cache_miss) as Misses","source=*metrics.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` search_group=$search_group$ source=*splunkd.log CacheManager AND TERM(action=download) AND TERM(status=succeeded) AND download_set=""*journal*""
              | rex field=cache_id "".*\|(?<customer_index>.*)~.*~.*\|""
              | eval identifier=(cache_id + host)
              | stats count by identifier, customer_index
              | stats count(eval(count>1)) as duplicate_downloads, sum(count) as all_downloads count(eval(count>=10)) as excessive_duplicate_downloads by customer_index
              | eval duplicate_percent=if(all_downloads=0, 0, round((duplicate_downloads/all_downloads)*100, 2))
              | sort  - duplicate_percent
              | fields customer_index, duplicate_percent  all_downloads duplicate_downloads excessive_duplicate_downloads 
              | rename customer_index as Index, duplicate_percent as ""Repeat Download Percent"",  all_downloads as ""All Downloads"", duplicate_downloads as ""Repeated Downloads"", excessive_duplicate_downloads as ""Excessively Repeated Downloads""
            ]]>
          ","source=*splunkd.log",,"smartstore_cache_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/diskUsage/minFreeSpace | eval label = IF(match(value,""\%""), value, tostring(value, ""commas"").""MB"") | fields label",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/eviction_padding | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/max_cache_size | eval size=if(value=0, ""No Max"", tostring(value, ""commas"").""MB"") | fields size",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/hotlist_recency_secs | appendpipe [stats count | eval value=86500 | where count=0] | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"| REST splunk_server=$splunk_server$ /services/properties/server/cachemanager/hotlist_bloom_filter_recency_hours | appendpipe [stats count | eval value=360 | where count=0] | fields value",,,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*metrics.log | timechart sum(evicted) as Evicted","source=*metrics.log",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$  source=*/splunkd_access.log
              | rex field=uri ""/services/admin/cacheman/bid|(?<bid>[^|]*)|/close""
              | search uri=*/close*
              | eval mytime=strftime(_time, ""%Y-%m-%d %H:%M:%S"")
              | stats last(_time) as _time count as buckets, sum(miss_ms) as miss_ms sum(search_ms) as search_ms, min(mytime) as issuetime by sid
              | fillnull value=0 search_ms
              | eval overheadRatio=(miss_ms/search_ms)
              | fillnull value=0 overheadRatio
              | eval searchSpeed=case(overheadRatio > 2,""More than 50%"", overheadRatio < .2, ""Less than 10%"", true(), ""10%-50%"")
              | timechart count by searchSpeed
            ]]>
          ","source=*",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_set_index_internal` host=$splunk_server$ source=*metrics.log group=cachemgr_bucket | timechart count(cache_hit) as Hits count(cache_miss) as Misses","source=*metrics.log",,"smartstore_cache_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              `dmc_set_index_internal` host=$splunk_server$  source=*splunkd.log CacheManager AND TERM(action=download) AND TERM(status=succeeded) AND download_set=""*journal*""
              | rex field=cache_id "".*\|(?<customer_index>.*)~.*~.*\|""
              | eval identifier=(cache_id + host)
              | stats count by identifier, customer_index
              | stats count(eval(count>1)) as duplicate_downloads, sum(count) as all_downloads count(eval(count>=10)) as excessive_duplicate_downloads by customer_index
              | eval duplicate_percent=if(all_downloads=0, 0, round((duplicate_downloads/all_downloads)*100, 2))
              | sort  - duplicate_percent
              | fields customer_index, duplicate_percent  all_downloads duplicate_downloads excessive_duplicate_downloads 
              | rename customer_index as Index, duplicate_percent as ""Repeated Download Percent"",  all_downloads as ""All Downloads"", duplicate_downloads as ""Repeated Downloads"", excessive_duplicate_downloads as ""Excessively Repeated Downloads""
            ]]>
          ","source=*splunkd.log",,"smartstore_cache_performance_instance"
tbd,,,"index=*",,,"lame_training",,"| eventcount summarize=false index=* | dedup index | fields index",,,"sourcetype_documentation"
tbd,,,,,,"lame_training",,"| metadata index=$idx$ type=sourcetypes | fields sourcetype",,,"sourcetype_documentation"
tbd,,,,"inputlookup SourcetypeInfo",,"lame_training",,"| inputlookup SourcetypeInfo 
| search sourcetype=$src_type$
| eval fieldvalue = case(fieldvalue=1, ""Useless"", fieldvalue=5, ""Mostly Useless"", fieldvalue=10, ""Useful"")
| table sourcetype, fieldname, fieldvalue, rationale, fieldvalue",,,"sourcetype_documentation"
tbd,,,,"inputlookup Sourcetype_Analytics",,"lame_training",,"| inputlookup Sourcetype_Analytics 
| search sourcetype=$src_type$
| table sourcetype, metric_description, metric_query",,,"sourcetype_documentation"
tbd,,,,"inputlookup sourcetype_info",,"lame_analytic_documentation",,"| inputlookup sourcetype_info | search description = ""tbd""
| rename _key as the_key
| table the_key, index, sourcetype, description, usegroup",,,"sourcetype_information"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* finished | eval last_bucket_time=strftime(latest_bucket_time_secs, ""%F %T %z"")| eval transfered_mb=remote_bucket_bytes/1000000 | rename splunk_index AS ""Splunk Index"", virtual_index AS ""Archive Index"" | stats max(last_bucket_time) as ""Latest Archive Bucket Time"" sum(transfered_mb) as ""Total Transfered MB"" sum(buckets_copied) as ""Total Buckets Copied"" by ""Splunk Index"", ""Archive Index""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* earliest=-1d | rex max_match=1000 ""\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+ -\d{4} (?&lt;severity&gt;\w+) "" | where severity=""ERROR""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed $splunk_idx1$ | timechart count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed ""$splunk_idx2$"" |  eval mb = remote_bucket_bytes/1000000 | timechart sum(mb) by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* committed | rename bucket_name AS ""Archived Bucket"", splunk_index AS ""Splunk Index"" | eval mb=round(remote_bucket_bytes/1000000,2) | stats sum(mb) as ""Archived Bucket MB"" by ""Splunk Index"", ""Archived Bucket""","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* | rex max_match=1000 ""\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+ -\d{4} (?&lt;severity&gt;\w+) "" | where severity=""ERROR"" | timechart count AS errors","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* report: buckets_to_freeze_remaining_count buckets_to_freeze_deleted | stats count by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* buckets_to_freeze_remaining_count buckets_to_freeze_deleted report: $splunk_idx3$ | timechart sum(buckets_to_freeze_remaining_count) as ""Buckets to freeze"", sum(buckets_to_freeze_deleted) as ""Buckets frozen"" by splunk_index","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* buckets_to_freeze_size_bytes buckets_to_freeze_deleted_size_bytes report: $splunk_idx3$ | timechart sum(buckets_to_freeze_size_bytes) as ""to_freeze"", sum(buckets_to_freeze_deleted_size_bytes) as ""frozen"", by splunk_index | eval ""to_freeze_mb""=to_freeze/1000000 | eval frozen_mb=frozen/1000000 | rename to_freeze_mb AS ""Remaning diskspace to free (MB)"", frozen_mb AS ""Frozen transfered (MB)"", splunk_index AS ""Splunk index"" | fields -  to_freeze, frozen","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* Report: | eval secs = total_elapsed_ms/1000 | timechart sum(secs) as ""Seconds spent archiving"" by host","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,"index=_internal",,,"splunk_archiver",,"index=_internal source=*splunk_archiver.log* Report: | eval mb = remote_bucket_bytes/1000000 | timechart sum(mb) as ""Data transferred"" by host","source=*splunk_archiver.log*",,"splunk_archiver_dashboard"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
        | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" services/server/introspection/queues
        | search title=""tcpin_queue*""
        | rex field=title ""tcpin_queue\.(?<pipeline_number>\d+)""
        | join splunk_server
        [ | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/properties/inputs
        | where match(title, ""splunktcp(-ssl)?:"")
        `dmc_get_port_from_splunktcp_stanza(title)`
        | stats delim="", "" values(port) as ports by splunk_server
        | mvcombine ports
        | fields splunk_server, ports]
        | join splunk_server [|rest splunk_server_group=dmc_group_indexer services/data/inputs/tcp/ssl]
        | eval val_last_1min=if(isnotnull(value_cntr1_size_bytes_lookback), round(value_cntr1_size_bytes_lookback, 2), ""N/A"")
        | eval val_last_10min=if(isnotnull(value_cntr2_size_bytes_lookback), round(value_cntr2_size_bytes_lookback, 2), ""N/A"")
        | eval queue_fill_last_1min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_1min, val_last_1min)
        | eval queue_fill_last_10min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_10min, val_last_10min)
      ]]>
    ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`| search search_group!=""dmc_group_*""
        ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              rex field=queue_fill_last_10min ""pset\d+:\s(?<queue_fill_last_10min>\d+\.\d+)""
              | stats max(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
              | where queue_fill_last_10min > 60
              | stats count as degraded_instance_count_queue_fill_ratio
            ]]>
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_indexer search_group=""$group$"" sourcetype=splunkd source=*splunkd.log ""WARN"" TcpInputConfig ""reverse dns lookups appear to be excessively slow, this may impact receiving from network inputs.""
            | stats count as degraded_instance_count_dnc_lookup, values(host) as hosts
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` search_group=dmc_group_indexer search_group=""$group$"" sourcetype=splunkd source=*splunkd.log log_level=""WARN"" component=TcpInputProc ""Stopping all listening ports.""
            | stats count as degraded_instance_count_port_closure, values(host) as hosts
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count as pset_count, values(ports), as ports values(queue_fill_last_1min) as queue_fill_last_1min, values(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
            | fields splunk_server, pset_count, ports, queue_fill_last_1min, queue_fill_last_10min
            | rename splunk_server as ""Instance"", pset_count as ""Pipeline Set Count"", ports as ""Ports"", queue_fill_last_1min as ""Queue Fill Ratio (Last 1 Minute)"", queue_fill_last_10min as ""Queue Fill Ratio (Last 10 Minutes)""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_throughput_split_by(host, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_timechart_for_metrics_log` per_second(kb) as avg_tcp_KBps by fwdType
            | rename avg_tcp_KBps as ""KB/s""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_throughput_split_by(destPort, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count, per_second(kb) as tcp_KBps
            | rename forwarder_count as ""Forwarder Count"", tcp_KBps as ""Throughput (KB/s)""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_forwarder_count_split_by(host, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count by fwdType
            | rename forwarder_count as ""Forwarder Count""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_tcp_forwarder_count_split_by(destPort, ""$group$"")`
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | where host=""$drilldown_indexer_name$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_rename_forwarder_type(fwdType)`
            | where fwdType=""$drilldown_type_forwarder_type$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps)
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` search_group=dmc_group_indexer search_group=""$group$""
            | where destPort=""$drilldown_port_forwarder_port$""
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb)
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline)
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps)
            | `dmc_rename_forwarder_type(fwdType)`
            | `dmc_time_format(last_connected)`
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count
            | search hostname=""***""
            | search status=""*""
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | bin _time minspan=30s
            | stats $fillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
            | `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | bin _time minspan=30s
            | stats $fillRatioAggrFunc$(fill_perc) AS ""fill_percentage"" by host, _time
            | `dmc_queue_fill_ratio_rangemap_and_timechart`
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"`dmc_drilldown_indexing_performance_deployment_queue_fill_ratio(""*"", splunktcpin, $fillRatioAggrFunc$, $drilldown_queue_fill_ratio_metric$)`",,,"splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | `dmc_timechart_for_metrics_log` partial=f limit=25 $fillRatioAggrFunc$(fill_perc) AS fill_percentage by host
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` sourcetype=splunkd source=*metrics.log search_group=dmc_group_indexer search_group=""$group$"" group=queue name=splunktcpin
            | eval max=if(isnotnull(max_size_kb),max_size_kb,max_size)
            | eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size)
            | eval fill_perc=round((curr/max)*100,2)
            | `dmc_timechart_for_metrics_log` partial=f $fillRatioAggrFunc$(fill_perc) AS fill_percentage
          ","source=*metrics.log","sourcetype=splunkd","splunk_tcpin_performance_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      <![CDATA[
            | rest splunk_server=$splunk_server$ services/server/introspection/queues
            | search title=""tcpin_queue*""
            | rex field=title ""tcpin_queue\.(?<pipeline_number>\d+)""
            | join splunk_server
            [ | rest splunk_server=$splunk_server$ /services/properties/inputs
              | where match(title, ""splunktcp(-ssl)?:"")
              `dmc_get_port_from_splunktcp_stanza(title)`
              | stats delim="", "" values(port) as ports by splunk_server
              | mvcombine ports
              | fields splunk_server, ports]
            | join splunk_server [|rest splunk_server_group=dmc_group_indexer services/data/inputs/tcp/ssl]
            | eval val_last_1min=if(isnotnull(value_cntr1_size_bytes_lookback), round(value_cntr1_size_bytes_lookback, 2), ""N/A"")
            | eval val_last_10min=if(isnotnull(value_cntr2_size_bytes_lookback), round(value_cntr2_size_bytes_lookback, 2), ""N/A"")
            | eval queue_fill_last_1min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_1min, val_last_1min)
            | eval queue_fill_last_10min = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".val_last_10min, val_last_10min)
            ]]>
    ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            <![CDATA[
              rex field=queue_fill_last_10min ""pset\d+:\s(?<queue_fill_last_10min>\d+\.\d+)""
              | stats max(queue_fill_last_10min) as queue_fill_last_10min
              | where queue_fill_last_10min > 60
              | stats count as degraded_instance_count_queue_fill_ratio
            ]]>
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` splunk_server=$splunk_server$ sourcetype=splunkd source=*splunkd.log ""WARN"" TcpInputConfig ""reverse dns lookups appear to be excessively slow, this may impact receiving from network inputs.""
            | stats count as dns_lookup_warning_count
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_set_index_internal` splunk_server=$splunk_server$ sourcetype=splunkd source=*splunkd.log log_level=""WARN"" component=TcpInputProc ""Stopping all listening ports.""
            | stats count as port_closure_count
          ","source=*splunkd.log","sourcetype=splunkd","splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            stats count as pset_count, values(ports), as ports values(queue_fill_last_1min) as queue_fill_last_1min, values(queue_fill_last_10min) as queue_fill_last_10min by splunk_server
            | fields splunk_server, pset_count, ports, queue_fill_last_1min, queue_fill_last_10min
            | rename splunk_server as ""Instance"", pset_count as ""Pipeline Set Count"", ports as ""Ports"", queue_fill_last_1min as ""Queue Fill Ratio (Last 1 Minute)"", queue_fill_last_10min as ""Queue Fill Ratio (Last 10 Minutes)""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` host=$host$
            | `dmc_timechart_for_metrics_log` dc(guid) as forwarder_count, avg(tcp_KBps) as avg_tcp_KBps
            | rename forwarder_count as ""Forwarder Count"", avg_tcp_KBps as ""Throughput (KB/s)""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,"inputlookup dmc_forwarder_assets",,"splunk_monitoring_console",,"
            `dmc_get_forwarder_tcpin` host=$host$
            | stats dc(host) as connection_count by guid
            | join type=outer guid [| inputlookup dmc_forwarder_assets]
            | `dmc_rename_forwarder_type(forwarder_type)`
            | makemv delim="" "" avg_tcp_kbps_sparkline 
            | eval sum_kb = if (status == ""missing"", ""N/A"", sum_kb) 
            | eval avg_tcp_kbps_sparkline = if (status == ""missing"", ""N/A"", avg_tcp_kbps_sparkline) 
            | eval avg_tcp_kbps = if (status == ""missing"", ""N/A"", avg_tcp_kbps) 
            | eval avg_tcp_eps = if (status == ""missing"", ""N/A"", avg_tcp_eps) 
            | `dmc_rename_forwarder_type(fwdType)` 
            | `dmc_time_format(last_connected)` 
            | fields hostname, forwarder_type, version, os, arch, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps, connection_count 
            | search hostname=""***"" 
            | search status=""*"" 
            | rename hostname as Instance, forwarder_type as Type, version as Version, os as OS, arch as Architecture, status as Status, last_connected as ""Last Connected to Indexers"", sum_kb as ""Total KB"", avg_tcp_kbps_sparkline as ""Average KB/s Over Time"", avg_tcp_kbps as ""Average KB/s"", avg_tcp_eps as ""Average Events/s"", connection_count as ""Number of Connections""
          ",,,"splunk_tcpin_performance_instance"
tbd,,,,,,"splunk_secure_gateway",,"| rest ""services/ssg/test_websocket"" request_type=$request_type$ request_mode=$request_mode$",,,"ssg_e2e_wss_test"
tbd,,,,,,"splunk_secure_gateway",,"| eval status=if(auth_code_status=""200"", ""Success"", if(auth_code_status=""0"", ""Did not run"", ""Failure""))
| eval range=if(status=""Success"", ""low"", ""severe"")
| table  status range",,,"ssg_e2e_wss_test"
tbd,,,,,,"splunk_secure_gateway",,"| eval status=if(server_registration_status=""201"", ""Success"", if(server_registration_status=""0"", ""Did not run"", ""Failure""))
| eval range=if(status=""Success"", ""low"",  if(status=""Failure"", ""severe"", ""guarded""))
| table status range",,,"ssg_e2e_wss_test"
tbd,,,,,,"splunk_secure_gateway",,"| eval status=if(wss_response=""0"", ""Did Not Run"", if(wss_response!=""[]"", ""Success"", ""Failure""))
| eval range=if(status=""Success"", ""low"",  if(status=""Failure"", ""severe"", ""guarded""))
| table status range",,,"ssg_e2e_wss_test"
tbd,,,,,,"splunk_secure_gateway",,"| table  error | eval message=if(LEN(error) &gt; 1, error, ""N/A"") | table message",,,"ssg_e2e_wss_test"
tbd,,,,,,"splunk_secure_gateway",,"
| table wss_response ",,,"ssg_e2e_wss_test"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv | search id = $qID$
| table question optionA, optionB, optionC,optionD, reference, video, answer, id",,,"study_questions"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv 
| stats count by testType",,,"study_questions"
tbd,,,,"inputlookup Lame_Testbank.csv",,"lame_training",,"|  inputlookup Lame_Testbank.csv
| search testType=""$s_cert$""
| table question optionA, optionB, optionC,optionD, reference, video, answer, id",,,"study_questions"
tbd,,,,,,"lame_training",,"| table id, answer ",,,"study_questions"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal source=*secure* request_id=subscription-iteration* 
| rex ""subscription-iteration-(?&lt;iteration&gt;[0-9]+) ""
| eval iteration=strftime(iteration, ""%Y-%m-%dT %H:%M:%S.%Z"")
| stats count by request_id, iteration 
| sort -_time 
| reverse","source=*secure*",,"subscription_search_inspector"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal source=*secure* request_id=$request_id$ Processing pubsub subscription  
| rex ""query=(?&lt;query&gt;[^\,]+)\,""
| eval last_updated=strftime(last_updated, ""%Y-%m-%dT %H:%M:%S.%Z"")
| eval next_update=strftime(next_update, ""%Y-%m-%dT %H:%M:%S.%Z"")
| table dashboard_id, query, subscribers, search_id, base, owner,input_tokens, last_updated, next_update, sid, search_key","source=*secure*",,"subscription_search_inspector"
tbd,,,,,,"splunk_secure_gateway",,"|rest services/search/jobs/$sid$
| table author, dispatchState, runDuration, doneProgress, isDone, eventCount, diskUsage, earliestTime, latestTime, sid",,,"subscription_search_inspector"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=secure_gateway_app_internal_log (""[create_subscription]"" ""Subscription created."") OR ""Finished process_unsubscribe_request"" | fields subscription_id, search_key, request_id, device_id, current_user, type, log_level, count, time_taken, update_id",,"sourcetype=secure_gateway_app_internal_log","subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| search ""[create_subscription]"" ""Subscription created.""",,,"subscription_tracing_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=secure_gateway_app_internal_log subscription_id=$subscriptionId$",,"sourcetype=secure_gateway_app_internal_log","subscription_tracing_dashboard"
tbd,,,"index=_internal",,,"splunk_secure_gateway",,"index=_internal sourcetype=secure_gateway_app_internal_log send_subscription_updates subscription_id=$subscriptionId$ | transaction update_id startswith=""Sending subscription update"" endswith=""Subscription Update Sent"" | eval latency = duration | fields _time, update_id, type, latency",,"sourcetype=secure_gateway_app_internal_log","subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| search ""Finished process_unsubscribe_request"" | fields count",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| table _time, subscription_id, search_key, request_id, device_id, current_user",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"|search log_level=ERROR | stats count as error_count | eval errors = if(error_count &gt; 0, ""YES"", ""NONE"") | eval range=if(error_count &gt; 0, ""severe"", ""low"") | table errors, range",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| table type",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| table current_user",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| eval device = substr(device_id, 0, 6) | table device",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| stats count | table count",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| stats avg(latency) as avg_latency by update_id | table avg_latency",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| stats count as count
                 | eval ok = if(count &gt; 0, ""NO"", ""YES"")
                 | eval range=if(count &gt; 0, ""severe"", ""low"")
                 | table ok, range
          ",,,"subscription_tracing_dashboard"
tbd,,,,,,"splunk_secure_gateway",,"| table _time, update_id, type, latency",,,"subscription_tracing_dashboard"
tbd,,,,"inputlookup source_info",,"lame_analytic_documentation",,"| inputlookup source_info
| rename _key as the_key
| table the_key, index, source, description, usegroup",,,"summary_index_information"
tbd,,,,,,"splunk_secure_gateway",,"| makeresults | eval testValue=random() | table testValue",,,"test_dashboard_single_value"
tbd,,,,,,"threat_object_fun",,"
      | tstats summariesonly=true count dc(All_Risk.risk_object) as dc_r_obj dc(All_Risk.threat_object) as dc_t_obj dc(All_Risk.src) as dc_src dc(All_Risk.dest) as dc_dest dc(All_Risk.user) as dc_users dc(All_Risk.user_bunit) as dc_bunit sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.risk_object) as risk_object values(All_Risk.src) as src values(All_Risk.dest) as dest values(All_Risk.user) as user values(All_Risk.user_bunit) as bunit from datamodel=Risk.All_Risk where All_Risk.threat_object IN (""$threat_token$"") by All_Risk.threat_object,All_Risk.threat_object_type,source | `drop_dm_object_name(""All_Risk"")` 
    ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval num_objects = if(isint($num_objects$),""$num_objects$"",0) | table num_objects
          ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval source_count = if(isint($source_count$),""$source_count$"",0) | table source_count",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| makeresults 
| eval risk_objects = if(isint($ro_count$),""$ro_count$"",0)
| table risk_objects",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval related_count = if(isint($related_count$),""$related_count$"",0) | table related_count",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"
          | tstats summariesonly=t count values(All_Risk.risk_message) as risk_message sum(All_Risk.calculated_risk_score) as risk_score values(source) as sources dc(All_Risk.threat_object) as dc_t_obj values(All_Risk.threat_object_type) as t_types values(All_Risk.threat_object) as threat_object values(All_Risk.risk_object_type) as risk_object_type from datamodel=Risk.All_Risk where All_Risk.threat_object IN (""$threat_token$"") groupby All_Risk.risk_object All_Risk.calculated_risk_score source _time span=30d | `drop_dm_object_name(""All_Risk"")` | table * | dedup risk_message | eval risk_message=""-----=== "".source."" ===-----||"".""===== THREAT OBJECTS ====||"".mvjoin(threat_object,""||"").""||===== RISK MESSAGES =====||"".mvjoin(risk_message,""||"").""|| ||"" | rex mode=sed field=risk_message ""s/\|\|/\n/g"" | stats sum(count) as events sum(dc_t_obj) as dc_t_obj values(t_types) as t_types sum(risk_score) as risk_score values(sources) as sources values(risk_message) as risk_message by risk_object | eventstats dc(risk_object) as ro_count dc(sources) as source_count sum(events) as num_objects
          ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"
            | tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.$search_values$ IN (""$values_search$"") by All_Risk.$search_values$,All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by $search_values$,risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by $search_values$,risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")
          ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"
            | tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.risk_object IN (""$values_search$"") by All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")
          ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| search threat_object IN (""$threat_token$"") | sort 1000 + count | eval threat_object_full=threat_object | eval type=threat_object_type | eval threat_object=if(len(threat_object)&lt;50, threat_object, substr(threat_object,1,50).""..."")",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| search NOT threat_object IN (""$threat_token$"") | sort 1000 + count | eval threat_object_full=threat_object | eval type=threat_object_type | eval threat_object=if(len(threat_object)&lt;50, threat_object, substr(threat_object,1,50).""..."") | eventstats dc(threat_object_full) as dc_related_objects",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"
  | rest splunk_server=local count=0 /services/saved/searches 
  | search title=""$risk_drilldown$"" | rename dispatch.earliest_time as early_time qualifiedSearch as search_spl
  | table description search_spl early_time
  | eval search_spl = if(match(search_spl,""^(|\s)(tstats|from).*""),""|"".search_spl,search_spl)
          ",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=false sum(All_Risk.calculated_risk_score) as risk_score,dc(All_Risk.risk_object) as risk_objects,count from datamodel=Risk.All_Risk where All_Risk.threat_object IN(""$threat_token$"") All_Risk.risk_object_type=""*"" (All_Risk.risk_object=""*"" OR risk_object=""*"") by source | sort 1000 - count,risk_score",,,"threat_object__hunting"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval title = ""tune"" | table title
          ",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval title = ""develop"" | table title
          ",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=false sum(All_Risk.calculated_risk_score) as risk_score dc(All_Risk.risk_object) as risk_objects dc(All_Risk.threat_object) as threat_objects count from datamodel=Risk.All_Risk where * All_Risk.risk_object_type=""*"" (All_Risk.risk_object=""*"" OR risk_object=""*"") by source | sort 1000 - count risk_score",,,"threat_object_content_dev"
tbd,"eventtype=risk_notables",,,,,"threat_object_fun",,"`notable` | search eventtype=risk_notables | eventstats count as sum | stats count(eval(status_label=""New"")) as rule_count values(sum) as sum by orig_source | eval notable_percent = rule_count / sum | sort - notable_percent | eval notable_percent=round(notable_percent*100) | rename orig_source as source",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| rest splunk_server=local count=0 /services/saved/searches 
  | search title=""$risk_drilldown$"" | rename dispatch.earliest_time as early_time qualifiedSearch as search_spl
  | table search_spl early_time",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.$search_values$ IN (""$values_search$"") by All_Risk.$search_values$,All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by $search_values$,risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by $search_values$,risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.risk_object IN (""$values_search$"") by All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"
      | tstats summariesonly=true count dc(All_Risk.risk_object) as dc_r_obj dc(All_Risk.threat_object) as dc_t_obj dc(All_Risk.src) as dc_src dc(All_Risk.dest) as dc_dest dc(All_Risk.user) as dc_users dc(All_Risk.user_bunit) as dc_bunit sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.risk_object) as risk_object values(All_Risk.src) as src values(All_Risk.dest) as dest values(All_Risk.user) as user values(All_Risk.user_bunit) as bunit from datamodel=Risk.All_Risk where source=""$risk_drilldown$"" by All_Risk.threat_object,All_Risk.threat_object_type | `drop_dm_object_name(""All_Risk"")` | sort 1000 - count | eval threat_object_full=threat_object | eval type=threat_object_type | eval threat_object=if(len(threat_object)&lt;50, threat_object, substr(threat_object,1,50).""..."")",,,"threat_object_content_dev"
tbd,,,"index=risk",,,"threat_object_fun",,"index=risk (certificate_common_name=* OR
    certificate_organization=* OR
    certificate_serial=* OR
    certificate_unit=* OR
    domain=* OR
    email=* OR
    email_subject=* OR
    file_hash=* OR
    file_name=* OR
    http_user_agent=* OR
    process=* OR
    process_hash=* OR
    process_name=* OR
    parent_process_name=* OR
    parent_process=* OR
    registry_path=* OR
    registry_value_name=* OR
    registry_value_text=* OR
    service=* OR
    url=* OR
    command=*) 
| stats 
    count(eval(like(certificate_common_name,""%""))) as count_certificate_common_name
    count(eval(like(certificate_organization,""%""))) as count_certificate_organization
    count(eval(like(certificate_serial,""%""))) as count_certificate_serial
    count(eval(like(certificate_unit,""%""))) as count_certificate_unit
    count(eval(like(domain,""%""))) as count_domain
    count(eval(like(email,""%""))) as count_email
    count(eval(like(email_subject,""%""))) as count_email_subject
    count(eval(like(file_hash,""%""))) as count_file_hash
    count(eval(like(file_name,""%""))) as count_file_name
    count(eval(like(http_user_agent,""%""))) as count_http_user_agent
    count(eval(like(process,""%""))) as count_process
    count(eval(like(process_hash,""%""))) as count_process_hash
    count(eval(like(process_name,""%""))) as count_process_name
    count(eval(like(parent_process_name,""%""))) as count_parent_process_name
    count(eval(like(parent_process,""%""))) as count_parent_process
    count(eval(like(registry_path,""%""))) as count_registry_path
    count(eval(like(registry_value_name,""%""))) as count_registry_value_name
    count(eval(like(registry_value_text,""%""))) as count_registry_value_text
    count(eval(like(service,""%""))) as count_service
    count(eval(like(url,""%""))) as count_url
    count(eval(like(command,""%""))) as count_command,
    values(threat_object_type) as threat_object_type,
    count
    by source 
| eval observed_threat_object_types=mvappend(
    if(count_certificate_common_name&gt;0, ""certificate_common_name="".count_certificate_common_name, """"),
    if(count_certificate_organization&gt;0, ""certificate_organization="".count_certificate_organization, """"),
    if(count_certificate_serial&gt;0, ""certificate_serial="".count_certificate_serial, """"),
    if(count_certificate_unit&gt;0, ""certificate_unit="".count_certificate_unit, """"),
    if(count_domain&gt;0, ""domain="".count_domain, """"),
    if(count_email&gt;0, ""email="".count_email, """"),
    if(count_email_subject&gt;0, ""email_subject="".count_email_subject, """"),
    if(count_file_hash&gt;0, ""file_hash="".count_file_hash, """"),
    if(count_file_name&gt;0, ""file_name="".count_file_name, """"),
    if(count_http_user_agent&gt;0, ""http_user_agent="".count_http_user_agent, """"),
    if(count_process&gt;0, ""process="".count_process, """"),
    if(count_process_hash&gt;0, ""process_hash="".count_process_hash, """"),
    if(count_process_name&gt;0, ""process_name="".count_process_name, """"),
    if(count_parent_process_name&gt;0, ""parent_process_name="".count_parent_process_name, """"),
    if(count_parent_process&gt;0, ""parent_process="".count_parent_process, """"),
    if(count_registry_path&gt;0, ""registry_path="".count_registry_path, """"),
    if(count_registry_value_name&gt;0, ""registry_value_name="".count_registry_value_name, """"),
    if(count_registry_value_text&gt;0, ""registry_value_text="".count_registry_value_text, """"),
    if(count_service&gt;0, ""service="".count_service, """"),
    if(count_url&gt;0, ""url="".count_url, """"),
    if(count_command&gt;0, ""command="".count_command, """"),
    """")
| stats values(count) as event_count, values(observed_threat_object_types) as observed_threat_object_types, values(threat_object_type) as existing_threat_object_types by source 
| sort - count",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| rest splunk_server=local count=0 /services/saved/searches 
| where match('action.correlationsearch.enabled', ""1|[Tt]|[Tt][Rr][Uu][Ee]"") AND NOT disabled=1
| table title, disabled, alert.track, action.risk, cron_schedule, description, eai:acl.app, action.risk.param._risk
| search action.risk.param._risk=""*threat_object*""
| eval _temp=split(replace(replace('action.risk.param._risk',""\["",""""),""\]"",""""), ""{"")
| eval _temp2=mvfilter(_temp!="""")
| eval _temp2=mvindex(_temp2, mvfind(_temp2,""threat_""))
| eval _temp3=split(_temp2,"","")
| replace ""*}"" with ""*"" in _temp3
| eval threat_objects=_temp3
| fields - action.risk.param._risk, alert.track, action.risk",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| rest splunk_server=local count=0 /services/saved/searches 
| where match('action.correlationsearch.enabled', ""1|[Tt]|[Tt][Rr][Uu][Ee]"") AND NOT disabled=1 AND 'action.risk'=1
| table title, alert.track, action.risk, cron_schedule, description, eai:acl.app, action.risk.param._risk
| search action.risk.param._risk!=""*threat_object*""
| fields - action.risk.param._risk, alert.track, action.risk",,,"threat_object_content_dev"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""" All_Risk.threat_object=$input_threat_object|s$)
    by All_Risk.threat_object, _time
| `drop_dm_object_name(""All_Risk"")`
| search threat_object=$input_threat_object|s$
| timechart span=1d sum(count) as count BY threat_object",,,"threat_object_detail"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count, values(source) as detections
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""" All_Risk.threat_object=$input_threat_object|s$)
    by All_Risk.threat_object, All_Risk.threat_object_type
| `drop_dm_object_name(""All_Risk"")`
| search threat_object=$input_threat_object|s$",,,"threat_object_detail"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count, values(source) as detections
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""" All_Risk.threat_object=$input_threat_object|s$)
    by All_Risk.threat_object, All_Risk.threat_object_type, All_Risk.risk_object, All_Risk.risk_object_type
| `drop_dm_object_name(""All_Risk"")`
| search threat_object=$input_threat_object|s$
| fields - threat_object, threat_object_type
| dedup risk_object, risk_object_type, detections
| sort - count",,,"threat_object_detail"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""")
    by All_Risk.threat_object_type
| `drop_dm_object_name(""All_Risk"")`",,,"threat_object_overview"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""")
    by _time All_Risk.threat_object_type
| `drop_dm_object_name(""All_Risk"")`
| chart bins=75 sum(count) as count over _time by threat_object_type",,,"threat_object_overview"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count as total,
    dc(All_Risk.risk_object) as distinct_risk_objects,
    values(source) as detections
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""")
    by All_Risk.threat_object 
| `drop_dm_object_name(""All_Risk"")`
| sort 100 - total
| eval threat_object_full = threat_object
| `line_shorten(threat_object)`",,,"threat_object_overview"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count as total,
    dc(All_Risk.risk_object) as distinct_risk_objects,
    values(source) as detections
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""")
    by All_Risk.threat_object 
| `drop_dm_object_name(""All_Risk"")`
| sort 100 total
| eval threat_object_full = threat_object
| `line_shorten(threat_object)`",,,"threat_object_overview"
tbd,,,,,,"threat_object_fun",,"| tstats `summariesonly` count, values(source) as detections, values(All_Risk.threat_object_type) as threat_object_type, dc(All_Risk.threat_object_type) as threat_object_type_count
    from datamodel=Risk.All_Risk 
    where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""" All_Risk.threat_object_type!=file_name) ( 
    [| tstats `summariesonly` count, values(source) as detections
        from datamodel=Risk.All_Risk 
        where (All_Risk.threat_object_type!=""unknown"" All_Risk.threat_object_type!="""" All_Risk.threat_object_type=file_name)
        by All_Risk.threat_object, All_Risk.threat_object_type 
    | `drop_dm_object_name(""All_Risk"")` 
    | fields threat_object 
    | eval threat_object=""*"".mvindex(split(threat_object,""\\""), mvcount(split(threat_object,""\\""))-1).""*"" 
    | rename threat_object AS ""All_Risk.threat_object""])
    by All_Risk.threat_object 
| `drop_dm_object_name(""All_Risk"")`
| sort - threat_object_type_count",,,"threat_object_overview"
tbd,,,,,,"threat_object_fun",,"
      | tstats summariesonly=true count dc(All_Risk.risk_object) as dc_r_obj dc(All_Risk.threat_object) as dc_t_obj dc(All_Risk.src) as dc_src dc(All_Risk.dest) as dc_dest dc(All_Risk.user) as dc_users dc(All_Risk.user_bunit) as dc_bunit sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.risk_object) as risk_object values(All_Risk.src) as src values(All_Risk.dest) as dest values(All_Risk.user) as user values(All_Risk.user_bunit) as bunit from datamodel=Risk.All_Risk where All_Risk.threat_object IN (""$threat_token$"") by All_Risk.threat_object,All_Risk.threat_object_type,source | `drop_dm_object_name(""All_Risk"")` 
    ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval num_objects = if(isint($num_objects$),""$num_objects$"",0) | table num_objects
          ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval source_count = if(isint($source_count$),""$source_count$"",0) | table source_count",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| makeresults 
| eval risk_objects = if(isint($ro_count$),""$ro_count$"",0)
| table risk_objects",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| makeresults | eval related_count = if(isint($related_count$),""$related_count$"",0) | table related_count",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"
          | tstats summariesonly=t count values(All_Risk.risk_message) as risk_message sum(All_Risk.calculated_risk_score) as risk_score values(source) as sources dc(All_Risk.threat_object) as dc_t_obj values(All_Risk.threat_object_type) as t_types values(All_Risk.threat_object) as threat_object values(All_Risk.risk_object_type) as risk_object_type from datamodel=Risk.All_Risk where All_Risk.threat_object IN (""$threat_token$"") groupby All_Risk.risk_object All_Risk.calculated_risk_score source _time span=30d | `drop_dm_object_name(""All_Risk"")` | table * | dedup risk_message | eval risk_message=""-----=== "".source."" ===-----||"".""===== THREAT OBJECTS ====||"".mvjoin(threat_object,""||"").""||===== RISK MESSAGES =====||"".mvjoin(risk_message,""||"").""|| ||"" | rex mode=sed field=risk_message ""s/\|\|/\n/g"" | stats sum(count) as events sum(dc_t_obj) as dc_t_obj values(t_types) as t_types sum(risk_score) as risk_score values(sources) as sources values(risk_message) as risk_message by risk_object | eventstats dc(risk_object) as ro_count dc(sources) as source_count sum(events) as num_objects
          ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"
            | tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.$search_values$ IN (""$values_search$"") by All_Risk.$search_values$,All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by $search_values$,risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by $search_values$,risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")
          ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"
            | tstats summariesonly=true count sum(All_Risk.calculated_risk_score) as risk_sum values(All_Risk.calculated_risk_score) as scores values(All_Risk.threat_object_type) as threat_object_type from datamodel=Risk.All_Risk where All_Risk.risk_object IN (""$values_search$"") by All_Risk.risk_object,source,All_Risk.threat_object | `drop_dm_object_name(""All_Risk"")` | eval threat_object_type=mvjoin(threat_object_type,""+"") , threat_object_full=threat_object , threat_object=if(len(threat_object)&lt;90, threat_object, substr(threat_object,1,90).""..."") , threat_object=threat_object_type."":  "".threat_object | stats values(threat_object) as threat_object values(threat_object_full) as threat_object_full sum(count) as count sum(risk_sum) as risk_sum by risk_object,source | eval threat_object=""--= "".source."" =--||"".mvjoin(threat_object,""||"") | rex mode=sed field=threat_object ""s/\|\|/\n/g"" | stats values(threat_object) as threat_object sum(count) as count sum(risk_sum) as risk_sum values(threat_object_full) as threat_object_full by risk_object  | eval threat_object_full=mvjoin(threat_object_full,""&amp;&amp;"")
          ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| search threat_object IN (""$threat_token$"") | sort 1000 + count | eval threat_object_full=threat_object | eval type=threat_object_type | eval threat_object=if(len(threat_object)&lt;50, threat_object, substr(threat_object,1,50).""..."")",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| search NOT threat_object IN (""$threat_token$"") | sort 1000 + count | eval threat_object_full=threat_object | eval type=threat_object_type | eval threat_object=if(len(threat_object)&lt;50, threat_object, substr(threat_object,1,50).""..."") | eventstats dc(threat_object_full) as dc_related_objects",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"
  | rest splunk_server=local count=0 /services/saved/searches 
  | search title=""$risk_drilldown$"" | rename dispatch.earliest_time as early_time qualifiedSearch as search_spl
  | table description search_spl early_time
  | eval search_spl = if(match(search_spl,""^(|\s)(tstats|from).*""),""|"".search_spl,search_spl)
          ",,,"threat_object_prevalence"
tbd,,,,,,"threat_object_fun",,"| tstats summariesonly=false sum(All_Risk.calculated_risk_score) as risk_score,dc(All_Risk.risk_object) as risk_objects,count from datamodel=Risk.All_Risk where All_Risk.threat_object IN(""$threat_token$"") All_Risk.risk_object_type=""*"" (All_Risk.risk_object=""*"" OR risk_object=""*"") by source | sort 1000 - count,risk_score",,,"threat_object_prevalence"
tbd,,,"index=risk",,,"threat_object_fun",,"index=risk risk_object=""$input_risk_object$""
| eval threat_object_and_type=mvzip(threat_object_type, threat_object, ""="")
| table source, risk_object, threat_object_and_type, threat_object_type, threat_object
| fields threat_object_type
| mvexpand threat_object_type
| dedup threat_object_type
| sort threat_object_type",,,"threat_object_soar"
tbd,,,"index=risk",,,"threat_object_fun",,"index=risk risk_object=""$input_risk_object$""
| eval threat_object_and_type=mvzip(threat_object_type, threat_object, ""="")
| table source, risk_object, threat_object_and_type, threat_object_type, threat_object
| mvexpand threat_object_and_type
```| stats count, values(source) as detections, values(threat_object_and_type) as threat_object_and_type by risk_object```
| fields threat_object_and_type
| where threat_object_and_type!=""""
| dedup threat_object_and_type
| sort threat_object_and_type
| search threat_object_and_type=""$input_threat_object_type$*""
| rename threat_object_and_type AS IOBs",,,"threat_object_soar"
tbd,,,,,,"lame_training",,"| makeresults | eval foo = ""bar""",,,urlgenerator
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/index-volumes/$volume$
      | eval volumeSizeGB = if(total_size > 1, round(total_size / 1024, 2), 0)
      | eval maxSizeGB = if(isNum(max_size), round(max_size / 1024, 2), ""unlimited"")
      | eval diskUsageGB = if(volumeSizeGB == 0, ""-"", volumeSizeGB)."" / "".maxSizeGB
    ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | where search_group!=""dmc_group_indexer""
        ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | rest splunk_server_group=dmc_group_indexer splunk_server_group=""$group$"" /services/data/index-volumes
          | `dmc_exclude_volumes`
          | fields title
          | dedup title
        ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats dc(splunk_server) as Indexers
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats sum(volumeSizeGB) as totalVolumeSizeGB
            | eval totalVolumeSizeGB = round(totalVolumeSizeGB, 2)."" GB""
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            stats avg(volumeSizeGB) as avgVolumeSizeGB
            | eval avgVolumeSizeGB = round(avgVolumeSizeGB, 2)."" GB""
          ",,,"volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            fields splunk_server, diskUsageGB, maxSizeGB, volume_path
            | rename splunk_server as ""Instance"", diskUsageGB as ""Volume Usage (GB)"", maxSizeGB as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | `dmc_set_bin_for_disk_usage`
            | stats latest(data.total_size) as totalSize by host _time
            | `dmc_timechart_for_disk_usage` $funcDiskSizeUsage$(eval(totalSize /1024)) as ""Volume Size""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | eval pctDiskUsage = 'data.total_size' / 'data.max_size'
            | bin _time minspan=10min
            | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
            | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
            | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
            | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection sourcetype=splunk_disk_objects component=Volumes data.name=$volume$ search_group=dmc_group_indexer search_group=""$group$""
            | eval pctDiskUsage = 'data.total_size' / 'data.max_size'
            | bin _time minspan=10min
            | stats $funcDiskPercUsage$(pctDiskUsage) as pctDiskUsage by host _time
            | rangemap field=pctDiskUsage ""0-25%""=0-0.25 ""25-50%""=0.2501-0.5 ""50-75%""=0.5001-0.75 ""75-100%""=0.7501-1 default=abnormal
            | `dmc_timechart_for_disk_usage` partial=f dc(host) as host by range
            | fields _time, ""0-25%"", ""25-50%"", ""50-75%"", ""75-100%""
          ",,"sourcetype=splunk_disk_objects","volume_detail_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/index-volumes
      | fields title, total_size, max_size, volume_path
      | `dmc_exclude_volumes`
    ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
      | rest splunk_server=$splunk_server$ /services/data/indexes-extended $datatype$
      | join type=outer title [
        | rest splunk_server=$splunk_server$ /services/data/indexes $datatype$]
      | `dmc_exclude_indexes`
      | rex field=homePath ""volume:(?&lt;home_vol_name&gt;[^/\\\]*)(?:/|\\\)""
      | rex field=coldPath ""volume:(?&lt;cold_vol_name&gt;[^/\\\]*)(?:/|\\\)""
    ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role(dmc_group_indexer)`
          | search search_group!=""dmc_group_*""
        ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"stats count by title",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | stats dc(title) as count
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.hot_bucket_count, bucket_dirs.home.warm_bucket_count, bucket_dirs.cold.bucket_count, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval bucket_dirs.home.hot_bucket_count = if(isnotnull('bucket_dirs.home.hot_bucket_count'), 'bucket_dirs.home.hot_bucket_count', 0)
            | eval bucket_dirs.home.warm_bucket_count = if(isnotnull('bucket_dirs.home.warm_bucket_count'), 'bucket_dirs.home.warm_bucket_count', 0)
            | eval bucket_dirs.cold.bucket_count = if(isnotnull('bucket_dirs.cold.bucket_count'), 'bucket_dirs.cold.bucket_count', 0)
            | eval bucket_count = case(
                home_vol_name == cold_vol_name, 'bucket_dirs.home.hot_bucket_count' + 'bucket_dirs.home.warm_bucket_count' + 'bucket_dirs.cold.bucket_count',
                home_vol_name == ""$volume_name$"", 'bucket_dirs.home.hot_bucket_count' + 'bucket_dirs.home.warm_bucket_count',
                cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.bucket_count')
            | stats sum(bucket_count) as bucket_count
            | eval bucket_count = toString(bucket_count, ""commas"")
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_count, bucket_dirs.cold.event_count, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval bucket_dirs.home.event_count = if(isnotnull('bucket_dirs.home.event_count'), 'bucket_dirs.home.event_count', 0)
            | eval bucket_dirs.cold.event_count = if(isnotnull('bucket_dirs.cold.event_count'), 'bucket_dirs.cold.event_count', 0)
            | eval event_count = case(
            home_vol_name == cold_vol_name, 'bucket_dirs.home.event_count' + 'bucket_dirs.cold.event_count',
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_count',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_count')
            | stats sum(event_count) as count
            | eval count = toString(count, ""commas"")
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval min_time = case(
            home_vol_name == cold_vol_name, min('bucket_dirs.home.event_min_time', 'bucket_dirs.cold.event_min_time'),
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_min_time',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_min_time')
            | stats min(min_time) as min_time
            | `dmc_time_format(min_time)`
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_max_time, bucket_dirs.cold.event_max_time, home_vol_name, cold_vol_name
            | where home_vol_name == ""$volume_name$"" OR cold_vol_name == ""$volume_name$""
            | eval max_time = case(
            home_vol_name == cold_vol_name, max('bucket_dirs.home.event_max_time', 'bucket_dirs.cold.event_max_time'),
            home_vol_name == ""$volume_name$"", 'bucket_dirs.home.event_max_time',
            cold_vol_name == ""$volume_name$"", 'bucket_dirs.cold.event_max_time')
            | stats max(max_time) as max_time
            | `dmc_time_format(max_time)`
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            where title == ""$volume_name$""
            | eval total_size_gb = if(isnull(total_size), ""-"", round(total_size / 1024, 2))
            | eval max_size_gb = if(isnull(max_size) OR max_size = ""infinite"", ""unlimited"", round(max_size / 1024, 2))
            | eval disk_usage_gb = total_size_gb."" / "".max_size_gb
            | fields title, disk_usage_gb, max_size_gb, volume_path
            | rename title as Volume, disk_usage_gb as ""Volume Usage (GB)"", max_size_gb as ""Volume Capacity (GB)"", volume_path as ""Volume Path""
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
            | fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, cold_bucket_size, total_size, home_vol_name, cold_vol_name
            | rename cold_bucket_size AS cold_size
            | eval cold_size = if(isnull(cold_size), 0, cold_size)
            | eval home_size = total_size - cold_size
            | eval cold_size = round(cold_size / 1024, 2)
            | eval home_size = round(home_size / 1024, 2)
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval dir_size = case(dir == ""home"", home_size, dir == ""cold"", cold_size)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | eval data_age_home_day = if(dir == ""home"", round((now() - 'bucket_dirs.home.event_min_time') / 86400, 0), NULL)
            | eval data_age_cold_day = if(dir == ""cold"", round((now() - 'bucket_dirs.cold.event_min_time') / 86400, 0), NULL)
            | eval data_age_day = case(dir == ""home"", data_age_home_day, dir == ""cold"", data_age_cold_day)
            | eval data_age_day = if(isnull(data_age_day), ""n/a"", data_age_day)
            | where vol_name == ""$volume_name$""
            | fields title dir dir_size data_age_day vol_name
            | where isnotnull(vol_name)
            | eval index_dir_name = title."":"".dir
            | fields index_dir_name dir_size data_age_day
            | sort - dir_size
            | rename index_dir_name as ""Index:Directory"" dir_size AS ""Disk Usage (GB)"" data_age_day as ""Data Age (Days)""
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            eval cold_bucket_size = if(isnotnull('bucket_dirs.cold.bucket_size'), 'bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size')
            | fields title, cold_bucket_size, total_size, home_vol_name, cold_vol_name
            | rename cold_bucket_size AS cold_size
            | eval cold_size = if(isnull(cold_size), 0, cold_size)
            | eval home_size = total_size - cold_size
            | eval cold_size = round(cold_size / 1024, 2)
            | eval home_size = round(home_size / 1024, 2)
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval dir_size = case(dir == ""home"", home_size, dir == ""cold"", cold_size)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | where vol_name == ""$volume_name$""
            | fields title dir dir_size vol_name
            | where isnotnull(vol_name) AND isnotnull(dir_size) AND dir_size > 0
            | eval index_dir_name = title."":"".dir
            | where vol_name == ""$volume_name$""
            | stats sum(dir_size) AS dir_size by index_dir_name
          ",,,"volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
            fields title, bucket_dirs.home.event_min_time, bucket_dirs.cold.event_min_time, home_vol_name, cold_vol_name, datatype
            | eval dir = ""home,cold""
            | makemv dir delim="",""
            | mvexpand dir
            | eval indexwithtype=title."":"".datatype, title = case(dir == ""home"", title."":home"", dir == ""cold"", title."":cold"")
            | eval data_age_home_day = if(dir == ""home"", round((now() - 'bucket_dirs.home.event_min_time') / 86400, 0), NULL)
            | eval data_age_cold_day = if(dir == ""cold"", round((now() - 'bucket_dirs.cold.event_min_time') / 86400, 0), NULL)
            | eval data_age_day = case(dir == ""home"", data_age_home_day, dir == ""cold"", data_age_cold_day)
            | eval vol_name = case(dir == ""home"", home_vol_name, dir == ""cold"", cold_vol_name)
            | where vol_name == ""$volume_name$""
            | sort - data_age_day
            | fields title, data_age_home_day, data_age_cold_day, indexwithtype
            | where NOT (isnull(data_age_home_day) AND isnull(data_age_cold_day))
            | rename title as Index, data_age_home_day as ""home data age (days)"", data_age_cold_day as ""cold data age (days)""
          ",,,"volume_detail_instance"
tbd,,,"index=_introspection",,,"splunk_monitoring_console",,"
            index=_introspection host=$host$ sourcetype=splunk_disk_objects component=Volumes data.name=""$volume_name$""
            | eval data.total_size = round('data.total_size' / 1024, 2)
            | eval data.total_capacity = round('data.max_size' / 1024, 2)                    
            | `dmc_timechart_for_disk_usage` partial=false $funcVolume$(data.total_size) as volume_usage, latest(data.total_capacity) as volume_capacity
            | fields _time, volume_usage $capacityOverlay$
          ",,"sourcetype=splunk_disk_objects","volume_detail_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_introspection` search_group=""$role$"" sourcetype=splunk_resource_usage component=PerProcess
             | `dmc_rename_introspection_fields`
             | `dmc_set_bin`
             | stats sum(pct_cpu) as pct_cpu by host, workload_pool, _time
             | `dmc_timechart` $CPUFunction$(pct_cpu) as pct_cpu by workload_pool
          ",,"sourcetype=splunk_resource_usage","workload_management"
tbd,,,,,,"splunk_monitoring_console",,"
             `dmc_set_index_introspection` search_group=""$role$"" sourcetype=splunk_resource_usage component=PerProcess
             | `dmc_rename_introspection_fields`
             | `dmc_set_bin`
             | stats sum(mem_used) as mem_used by host, workload_pool _time
             | `dmc_timechart` $MemFunc$(mem_used) as mem_used by workload_pool
          ",,"sourcetype=splunk_resource_usage","workload_management"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
         `dmc_set_index_internal` search_group=$role$ search_group=$dmc_group$ host=$host$ sourcetype=wlm_*
         | stats dc(search_id) as ""search_count"" by wlm_action
      ",,"sourcetype=wlm_*","workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""abort"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""move"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""alert"" | fields search_count
          ",,,"workload_management_monitoring_distributed"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
	  `dmc_set_index_internal` host=$host$ sourcetype=wlm_*
	  | stats dc(search_id) as ""search_count"" by wlm_action
      ",,"sourcetype=wlm_*","workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""abort"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""move"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
             | where wlm_action = ""alert"" | fields search_count
          ",,,"workload_management_monitoring_instance"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_per_pool_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
            | rest /services/workloads/pools splunk_server_group=* splunk_server_group=*
            | rename title AS pool_name category AS pool_type
            | eval pool_and_type = pool_name."" ("".pool_type."")""
            | stats values(splunk_server)  values(pool_type) as pool_type values(pool_name) as pool_name by pool_and_type
            | fields pool_and_type pool_type pool_name
          ",,,"workload_management_per_pool_deployment"
tbd,,,,,,"splunk_monitoring_console",,"
          | `dmc_get_groups_containing_role($role$)`
          | search search_group!=""dmc_group_*""
        ",,,"workload_management_per_pool_instance"
tbd,,,,,,"splunk_monitoring_console",,"
              | rest splunk_server_group=$role$ splunk_server_group=$dmc_group$ splunk_server=$host$ /services/workloads/pools
              | rename title AS pool_name category AS pool_type
              | eval pool_and_type = pool_name."" ("".pool_type."")""
              | stats values(pool_type) as pool_type values(pool_name) as pool_name by pool_and_type
              | fields pool_and_type pool_type pool_name
          ",,,"workload_management_per_pool_instance"
tbd,,,"index=lame_training",,,"lame_training",,"index=lame_training sourcetype=lame_conn src_ip=""$mySrcIP$"" direction=$myDirection$ dest_ip IN ($dest_ip$)
          | table _time, src_ip, dest_ip, direction, bytes_in, bytes_out",,"sourcetype=lame_conn","youtube_dashboard"
tbd,,,"index=lame_training",,,"lame_training",,"index=lame_training sourcetype=lame_conn | table direction, dest_ip",,"sourcetype=lame_conn","youtube_dashboard"
tbd,,,,,,"lame_training",,"
| stats count by direction",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by dest_ip",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| table _time, src_ip, dest_ip, direction, bytes_in, bytes_out",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by direction",,,"youtube_dashboard"
tbd,,,,,,"lame_training",,"| stats count by bytes_in",,,"youtube_dashboard"
